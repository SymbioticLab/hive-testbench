18/06/26 03:09:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/06/26 03:09:40 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18/06/26 03:09:40 INFO metastore.ObjectStore: ObjectStore, initialize called
18/06/26 03:09:40 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
18/06/26 03:09:40 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored
18/06/26 03:09:42 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
18/06/26 03:09:43 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 03:09:43 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 03:09:43 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 03:09:43 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 03:09:43 INFO DataNucleus.Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18/06/26 03:09:43 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
18/06/26 03:09:43 INFO metastore.ObjectStore: Initialized ObjectStore
18/06/26 03:09:43 INFO metastore.HiveMetaStore: Added admin role in metastore
18/06/26 03:09:44 INFO metastore.HiveMetaStore: Added public role in metastore
18/06/26 03:09:44 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
18/06/26 03:09:44 INFO metastore.HiveMetaStore: 0: get_all_databases
18/06/26 03:09:44 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_all_databases	
18/06/26 03:09:44 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
18/06/26 03:09:44 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
18/06/26 03:09:44 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 03:09:44 INFO metastore.HiveMetaStore: 0: get_functions: db=tpcds_text_2 pat=*
18/06/26 03:09:44 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_functions: db=tpcds_text_2 pat=*	
18/06/26 03:09:44 INFO session.SessionState: Created local directory: /tmp/b2dd94d3-a7d1-4da7-92fb-c4e5bf59866a_resources
18/06/26 03:09:44 INFO session.SessionState: Created HDFS directory: /tmp/hive/wentingt/b2dd94d3-a7d1-4da7-92fb-c4e5bf59866a
18/06/26 03:09:44 INFO session.SessionState: Created local directory: /tmp/wentingt/b2dd94d3-a7d1-4da7-92fb-c4e5bf59866a
18/06/26 03:09:44 INFO session.SessionState: Created HDFS directory: /tmp/hive/wentingt/b2dd94d3-a7d1-4da7-92fb-c4e5bf59866a/_tmp_space.db
18/06/26 03:09:44 INFO spark.SparkContext: Running Spark version 2.4.0-SNAPSHOT
18/06/26 03:09:44 INFO spark.SparkContext: Submitted application: SparkSQL::10.0.1.253
18/06/26 03:09:45 INFO spark.SecurityManager: Changing view acls to: wentingt
18/06/26 03:09:45 INFO spark.SecurityManager: Changing modify acls to: wentingt
18/06/26 03:09:45 INFO spark.SecurityManager: Changing view acls groups to: 
18/06/26 03:09:45 INFO spark.SecurityManager: Changing modify acls groups to: 
18/06/26 03:09:45 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(wentingt); groups with view permissions: Set(); users  with modify permissions: Set(wentingt); groups with modify permissions: Set()
18/06/26 03:09:45 INFO util.Utils: Successfully started service 'sparkDriver' on port 46289.
18/06/26 03:09:45 INFO spark.SparkEnv: Registering MapOutputTracker
18/06/26 03:09:45 INFO spark.SparkEnv: Registering BlockManagerMaster
18/06/26 03:09:45 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/06/26 03:09:45 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/06/26 03:09:45 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-f8725421-3f77-42e9-a01f-85d4b63ff850
18/06/26 03:09:45 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
18/06/26 03:09:45 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/06/26 03:09:45 INFO util.log: Logging initialized @7440ms
18/06/26 03:09:45 INFO server.Server: jetty-9.3.z-SNAPSHOT
18/06/26 03:09:45 INFO server.Server: Started @7518ms
18/06/26 03:09:46 INFO server.AbstractConnector: Started ServerConnector@13ca16bf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18/06/26 03:09:46 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@8b41ad{/jobs,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@203b953c{/jobs/json,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@730bea0{/jobs/job,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@677cb96e{/jobs/job/json,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b1252c8{/stages,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19d53ab4{/stages/json,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53cb0bcb{/stages/stage,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@684372d0{/stages/stage/json,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63dda940{/stages/pool,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41f964f9{/stages/pool/json,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@652e345{/storage,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7574d4ad{/storage/json,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7bede4ea{/storage/rdd,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@713999c2{/storage/rdd/json,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6060146b{/environment,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33627576{/environment/json,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27bc1d44{/executors,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1af677f8{/executors/json,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a55fb81{/executors/threadDump,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a3cf878{/executors/threadDump/json,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d2d8846{/static,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@240a2619{/,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e3283f6{/api,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72224107{/jobs/job/kill,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@394fb736{/stages/stage/kill,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://dc1master-lan1:4040
18/06/26 03:09:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 0
18/06/26 03:09:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:46 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://dc1master-lan1:7077...
18/06/26 03:09:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 0
18/06/26 03:09:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:46 INFO client.TransportClientFactory: Successfully created connection to dc1master-lan1/10.0.1.253:7077 after 51 ms (0 ms spent in bootstraps)
18/06/26 03:09:46 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20180626030946-0058
18/06/26 03:09:46 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20180626030946-0058/0 on worker-20180626004527-10.0.1.2-38332 (10.0.1.2:38332) with 10 core(s)
18/06/26 03:09:46 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20180626030946-0058/0 on hostPort 10.0.1.2:38332 with 10 core(s), 2.0 GB RAM
18/06/26 03:09:46 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20180626030946-0058/1 on worker-20180626004527-10.0.1.1-34053 (10.0.1.1:34053) with 10 core(s)
18/06/26 03:09:46 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20180626030946-0058/1 on hostPort 10.0.1.1:34053 with 10 core(s), 2.0 GB RAM
18/06/26 03:09:46 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36696.
18/06/26 03:09:46 INFO netty.NettyBlockTransferService: Server created on dc1master-lan1:36696
18/06/26 03:09:46 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/06/26 03:09:46 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20180626030946-0058/0 is now RUNNING
18/06/26 03:09:46 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20180626030946-0058/1 is now RUNNING
18/06/26 03:09:46 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, dc1master-lan1, 36696, None)
18/06/26 03:09:46 INFO storage.BlockManagerMasterEndpoint: Registering block manager dc1master-lan1:36696 with 366.3 MB RAM, BlockManagerId(driver, dc1master-lan1, 36696, None)
18/06/26 03:09:46 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, dc1master-lan1, 36696, None)
18/06/26 03:09:46 INFO storage.BlockManager: external shuffle service port = 7337
18/06/26 03:09:46 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, dc1master-lan1, 36696, None)
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1fb6b8fb{/metrics/json,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO scheduler.EventLoggingListener: Logging events to hdfs://dc1master:9000/user/wentingt/spark-logs/app-20180626030946-0058
18/06/26 03:09:46 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
18/06/26 03:09:46 INFO internal.SharedState: loading hive config file: file:/users/wentingt/spark-terra/conf/hive-site.xml
18/06/26 03:09:46 INFO internal.SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
18/06/26 03:09:46 INFO internal.SharedState: Warehouse path is '/user/hive/warehouse'.
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3eec8583{/SQL,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@66e1b2a{/SQL/json,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@dae5e0{/SQL/execution,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59a94d0f{/SQL/execution/json,null,AVAILABLE,@Spark}
18/06/26 03:09:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e364f1f{/static/sql,null,AVAILABLE,@Spark}
18/06/26 03:09:47 INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
18/06/26 03:09:47 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
18/06/26 03:09:47 INFO metastore.HiveMetaStore: 0: get_database: default
18/06/26 03:09:47 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: default	
18/06/26 03:09:47 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 0
18/06/26 03:09:47 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:47 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 0
18/06/26 03:09:47 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:47 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
18/06/26 03:09:47 INFO metastore.HiveMetaStore: 0: get_database: global_temp
18/06/26 03:09:47 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: global_temp	
18/06/26 03:09:47 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
18/06/26 03:09:47 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:47 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 0
18/06/26 03:09:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 0
18/06/26 03:09:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=web_sales
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=web_sales	
18/06/26 03:09:48 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.1.1:35648) with ID 1
18/06/26 03:09:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:48 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.1.2:54060) with ID 0
18/06/26 03:09:48 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.1:40144 with 912.3 MB RAM, BlockManagerId(1, 10.0.1.1, 40144, None)
18/06/26 03:09:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:48 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.2:33266 with 912.3 MB RAM, BlockManagerId(0, 10.0.1.2, 33266, None)
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=warehouse
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=warehouse	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=date_dim
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=date_dim	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=time_dim
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=time_dim	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=ship_mode
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=ship_mode	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=catalog_sales
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=catalog_sales	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=warehouse
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=warehouse	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=date_dim
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=date_dim	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=time_dim
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=time_dim	
18/06/26 03:09:48 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=ship_mode
18/06/26 03:09:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=ship_mode	
18/06/26 03:09:49 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:49 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:49 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:49 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:50 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:50 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:50 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:50 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:50 WARN util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
18/06/26 03:09:51 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:51 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:51 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:51 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:52 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:52 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:52 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:52 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:52 INFO codegen.CodeGenerator: Code generated in 204.511168 ms
18/06/26 03:09:52 INFO codegen.CodeGenerator: Code generated in 30.956926 ms
18/06/26 03:09:52 INFO codegen.CodeGenerator: Code generated in 34.523973 ms
18/06/26 03:09:52 INFO codegen.CodeGenerator: Code generated in 36.197154 ms
18/06/26 03:09:52 INFO codegen.CodeGenerator: Code generated in 49.346104 ms
18/06/26 03:09:52 INFO codegen.CodeGenerator: Code generated in 265.858269 ms
18/06/26 03:09:52 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 290.8 KB, free 365.2 MB)
18/06/26 03:09:52 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 290.4 KB, free 365.2 MB)
18/06/26 03:09:52 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 291.1 KB, free 365.2 MB)
18/06/26 03:09:52 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 290.4 KB, free 365.2 MB)
18/06/26 03:09:52 INFO spark.ContextCleaner: Cleaned accumulator 7
18/06/26 03:09:52 INFO spark.ContextCleaner: Cleaned accumulator 9
18/06/26 03:09:52 INFO spark.ContextCleaner: Cleaned accumulator 4
18/06/26 03:09:52 INFO spark.ContextCleaner: Cleaned accumulator 5
18/06/26 03:09:52 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 24.8 KB, free 365.1 MB)
18/06/26 03:09:52 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.8 KB, free 365.1 MB)
18/06/26 03:09:52 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.0 KB, free 365.1 MB)
18/06/26 03:09:52 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 24.9 KB, free 365.1 MB)
18/06/26 03:09:52 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on dc1master-lan1:36696 (size: 25.0 KB, free: 366.3 MB)
18/06/26 03:09:52 INFO spark.SparkContext: Created broadcast 2 from 
18/06/26 03:09:52 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on dc1master-lan1:36696 (size: 24.8 KB, free: 366.3 MB)
18/06/26 03:09:52 INFO spark.SparkContext: Created broadcast 3 from 
18/06/26 03:09:52 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on dc1master-lan1:36696 (size: 24.9 KB, free: 366.2 MB)
18/06/26 03:09:52 INFO spark.SparkContext: Created broadcast 1 from 
18/06/26 03:09:52 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on dc1master-lan1:36696 (size: 24.8 KB, free: 366.2 MB)
18/06/26 03:09:52 INFO spark.SparkContext: Created broadcast 0 from 
18/06/26 03:09:53 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:09:53 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:09:53 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:09:53 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:09:53 INFO spark.SparkContext: Starting job: run at ThreadPoolExecutor.java:1142
18/06/26 03:09:53 INFO spark.SparkContext: Starting job: run at ThreadPoolExecutor.java:1142
18/06/26 03:09:53 INFO spark.SparkContext: Starting job: run at ThreadPoolExecutor.java:1142
18/06/26 03:09:53 INFO spark.SparkContext: Starting job: run at ThreadPoolExecutor.java:1142
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Got job 0 (run at ThreadPoolExecutor.java:1142) with 2 output partitions
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1142)
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Missing parents: List()
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1142), which has no missing parents
18/06/26 03:09:53 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.0 KB, free 365.1 MB)
18/06/26 03:09:53 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KB, free 365.0 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on dc1master-lan1:36696 (size: 6.0 KB, free: 366.2 MB)
18/06/26 03:09:53 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1074
18/06/26 03:09:53 INFO codegen.CodeGenerator: Code generated in 245.823259 ms
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[21] at run at ThreadPoolExecutor.java:1142) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Got job 1 (run at ThreadPoolExecutor.java:1142) with 2 output partitions
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (run at ThreadPoolExecutor.java:1142)
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Missing parents: List()
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[20] at run at ThreadPoolExecutor.java:1142), which has no missing parents
18/06/26 03:09:53 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 13.3 KB, free 365.0 MB)
18/06/26 03:09:53 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.2 KB, free 365.0 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on dc1master-lan1:36696 (size: 6.2 KB, free: 366.2 MB)
18/06/26 03:09:53 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1074
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[20] at run at ThreadPoolExecutor.java:1142) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
18/06/26 03:09:53 INFO codegen.CodeGenerator: Code generated in 49.575609 ms
18/06/26 03:09:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.0.1.2, executor 0, partition 0, ANY, 7915 bytes)
18/06/26 03:09:53 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.0.1.1, executor 1, partition 1, ANY, 7915 bytes)
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Got job 2 (run at ThreadPoolExecutor.java:1142) with 2 output partitions
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (run at ThreadPoolExecutor.java:1142)
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Missing parents: List()
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[22] at run at ThreadPoolExecutor.java:1142), which has no missing parents
18/06/26 03:09:53 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.9 KB, free 365.0 MB)
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:53 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.9 KB, free 365.0 MB)
18/06/26 03:09:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, 10.0.1.1, executor 1, partition 0, ANY, 7916 bytes)
18/06/26 03:09:53 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, 10.0.1.2, executor 0, partition 1, ANY, 7916 bytes)
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on dc1master-lan1:36696 (size: 5.9 KB, free: 366.2 MB)
18/06/26 03:09:53 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1074
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[22] at run at ThreadPoolExecutor.java:1142) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Got job 3 (run at ThreadPoolExecutor.java:1142) with 2 output partitions
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (run at ThreadPoolExecutor.java:1142)
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Missing parents: List()
18/06/26 03:09:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, 10.0.1.2, executor 0, partition 0, ANY, 7916 bytes)
18/06/26 03:09:53 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, 10.0.1.1, executor 1, partition 1, ANY, 7916 bytes)
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[23] at run at ThreadPoolExecutor.java:1142), which has no missing parents
18/06/26 03:09:53 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 11.2 KB, free 365.0 MB)
18/06/26 03:09:53 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.7 KB, free 365.0 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on dc1master-lan1:36696 (size: 5.7 KB, free: 366.2 MB)
18/06/26 03:09:53 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1074
18/06/26 03:09:53 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[23] at run at ThreadPoolExecutor.java:1142) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6, 10.0.1.1, executor 1, partition 0, ANY, 7915 bytes)
18/06/26 03:09:53 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 7, 10.0.1.2, executor 0, partition 1, ANY, 7915 bytes)
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:53 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 03:09:53 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 0
18/06/26 03:09:53 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 0
18/06/26 03:09:53 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 1
18/06/26 03:09:53 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 1
18/06/26 03:09:53 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 2
18/06/26 03:09:53 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 2
18/06/26 03:09:53 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 3
18/06/26 03:09:53 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 3
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.1.2:33266 (size: 6.2 KB, free: 912.3 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.1.1:40144 (size: 6.0 KB, free: 912.3 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.1.2:33266 (size: 5.7 KB, free: 912.3 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.1.1:40144 (size: 6.2 KB, free: 912.3 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.1.2:33266 (size: 5.9 KB, free: 912.3 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.1.1:40144 (size: 5.7 KB, free: 912.3 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.1.2:33266 (size: 6.0 KB, free: 912.3 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.1.1:40144 (size: 5.9 KB, free: 912.3 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.2:33266 (size: 24.8 KB, free: 912.3 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.1:40144 (size: 24.9 KB, free: 912.3 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.2:33266 (size: 24.8 KB, free: 912.2 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.2:33266 (size: 24.9 KB, free: 912.2 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.1:40144 (size: 24.8 KB, free: 912.2 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.2:33266 (size: 25.0 KB, free: 912.2 MB)
18/06/26 03:09:53 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.1:40144 (size: 24.8 KB, free: 912.2 MB)
18/06/26 03:09:54 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.1:40144 (size: 25.0 KB, free: 912.2 MB)
18/06/26 03:09:54 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:54 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:54 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:54 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 2246 ms on 10.0.1.2 (executor 0) (1/2)
18/06/26 03:09:55 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 2249 ms on 10.0.1.2 (executor 0) (1/2)
18/06/26 03:09:55 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 2253 ms on 10.0.1.1 (executor 1) (2/2)
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/06/26 03:09:55 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 2250 ms on 10.0.1.1 (executor 1) (2/2)
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
18/06/26 03:09:55 INFO scheduler.DAGScheduler: ResultStage 1 (run at ThreadPoolExecutor.java:1142) finished in 2.304 s
18/06/26 03:09:55 INFO scheduler.DAGScheduler: ResultStage 2 (run at ThreadPoolExecutor.java:1142) finished in 2.276 s
18/06/26 03:09:55 INFO scheduler.DAGScheduler: Job 1 finished: run at ThreadPoolExecutor.java:1142, took 2.486667 s
18/06/26 03:09:55 INFO scheduler.DAGScheduler: Job 2 finished: run at ThreadPoolExecutor.java:1142, took 2.486705 s
18/06/26 03:09:55 INFO codegen.CodeGenerator: Code generated in 11.288289 ms
18/06/26 03:09:55 INFO codegen.CodeGenerator: Code generated in 11.96693 ms
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 2304 ms on 10.0.1.1 (executor 1) (1/2)
18/06/26 03:09:55 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 1024.1 KB, free 364.0 MB)
18/06/26 03:09:55 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 1024.1 KB, free 363.0 MB)
18/06/26 03:09:55 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 546.0 B, free 363.0 MB)
18/06/26 03:09:55 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 220.0 B, free 363.0 MB)
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on dc1master-lan1:36696 (size: 546.0 B, free: 366.2 MB)
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on dc1master-lan1:36696 (size: 220.0 B, free: 366.2 MB)
18/06/26 03:09:55 INFO spark.SparkContext: Created broadcast 8 from run at ThreadPoolExecutor.java:1142
18/06/26 03:09:55 INFO spark.SparkContext: Created broadcast 9 from run at ThreadPoolExecutor.java:1142
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 7) in 2312 ms on 10.0.1.2 (executor 0) (2/2)
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
18/06/26 03:09:55 INFO scheduler.DAGScheduler: ResultStage 3 (run at ThreadPoolExecutor.java:1142) finished in 2.320 s
18/06/26 03:09:55 INFO scheduler.DAGScheduler: Job 3 finished: run at ThreadPoolExecutor.java:1142, took 2.543945 s
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2346 ms on 10.0.1.1 (executor 1) (1/2)
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:55 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2377 ms on 10.0.1.2 (executor 0) (2/2)
18/06/26 03:09:55 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/06/26 03:09:55 INFO scheduler.DAGScheduler: ResultStage 0 (run at ThreadPoolExecutor.java:1142) finished in 2.486 s
18/06/26 03:09:55 INFO scheduler.DAGScheduler: Job 0 finished: run at ThreadPoolExecutor.java:1142, took 2.554338 s
18/06/26 03:09:55 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 1026.9 KB, free 362.0 MB)
18/06/26 03:09:55 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.0 KB, free 362.0 MB)
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on dc1master-lan1:36696 (size: 4.0 KB, free: 366.2 MB)
18/06/26 03:09:55 INFO spark.SparkContext: Created broadcast 10 from run at ThreadPoolExecutor.java:1142
18/06/26 03:09:55 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 1249.0 KB, free 360.8 MB)
18/06/26 03:09:55 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 286.4 KB, free 360.5 MB)
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on dc1master-lan1:36696 (size: 286.4 KB, free: 365.9 MB)
18/06/26 03:09:55 INFO spark.SparkContext: Created broadcast 11 from run at ThreadPoolExecutor.java:1142
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 139
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 121
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 133
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 126
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 125
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 129
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 131
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 138
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on dc1master-lan1:36696 in memory (size: 5.7 KB, free: 365.9 MB)
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.1.2:33266 in memory (size: 5.7 KB, free: 912.2 MB)
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.1.1:40144 in memory (size: 5.7 KB, free: 912.2 MB)
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 127
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 130
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 145
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 132
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 136
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 134
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 142
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 141
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 124
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 137
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on dc1master-lan1:36696 in memory (size: 5.9 KB, free: 365.9 MB)
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.1.1:40144 in memory (size: 5.9 KB, free: 912.2 MB)
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.1.2:33266 in memory (size: 5.9 KB, free: 912.2 MB)
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 122
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 123
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 128
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on dc1master-lan1:36696 in memory (size: 6.2 KB, free: 365.9 MB)
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.1.1:40144 in memory (size: 6.2 KB, free: 912.2 MB)
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.1.2:33266 in memory (size: 6.2 KB, free: 912.2 MB)
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 143
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 140
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 135
18/06/26 03:09:55 INFO spark.ContextCleaner: Cleaned accumulator 144
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on dc1master-lan1:36696 in memory (size: 6.0 KB, free: 365.9 MB)
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.1.2:33266 in memory (size: 6.0 KB, free: 912.2 MB)
18/06/26 03:09:55 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.1.1:40144 in memory (size: 6.0 KB, free: 912.2 MB)
18/06/26 03:09:56 INFO codegen.CodeGenerator: Code generated in 155.424375 ms
18/06/26 03:09:56 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 291.8 KB, free 360.3 MB)
18/06/26 03:09:56 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 25.2 KB, free 360.3 MB)
18/06/26 03:09:56 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on dc1master-lan1:36696 (size: 25.2 KB, free: 365.9 MB)
18/06/26 03:09:56 INFO spark.SparkContext: Created broadcast 12 from 
18/06/26 03:09:56 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:56 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:56 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:56 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:56 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:09:56 INFO codegen.CodeGenerator: Code generated in 45.40731 ms
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 156
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 153
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 111
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 157
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 117
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 114
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 120
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 97
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 113
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 175
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 176
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 101
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 188
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 172
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 159
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 112
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 177
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 183
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 146
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 110
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 184
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 103
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 100
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 186
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 166
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 182
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 174
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 162
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 158
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 154
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 163
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 99
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 178
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 160
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 106
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 194
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 116
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 149
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 102
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 107
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 147
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 187
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 171
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 193
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 152
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 185
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 167
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 115
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 169
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 148
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 189
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 96
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 104
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 180
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 191
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 179
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 98
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 118
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 105
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 165
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 151
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 109
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 190
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 195
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 164
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 168
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 108
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 173
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 155
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 192
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 119
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 170
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 161
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 150
18/06/26 03:09:56 INFO spark.ContextCleaner: Cleaned accumulator 181
18/06/26 03:09:56 INFO codegen.CodeGenerator: Code generated in 110.357471 ms
18/06/26 03:09:56 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 291.8 KB, free 360.0 MB)
18/06/26 03:09:56 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 25.2 KB, free 359.9 MB)
18/06/26 03:09:56 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on dc1master-lan1:36696 (size: 25.2 KB, free: 365.9 MB)
18/06/26 03:09:56 INFO spark.SparkContext: Created broadcast 13 from 
18/06/26 03:09:56 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:09:56 INFO spark.SparkContext: Starting job: processCmd at CliDriver.java:376
18/06/26 03:09:56 INFO scheduler.DAGScheduler: Registering RDD 37 (processCmd at CliDriver.java:376)
18/06/26 03:09:56 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerShuffle id:1
18/06/26 03:09:56 INFO scheduler.DAGScheduler: Registering RDD 29 (processCmd at CliDriver.java:376)
18/06/26 03:09:56 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerShuffle id:0
18/06/26 03:09:56 INFO scheduler.DAGScheduler: Registering RDD 42 (processCmd at CliDriver.java:376)
18/06/26 03:09:56 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerShuffle id:2
18/06/26 03:09:56 INFO scheduler.DAGScheduler: Got job 4 (processCmd at CliDriver.java:376) with 4 output partitions
18/06/26 03:09:56 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (processCmd at CliDriver.java:376)
18/06/26 03:09:56 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
18/06/26 03:09:56 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 6)
18/06/26 03:09:56 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:09:56 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 220.5 KB, free 359.7 MB)
18/06/26 03:09:56 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 63.0 KB, free 359.7 MB)
18/06/26 03:09:56 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on dc1master-lan1:36696 (size: 63.0 KB, free: 365.8 MB)
18/06/26 03:09:56 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1074
18/06/26 03:09:56 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
18/06/26 03:09:56 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 5 tasks
18/06/26 03:09:56 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:56 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:56 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:09:56 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 8, 10.0.1.1, executor 1, partition 0, ANY, 7909 bytes)
18/06/26 03:09:56 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 9, 10.0.1.2, executor 0, partition 1, ANY, 7909 bytes)
18/06/26 03:09:56 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 10, 10.0.1.1, executor 1, partition 2, ANY, 7909 bytes)
18/06/26 03:09:56 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 4.0 (TID 11, 10.0.1.2, executor 0, partition 3, ANY, 7909 bytes)
18/06/26 03:09:56 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 4.0 (TID 12, 10.0.1.1, executor 1, partition 4, ANY, 7909 bytes)
18/06/26 03:09:56 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:56 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 5
18/06/26 03:09:56 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 219.3 KB, free 359.5 MB)
18/06/26 03:09:56 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 62.7 KB, free 359.4 MB)
18/06/26 03:09:56 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on dc1master-lan1:36696 (size: 62.7 KB, free: 365.7 MB)
18/06/26 03:09:56 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1074
18/06/26 03:09:56 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2))
18/06/26 03:09:56 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 3 tasks
18/06/26 03:09:56 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:09:56 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:09:56 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:09:56 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:56 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:56 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:09:56 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:09:56 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 13, 10.0.1.2, executor 0, partition 0, ANY, 7905 bytes)
18/06/26 03:09:56 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:09:56 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 14, 10.0.1.1, executor 1, partition 1, ANY, 7905 bytes)
18/06/26 03:09:56 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:09:56 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 15, 10.0.1.2, executor 0, partition 2, ANY, 7905 bytes)
18/06/26 03:09:56 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:09:56 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:56 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 3
18/06/26 03:09:56 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.2:33266 (size: 63.0 KB, free: 912.1 MB)
18/06/26 03:09:56 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.2:33266 (size: 62.7 KB, free: 912.1 MB)
18/06/26 03:09:56 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.1:40144 (size: 63.0 KB, free: 912.1 MB)
18/06/26 03:09:56 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.1:40144 (size: 62.7 KB, free: 912.1 MB)
18/06/26 03:09:56 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.2:33266 (size: 25.2 KB, free: 912.1 MB)
18/06/26 03:09:56 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.2:33266 (size: 25.2 KB, free: 912.0 MB)
18/06/26 03:09:56 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.1:40144 (size: 25.2 KB, free: 912.1 MB)
18/06/26 03:09:57 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.1:40144 (size: 25.2 KB, free: 912.0 MB)
18/06/26 03:09:57 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:57 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:57 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:57 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:57 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.2:33266 (size: 546.0 B, free: 912.0 MB)
18/06/26 03:09:57 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.2:33266 (size: 4.0 KB, free: 912.0 MB)
18/06/26 03:09:57 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.2:33266 (size: 286.4 KB, free: 911.7 MB)
18/06/26 03:09:57 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.2:33266 (size: 220.0 B, free: 911.7 MB)
18/06/26 03:09:57 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.1:40144 (size: 546.0 B, free: 912.0 MB)
18/06/26 03:09:57 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.1:40144 (size: 4.0 KB, free: 912.0 MB)
18/06/26 03:09:57 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.1:40144 (size: 286.4 KB, free: 911.7 MB)
18/06/26 03:09:57 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.1:40144 (size: 220.0 B, free: 911.7 MB)
18/06/26 03:09:58 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:58 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:58 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:58 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:58 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:58 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:58 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:58 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:58 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 15) in 1526 ms on 10.0.1.2 (executor 0) (1/3)
18/06/26 03:09:58 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:09:58 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/36/shuffle_0_2_0.data
18/06/26 03:09:58 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/32/shuffle_0_2_0.index
18/06/26 03:09:58 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:58 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:58 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:58 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:58 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 4.0 (TID 12) in 2230 ms on 10.0.1.1 (executor 1) (1/5)
18/06/26 03:09:58 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:09:58 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/19/shuffle_1_4_0.data
18/06/26 03:09:58 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/35/shuffle_1_4_0.index
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:59 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 9) in 2799 ms on 10.0.1.2 (executor 0) (2/5)
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0a/shuffle_1_1_0.data
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/32/shuffle_1_1_0.index
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:59 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 13) in 2798 ms on 10.0.1.2 (executor 0) (2/3)
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_0_0_0.data
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/30/shuffle_0_0_0.index
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:59 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 10) in 2996 ms on 10.0.1.1 (executor 1) (3/5)
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/17/shuffle_1_2_0.data
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0d/shuffle_1_2_0.index
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:59 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 14) in 3018 ms on 10.0.1.1 (executor 1) (3/3)
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/15/shuffle_0_1_0.data
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_0_1_0.index
18/06/26 03:09:59 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:376) finished in 3.031 s
18/06/26 03:09:59 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:09:59 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 4)
18/06/26 03:09:59 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 6, ResultStage 7)
18/06/26 03:09:59 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:59 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 8) in 3053 ms on 10.0.1.1 (executor 1) (4/5)
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/2b/shuffle_1_0_0.data
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_1_0_0.index
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:09:59 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 4.0 (TID 11) in 3054 ms on 10.0.1.2 (executor 0) (5/5)
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/08/shuffle_1_3_0.data
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_1_3_0.index
18/06/26 03:09:59 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (processCmd at CliDriver.java:376) finished in 3.074 s
18/06/26 03:09:59 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:09:59 INFO scheduler.DAGScheduler: running: Set()
18/06/26 03:09:59 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 6, ResultStage 7)
18/06/26 03:09:59 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:09:59 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[42] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 374
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 374
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 372
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 372
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 635
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 635
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 630
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 630
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 606
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 606
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 600
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 600
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 337
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 337
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 250
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 250
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 570
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 570
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 389
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 389
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 536
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 536
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 360
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 360
18/06/26 03:09:59 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 377.0 KB, free 359.0 MB)
18/06/26 03:09:59 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 99.9 KB, free 358.9 MB)
18/06/26 03:09:59 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on dc1master-lan1:36696 (size: 99.9 KB, free: 365.6 MB)
18/06/26 03:09:59 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1074
18/06/26 03:09:59 INFO scheduler.DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[42] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 8 tasks
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:09:59 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 16, 10.0.1.2, executor 0, partition 0, NODE_LOCAL, 7856 bytes)
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:09:59 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 17, 10.0.1.1, executor 1, partition 1, NODE_LOCAL, 7856 bytes)
18/06/26 03:09:59 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 18, 10.0.1.2, executor 0, partition 3, NODE_LOCAL, 7856 bytes)
18/06/26 03:09:59 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 19, 10.0.1.1, executor 1, partition 4, NODE_LOCAL, 7856 bytes)
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:09:59 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 20, 10.0.1.2, executor 0, partition 5, NODE_LOCAL, 7856 bytes)
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 21, 10.0.1.1, executor 1, partition 7, NODE_LOCAL, 7856 bytes)
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 03:09:59 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 22, 10.0.1.2, executor 0, partition 2, PROCESS_LOCAL, 7856 bytes)
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:09:59 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 23, 10.0.1.1, executor 1, partition 6, PROCESS_LOCAL, 7856 bytes)
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:09:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:09:59 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:09:59 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.2:33266 (size: 99.9 KB, free: 911.6 MB)
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.1:40144 (size: 99.9 KB, free: 911.6 MB)
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:09:59 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:09:59 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:54060
18/06/26 03:09:59 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.2:54060
18/06/26 03:09:59 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.1:35648
18/06/26 03:09:59 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:35648
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.2:54060
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:54060
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.1:35648
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:35648
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:09:59 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:10:00 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:00 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:00 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:00 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:00 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.2:54060
18/06/26 03:10:00 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.1:35648
18/06/26 03:10:00 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.2:54060
18/06/26 03:10:00 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.1:35648
18/06/26 03:10:00 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:00 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 373
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 628
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 607
18/06/26 03:10:00 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:00 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 374
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 635
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 606
18/06/26 03:10:00 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 372
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 630
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 600
18/06/26 03:10:00 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 337
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 570
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 536
18/06/26 03:10:00 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 250
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 389
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:00 INFO scheduler.RawMapStatus: 360
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 6.0 (TID 23) in 1521 ms on 10.0.1.1 (executor 1) (1/8)
18/06/26 03:10:01 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:10:01 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/04/shuffle_2_6_0.data
18/06/26 03:10:01 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/08/shuffle_2_6_0.index
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:01 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 6.0 (TID 21, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 40144, None), shuffleId=1, mapId=3, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_1_3_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_1_3_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Task 7.0 in stage 6.0 (TID 21) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:01 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 6.0 (TID 17, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 40144, None), shuffleId=0, mapId=0, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Task 1.0 in stage 6.0 (TID 17) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:01 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 6.0 (TID 19, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 40144, None), shuffleId=1, mapId=3, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_1_3_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_1_3_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Task 4.0 in stage 6.0 (TID 19) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:01 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 6 (processCmd at CliDriver.java:376) as failed due to a fetch failure from ShuffleMapStage 4 (processCmd at CliDriver.java:376)
18/06/26 03:10:01 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:376) failed in 1.582 s due to org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_1_3_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_1_3_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

18/06/26 03:10:01 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 4 (processCmd at CliDriver.java:376) and ShuffleMapStage 6 (processCmd at CliDriver.java:376) due to fetch failure
18/06/26 03:10:01 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 2)
18/06/26 03:10:01 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
18/06/26 03:10:01 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 10.0.1.1, 40144, None)
18/06/26 03:10:01 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor
18/06/26 03:10:01 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 1 (epoch 2)
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 6.0 (TID 22) in 1550 ms on 10.0.1.2 (executor 0) (5/8)
18/06/26 03:10:01 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:10:01 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/38/shuffle_2_2_0.data
18/06/26 03:10:01 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_2_2_0.index
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:01 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 6.0 (TID 18, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 33266, None), shuffleId=0, mapId=1, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Task 3.0 in stage 6.0 (TID 18) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:01 INFO scheduler.DAGScheduler: Executor lost: 0 (epoch 2)
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:01 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:01 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.0.1.2, 33266, None)
18/06/26 03:10:01 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 6.0 (TID 20, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 33266, None), shuffleId=1, mapId=4, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/35/shuffle_1_4_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/35/shuffle_1_4_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:01 INFO storage.BlockManagerMaster: Removed 0 successfully in removeExecutor
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Task 5.0 in stage 6.0 (TID 20) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:01 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 0 (epoch 2)
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:01 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 6.0 (TID 16, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 33266, None), shuffleId=0, mapId=1, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Task 0.0 in stage 6.0 (TID 16) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
18/06/26 03:10:01 INFO scheduler.DAGScheduler: Resubmitting failed stages
18/06/26 03:10:01 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:10:01 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 219.3 KB, free 358.7 MB)
18/06/26 03:10:01 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 62.7 KB, free 358.6 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on dc1master-lan1:36696 (size: 62.7 KB, free: 365.6 MB)
18/06/26 03:10:01 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1074
18/06/26 03:10:01 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2))
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: Adding task set 5.1 with 3 tasks
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:01 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.1 (TID 24, 10.0.1.1, executor 1, partition 0, ANY, 7905 bytes)
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.1 (TID 25, 10.0.1.2, executor 0, partition 1, ANY, 7905 bytes)
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.1 (TID 26, 10.0.1.1, executor 1, partition 2, ANY, 7905 bytes)
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 3
18/06/26 03:10:01 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 220.5 KB, free 358.4 MB)
18/06/26 03:10:01 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 63.1 KB, free 358.4 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on dc1master-lan1:36696 (size: 63.1 KB, free: 365.5 MB)
18/06/26 03:10:01 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1074
18/06/26 03:10:01 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: Adding task set 4.1 with 5 tasks
18/06/26 03:10:01 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:01 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:10:01 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.1 (TID 27, 10.0.1.1, executor 1, partition 0, ANY, 7909 bytes)
18/06/26 03:10:01 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.1 (TID 28, 10.0.1.2, executor 0, partition 1, ANY, 7909 bytes)
18/06/26 03:10:01 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 4.1 (TID 29, 10.0.1.1, executor 1, partition 2, ANY, 7909 bytes)
18/06/26 03:10:01 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 4.1 (TID 30, 10.0.1.2, executor 0, partition 3, ANY, 7909 bytes)
18/06/26 03:10:01 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:10:01 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 4.1 (TID 31, 10.0.1.1, executor 1, partition 4, ANY, 7909 bytes)
18/06/26 03:10:01 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 5
18/06/26 03:10:01 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.2:33266 with 912.3 MB RAM, BlockManagerId(0, 10.0.1.2, 33266, None)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.2:33266 (size: 25.2 KB, free: 912.3 MB)
18/06/26 03:10:01 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.1:40144 with 912.3 MB RAM, BlockManagerId(1, 10.0.1.1, 40144, None)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.2:33266 (size: 220.0 B, free: 912.3 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.2:33266 (size: 63.1 KB, free: 912.2 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.2:33266 (size: 24.9 KB, free: 912.2 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.1:40144 (size: 63.1 KB, free: 912.2 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.2:33266 (size: 24.8 KB, free: 912.2 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.2:33266 (size: 24.8 KB, free: 912.1 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Updated broadcast_18_piece0 in memory on 10.0.1.1:40144 (current size: 63.1 KB, original size: 63.1 KB, free: 912.2 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.2:33266 (size: 25.0 KB, free: 912.1 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.2:33266 (size: 63.0 KB, free: 912.1 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.1:40144 (size: 25.2 KB, free: 912.2 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.2:33266 (size: 99.9 KB, free: 912.0 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.2:33266 (size: 546.0 B, free: 912.0 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.1:40144 (size: 220.0 B, free: 912.2 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.2:33266 (size: 286.4 KB, free: 911.7 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.2:33266 (size: 25.2 KB, free: 911.7 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.1:40144 (size: 24.9 KB, free: 912.2 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.2:33266 (size: 62.7 KB, free: 911.6 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.2:33266 (size: 62.7 KB, free: 911.5 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.2:33266 (size: 4.0 KB, free: 911.5 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.1:40144 (size: 24.8 KB, free: 912.2 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.1:40144 (size: 24.8 KB, free: 912.1 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.1:40144 (size: 25.0 KB, free: 912.1 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.1:40144 (size: 63.0 KB, free: 912.1 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.1:40144 (size: 99.9 KB, free: 912.0 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.1:40144 (size: 546.0 B, free: 912.0 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.1:40144 (size: 286.4 KB, free: 911.7 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.1:40144 (size: 25.2 KB, free: 911.7 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.1:40144 (size: 62.7 KB, free: 911.6 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.1:40144 (size: 62.7 KB, free: 911.5 MB)
18/06/26 03:10:01 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.1:40144 (size: 4.0 KB, free: 911.5 MB)
18/06/26 03:10:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:02 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.1 (TID 26) in 549 ms on 10.0.1.1 (executor 1) (1/3)
18/06/26 03:10:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:10:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/36/shuffle_0_2_0.data
18/06/26 03:10:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/32/shuffle_0_2_0.index
18/06/26 03:10:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:02 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 4.1 (TID 31) in 1131 ms on 10.0.1.1 (executor 1) (1/5)
18/06/26 03:10:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:10:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/19/shuffle_1_4_0.data
18/06/26 03:10:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/35/shuffle_1_4_0.index
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:03 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.1 (TID 25) in 1793 ms on 10.0.1.2 (executor 0) (2/3)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/15/shuffle_0_1_0.data
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0f/shuffle_0_1_0.index
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:03 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.1 (TID 28) in 1794 ms on 10.0.1.2 (executor 0) (2/5)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0a/shuffle_1_1_0.data
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/32/shuffle_1_1_0.index
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:03 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 4.1 (TID 29) in 1849 ms on 10.0.1.1 (executor 1) (3/5)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/17/shuffle_1_2_0.data
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0d/shuffle_1_2_0.index
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:03 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.1 (TID 27) in 1867 ms on 10.0.1.1 (executor 1) (4/5)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/2b/shuffle_1_0_0.data
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_1_0_0.index
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:03 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 4.1 (TID 30) in 1878 ms on 10.0.1.2 (executor 0) (5/5)
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.1, whose tasks have all completed, from pool 
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/08/shuffle_1_3_0.data
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_1_3_0.index
18/06/26 03:10:03 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (processCmd at CliDriver.java:376) finished in 1.890 s
18/06/26 03:10:03 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:10:03 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 5)
18/06/26 03:10:03 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 6, ResultStage 7)
18/06/26 03:10:03 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:03 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.1 (TID 24) in 2028 ms on 10.0.1.1 (executor 1) (3/3)
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.1, whose tasks have all completed, from pool 
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0c/shuffle_0_0_0.data
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/30/shuffle_0_0_0.index
18/06/26 03:10:03 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:376) finished in 2.039 s
18/06/26 03:10:03 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:10:03 INFO scheduler.DAGScheduler: running: Set()
18/06/26 03:10:03 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 6, ResultStage 7)
18/06/26 03:10:03 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:10:03 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[42] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 374
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 374
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 372
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 372
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 635
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 635
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 630
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 630
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 606
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 606
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 600
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 600
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 337
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 337
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 250
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 250
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 570
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 570
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 389
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 389
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 536
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 536
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 360
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 360
18/06/26 03:10:03 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 377.0 KB, free 358.0 MB)
18/06/26 03:10:03 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 100.0 KB, free 357.9 MB)
18/06/26 03:10:03 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on dc1master-lan1:36696 (size: 100.0 KB, free: 365.4 MB)
18/06/26 03:10:03 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1074
18/06/26 03:10:03 INFO scheduler.DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[42] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: Adding task set 6.1 with 8 tasks
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:03 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.1 (TID 32, 10.0.1.1, executor 1, partition 0, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 03:10:03 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.1 (TID 33, 10.0.1.2, executor 0, partition 1, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.1 (TID 34, 10.0.1.1, executor 1, partition 3, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.1 (TID 35, 10.0.1.2, executor 0, partition 4, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 03:10:03 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.1 (TID 36, 10.0.1.1, executor 1, partition 5, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.1 (TID 37, 10.0.1.2, executor 0, partition 7, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.1 (TID 38, 10.0.1.1, executor 1, partition 2, PROCESS_LOCAL, 7856 bytes)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.1 (TID 39, 10.0.1.2, executor 0, partition 6, PROCESS_LOCAL, 7856 bytes)
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 8
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.1:40144 (size: 100.0 KB, free: 911.4 MB)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.2:33266 (size: 100.0 KB, free: 911.4 MB)
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:10:03 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:03 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:35648
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:35648
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:10:03 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.1:35648
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.1:35648
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:10:03 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.2:54060
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.2:54060
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:10:03 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:54060
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:54060
18/06/26 03:10:03 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:04 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.1:35648
18/06/26 03:10:04 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.1:35648
18/06/26 03:10:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 373
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 628
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 607
18/06/26 03:10:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 374
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 635
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 606
18/06/26 03:10:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 372
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 630
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 600
18/06/26 03:10:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 337
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 570
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 536
18/06/26 03:10:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 250
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 389
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:04 INFO scheduler.RawMapStatus: 360
18/06/26 03:10:04 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.2:54060
18/06/26 03:10:04 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.2:54060
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:04 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 6.1 (TID 36, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 40144, None), shuffleId=1, mapId=3, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_1_3_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_1_3_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Task 5.0 in stage 6.1 (TID 36) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:04 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 6 (processCmd at CliDriver.java:376) as failed due to a fetch failure from ShuffleMapStage 4 (processCmd at CliDriver.java:376)
18/06/26 03:10:04 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:376) failed in 1.088 s due to org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_1_3_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_1_3_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

18/06/26 03:10:04 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 4 (processCmd at CliDriver.java:376) and ShuffleMapStage 6 (processCmd at CliDriver.java:376) due to fetch failure
18/06/26 03:10:04 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 12)
18/06/26 03:10:04 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
18/06/26 03:10:04 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 10.0.1.1, 40144, None)
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:04 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:04 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 1 (epoch 12)
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 6.1 (TID 38) in 1071 ms on 10.0.1.1 (executor 1) (2/8)
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:04 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 6.1 (TID 35, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 33266, None), shuffleId=1, mapId=4, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/35/shuffle_1_4_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/35/shuffle_1_4_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Task 4.0 in stage 6.1 (TID 35) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:04 INFO scheduler.DAGScheduler: Ignoring possibly bogus ShuffleMapTask(6, 2) completion from executor 1
18/06/26 03:10:04 INFO scheduler.DAGScheduler: Executor lost: 0 (epoch 12)
18/06/26 03:10:04 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
18/06/26 03:10:04 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.0.1.2, 33266, None)
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:04 INFO storage.BlockManagerMaster: Removed 0 successfully in removeExecutor
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:04 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 0 (epoch 12)
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:04 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 6.1 (TID 32, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 40144, None), shuffleId=0, mapId=1, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Task 0.0 in stage 6.1 (TID 32) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 6.1 (TID 39) in 1075 ms on 10.0.1.2 (executor 0) (5/8)
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:04 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 6.1 (TID 34, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 40144, None), shuffleId=0, mapId=1, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Task 3.0 in stage 6.1 (TID 34) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:04 INFO scheduler.DAGScheduler: Ignoring possibly bogus ShuffleMapTask(6, 6) completion from executor 0
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:04 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 6.1 (TID 37, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 33266, None), shuffleId=1, mapId=4, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/35/shuffle_1_4_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/35/shuffle_1_4_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Task 7.0 in stage 6.1 (TID 37) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:04 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 6.1 (TID 33, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 33266, None), shuffleId=0, mapId=0, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Task 1.0 in stage 6.1 (TID 33) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.1, whose tasks have all completed, from pool 
18/06/26 03:10:04 INFO scheduler.DAGScheduler: Resubmitting failed stages
18/06/26 03:10:04 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:10:04 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 219.3 KB, free 357.7 MB)
18/06/26 03:10:04 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 62.7 KB, free 357.6 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on dc1master-lan1:36696 (size: 62.7 KB, free: 365.4 MB)
18/06/26 03:10:04 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1074
18/06/26 03:10:04 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2))
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: Adding task set 5.2 with 3 tasks
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:04 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.2 (TID 40, 10.0.1.1, executor 1, partition 0, ANY, 7905 bytes)
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.2 (TID 41, 10.0.1.2, executor 0, partition 1, ANY, 7905 bytes)
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.2 (TID 42, 10.0.1.1, executor 1, partition 2, ANY, 7905 bytes)
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 3
18/06/26 03:10:04 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 220.5 KB, free 357.4 MB)
18/06/26 03:10:04 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 63.1 KB, free 357.4 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on dc1master-lan1:36696 (size: 63.1 KB, free: 365.3 MB)
18/06/26 03:10:04 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1074
18/06/26 03:10:04 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: Adding task set 4.2 with 5 tasks
18/06/26 03:10:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.2 (TID 43, 10.0.1.2, executor 0, partition 0, ANY, 7909 bytes)
18/06/26 03:10:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.2 (TID 44, 10.0.1.1, executor 1, partition 1, ANY, 7909 bytes)
18/06/26 03:10:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 4.2 (TID 45, 10.0.1.2, executor 0, partition 2, ANY, 7909 bytes)
18/06/26 03:10:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 4.2 (TID 46, 10.0.1.1, executor 1, partition 3, ANY, 7909 bytes)
18/06/26 03:10:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:10:04 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 4.2 (TID 47, 10.0.1.2, executor 0, partition 4, ANY, 7909 bytes)
18/06/26 03:10:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 5
18/06/26 03:10:04 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.1:40144 with 912.3 MB RAM, BlockManagerId(1, 10.0.1.1, 40144, None)
18/06/26 03:10:04 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.2:33266 with 912.3 MB RAM, BlockManagerId(0, 10.0.1.2, 33266, None)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.1:40144 (size: 63.1 KB, free: 912.2 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.2:33266 (size: 63.1 KB, free: 912.2 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.1:40144 (size: 25.2 KB, free: 912.2 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.1:40144 (size: 63.1 KB, free: 912.2 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.1:40144 (size: 62.7 KB, free: 912.1 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.2:33266 (size: 25.2 KB, free: 912.2 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.2:33266 (size: 63.1 KB, free: 912.2 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.1:40144 (size: 220.0 B, free: 912.1 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.2:33266 (size: 62.7 KB, free: 912.1 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.1:40144 (size: 24.9 KB, free: 912.1 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.1:40144 (size: 24.8 KB, free: 912.0 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.1:40144 (size: 24.8 KB, free: 912.0 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.1:40144 (size: 25.0 KB, free: 912.0 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.1:40144 (size: 63.0 KB, free: 911.9 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.2:33266 (size: 220.0 B, free: 912.1 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.1:40144 (size: 100.0 KB, free: 911.8 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.1:40144 (size: 99.9 KB, free: 911.7 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.2:33266 (size: 24.9 KB, free: 912.1 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.1:40144 (size: 546.0 B, free: 911.7 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.2:33266 (size: 24.8 KB, free: 912.0 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.2:33266 (size: 24.8 KB, free: 912.0 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.1:40144 (size: 286.4 KB, free: 911.5 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.1:40144 (size: 25.2 KB, free: 911.4 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.2:33266 (size: 25.0 KB, free: 912.0 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.1:40144 (size: 62.7 KB, free: 911.4 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.1:40144 (size: 62.7 KB, free: 911.3 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.1:40144 (size: 4.0 KB, free: 911.3 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.2:33266 (size: 63.0 KB, free: 911.9 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.2:33266 (size: 100.0 KB, free: 911.8 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.2:33266 (size: 99.9 KB, free: 911.7 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.2:33266 (size: 546.0 B, free: 911.7 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.2:33266 (size: 286.4 KB, free: 911.5 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.2:33266 (size: 25.2 KB, free: 911.4 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.2:33266 (size: 62.7 KB, free: 911.4 MB)
18/06/26 03:10:04 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.2:33266 (size: 62.7 KB, free: 911.3 MB)
18/06/26 03:10:05 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.2:33266 (size: 4.0 KB, free: 911.3 MB)
18/06/26 03:10:05 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:05 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:05 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:05 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:05 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:05 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:05 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:05 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:05 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.2 (TID 42) in 422 ms on 10.0.1.1 (executor 1) (1/3)
18/06/26 03:10:05 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:10:05 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/36/shuffle_0_2_0.data
18/06/26 03:10:05 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/32/shuffle_0_2_0.index
18/06/26 03:10:05 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:05 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:05 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:05 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:05 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 4.2 (TID 47) in 1063 ms on 10.0.1.2 (executor 0) (1/5)
18/06/26 03:10:05 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:10:05 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/19/shuffle_1_4_0.data
18/06/26 03:10:05 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/35/shuffle_1_4_0.index
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:06 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.2 (TID 43) in 1835 ms on 10.0.1.2 (executor 0) (2/5)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/2b/shuffle_1_0_0.data
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0f/shuffle_1_0_0.index
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:06 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.2 (TID 44) in 1846 ms on 10.0.1.1 (executor 1) (3/5)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0a/shuffle_1_1_0.data
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/32/shuffle_1_1_0.index
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:06 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.2 (TID 40) in 1861 ms on 10.0.1.1 (executor 1) (2/3)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0c/shuffle_0_0_0.data
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/30/shuffle_0_0_0.index
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:06 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 4.2 (TID 45) in 1851 ms on 10.0.1.2 (executor 0) (4/5)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/17/shuffle_1_2_0.data
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0d/shuffle_1_2_0.index
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:06 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.2 (TID 41) in 1904 ms on 10.0.1.2 (executor 0) (3/3)
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.2, whose tasks have all completed, from pool 
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/15/shuffle_0_1_0.data
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0f/shuffle_0_1_0.index
18/06/26 03:10:06 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:376) finished in 1.916 s
18/06/26 03:10:06 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:10:06 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 4)
18/06/26 03:10:06 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 6, ResultStage 7)
18/06/26 03:10:06 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:06 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 4.2 (TID 46) in 1971 ms on 10.0.1.1 (executor 1) (5/5)
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.2, whose tasks have all completed, from pool 
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/08/shuffle_1_3_0.data
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0c/shuffle_1_3_0.index
18/06/26 03:10:06 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (processCmd at CliDriver.java:376) finished in 1.982 s
18/06/26 03:10:06 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:10:06 INFO scheduler.DAGScheduler: running: Set()
18/06/26 03:10:06 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 6, ResultStage 7)
18/06/26 03:10:06 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:10:06 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[42] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 374
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 374
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 372
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 372
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 635
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 635
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 630
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 630
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 606
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 606
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 600
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 600
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 337
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 337
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 250
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 250
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 570
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 570
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 389
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 389
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 536
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 536
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 360
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 360
18/06/26 03:10:06 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 377.0 KB, free 357.0 MB)
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 220
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 274
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 293
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 271
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 238
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 301
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 277
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 281
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 215
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 226
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 225
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 342
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 264
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 313
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 202
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 209
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 257
18/06/26 03:10:06 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 100.0 KB, free 356.9 MB)
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on dc1master-lan1:36696 (size: 100.0 KB, free: 365.2 MB)
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on dc1master-lan1:36696 in memory (size: 62.7 KB, free: 365.3 MB)
18/06/26 03:10:06 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1074
18/06/26 03:10:06 INFO scheduler.DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[42] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: Adding task set 6.2 with 8 tasks
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.1.2:33266 in memory (size: 62.7 KB, free: 911.4 MB)
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.1.1:40144 in memory (size: 62.7 KB, free: 911.4 MB)
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:06 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.2 (TID 48, 10.0.1.2, executor 0, partition 0, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.2 (TID 49, 10.0.1.1, executor 1, partition 1, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 03:10:06 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.2 (TID 50, 10.0.1.2, executor 0, partition 3, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.2 (TID 51, 10.0.1.1, executor 1, partition 4, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.2 (TID 52, 10.0.1.2, executor 0, partition 5, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.2 (TID 53, 10.0.1.1, executor 1, partition 7, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 268
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.2 (TID 54, 10.0.1.2, executor 0, partition 2, PROCESS_LOCAL, 7856 bytes)
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 224
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 201
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 284
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.2 (TID 55, 10.0.1.1, executor 1, partition 6, PROCESS_LOCAL, 7856 bytes)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 219
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 262
18/06/26 03:10:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 8
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 273
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 267
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 325
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 222
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 276
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 246
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 330
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 295
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 287
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 253
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 228
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 322
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 235
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 197
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 221
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 218
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 251
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 234
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on dc1master-lan1:36696 in memory (size: 62.7 KB, free: 365.3 MB)
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:06 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.1.2:33266 in memory (size: 62.7 KB, free: 911.4 MB)
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.1.1:40144 in memory (size: 62.7 KB, free: 911.4 MB)
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 318
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 270
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 211
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 282
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 320
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 296
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 344
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 328
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 231
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 279
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 245
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 227
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 309
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 306
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 305
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 285
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 310
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 256
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 297
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 239
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 280
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 269
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 196
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 338
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 337
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 247
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 317
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.2:33266 (size: 100.0 KB, free: 911.3 MB)
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on dc1master-lan1:36696 in memory (size: 63.1 KB, free: 365.4 MB)
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.1.2:33266 in memory (size: 63.1 KB, free: 911.4 MB)
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.1.1:40144 in memory (size: 63.1 KB, free: 911.5 MB)
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.1:40144 (size: 100.0 KB, free: 911.4 MB)
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 292
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 302
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 229
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 331
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 258
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 288
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 208
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 217
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 236
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 232
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 332
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on dc1master-lan1:36696 in memory (size: 100.0 KB, free: 365.5 MB)
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.1.2:33266 in memory (size: 100.0 KB, free: 911.5 MB)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.1.1:40144 in memory (size: 100.0 KB, free: 911.5 MB)
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:06 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:54060
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:54060
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:10:06 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.1:35648
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.1:35648
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 303
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 265
18/06/26 03:10:06 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 311
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 319
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 255
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 241
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on dc1master-lan1:36696 in memory (size: 99.9 KB, free: 365.6 MB)
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.1.1:40144 in memory (size: 99.9 KB, free: 911.6 MB)
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.1.2:33266 in memory (size: 99.9 KB, free: 911.6 MB)
18/06/26 03:10:06 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.2:54060
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.2:54060
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:10:06 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:35648
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 286
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:35648
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 199
18/06/26 03:10:06 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 339
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 321
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 207
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 312
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 323
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 289
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 237
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 242
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 327
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 216
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 249
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 261
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 308
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 275
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.1.2:33266 in memory (size: 63.0 KB, free: 911.6 MB)
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.1.1:40144 in memory (size: 63.0 KB, free: 911.6 MB)
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on dc1master-lan1:36696 in memory (size: 63.0 KB, free: 365.6 MB)
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 294
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 341
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 334
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 248
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 198
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 206
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 291
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 324
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 204
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 214
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 300
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 252
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 259
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 336
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 326
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 333
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 315
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 329
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 243
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 244
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 343
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 233
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on dc1master-lan1:36696 in memory (size: 62.7 KB, free: 365.7 MB)
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.1.1:40144 in memory (size: 62.7 KB, free: 911.7 MB)
18/06/26 03:10:06 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.1.2:33266 in memory (size: 62.7 KB, free: 911.7 MB)
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 345
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 298
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 203
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 307
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 223
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 304
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 205
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 316
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 272
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 230
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 278
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 260
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 263
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 290
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 254
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 210
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 200
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 212
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 240
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 299
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 314
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 266
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 250
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 340
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 213
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 335
18/06/26 03:10:06 INFO spark.ContextCleaner: Cleaned accumulator 283
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:07 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.2:54060
18/06/26 03:10:07 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 373
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 628
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 607
18/06/26 03:10:07 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 374
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 635
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 606
18/06/26 03:10:07 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 372
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 630
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 600
18/06/26 03:10:07 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.1:35648
18/06/26 03:10:07 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 337
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 570
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 536
18/06/26 03:10:07 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 250
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 389
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:07 INFO scheduler.RawMapStatus: 360
18/06/26 03:10:07 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.2:54060
18/06/26 03:10:07 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.1:35648
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:07 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 6.2 (TID 48, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 33266, None), shuffleId=0, mapId=0, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:07 INFO scheduler.TaskSetManager: Task 0.0 in stage 6.2 (TID 48) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:07 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 6 (processCmd at CliDriver.java:376) as failed due to a fetch failure from ShuffleMapStage 5 (processCmd at CliDriver.java:376)
18/06/26 03:10:07 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:376) failed in 1.101 s due to org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

18/06/26 03:10:07 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 5 (processCmd at CliDriver.java:376) and ShuffleMapStage 6 (processCmd at CliDriver.java:376) due to fetch failure
18/06/26 03:10:07 INFO scheduler.DAGScheduler: Executor lost: 0 (epoch 22)
18/06/26 03:10:07 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
18/06/26 03:10:07 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.0.1.2, 33266, None)
18/06/26 03:10:07 INFO storage.BlockManagerMaster: Removed 0 successfully in removeExecutor
18/06/26 03:10:07 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 0 (epoch 22)
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:07 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 6.2 (TID 55) in 1061 ms on 10.0.1.1 (executor 1) (2/8)
18/06/26 03:10:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:10:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/04/shuffle_2_6_0.data
18/06/26 03:10:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/08/shuffle_2_6_0.index
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:07 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 6.2 (TID 50, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 33266, None), shuffleId=0, mapId=0, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:07 INFO scheduler.TaskSetManager: Task 3.0 in stage 6.2 (TID 50) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:07 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 6.2 (TID 49, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 40144, None), shuffleId=0, mapId=1, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:07 INFO scheduler.TaskSetManager: Task 1.0 in stage 6.2 (TID 49) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:07 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 22)
18/06/26 03:10:07 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
18/06/26 03:10:07 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 10.0.1.1, 40144, None)
18/06/26 03:10:07 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor
18/06/26 03:10:07 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 1 (epoch 22)
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:08 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 6.2 (TID 54) in 1078 ms on 10.0.1.2 (executor 0) (5/8)
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:08 INFO scheduler.DAGScheduler: Ignoring possibly bogus ShuffleMapTask(6, 2) completion from executor 0
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:08 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 6.2 (TID 53, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 40144, None), shuffleId=1, mapId=4, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/35/shuffle_1_4_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/35/shuffle_1_4_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:08 INFO scheduler.TaskSetManager: Task 7.0 in stage 6.2 (TID 53) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:08 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 6.2 (TID 52, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 33266, None), shuffleId=1, mapId=3, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0c/shuffle_1_3_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0c/shuffle_1_3_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:08 INFO scheduler.TaskSetManager: Task 5.0 in stage 6.2 (TID 52) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:08 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 6.2 (TID 51, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 40144, None), shuffleId=1, mapId=4, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/35/shuffle_1_4_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/35/shuffle_1_4_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:08 INFO scheduler.TaskSetManager: Task 4.0 in stage 6.2 (TID 51) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.2, whose tasks have all completed, from pool 
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:08 INFO scheduler.DAGScheduler: Resubmitting failed stages
18/06/26 03:10:08 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:10:08 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 219.3 KB, free 359.0 MB)
18/06/26 03:10:08 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 62.8 KB, free 358.9 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on dc1master-lan1:36696 (size: 62.8 KB, free: 365.6 MB)
18/06/26 03:10:08 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1074
18/06/26 03:10:08 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2))
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: Adding task set 5.3 with 3 tasks
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:08 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:10:08 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.3 (TID 56, 10.0.1.2, executor 0, partition 0, ANY, 7905 bytes)
18/06/26 03:10:08 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.3 (TID 57, 10.0.1.1, executor 1, partition 1, ANY, 7905 bytes)
18/06/26 03:10:08 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.3 (TID 58, 10.0.1.2, executor 0, partition 2, ANY, 7905 bytes)
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 3
18/06/26 03:10:08 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 220.5 KB, free 358.7 MB)
18/06/26 03:10:08 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 63.1 KB, free 358.6 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on dc1master-lan1:36696 (size: 63.1 KB, free: 365.6 MB)
18/06/26 03:10:08 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1074
18/06/26 03:10:08 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: Adding task set 4.3 with 5 tasks
18/06/26 03:10:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:10:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:10:08 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.3 (TID 59, 10.0.1.1, executor 1, partition 0, ANY, 7909 bytes)
18/06/26 03:10:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:10:08 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.3 (TID 60, 10.0.1.2, executor 0, partition 1, ANY, 7909 bytes)
18/06/26 03:10:08 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.1:40144 with 912.3 MB RAM, BlockManagerId(1, 10.0.1.1, 40144, None)
18/06/26 03:10:08 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 4.3 (TID 61, 10.0.1.1, executor 1, partition 2, ANY, 7909 bytes)
18/06/26 03:10:08 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 4.3 (TID 62, 10.0.1.2, executor 0, partition 3, ANY, 7909 bytes)
18/06/26 03:10:08 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.2:33266 with 912.3 MB RAM, BlockManagerId(0, 10.0.1.2, 33266, None)
18/06/26 03:10:08 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 4.3 (TID 63, 10.0.1.1, executor 1, partition 4, ANY, 7909 bytes)
18/06/26 03:10:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 5
18/06/26 03:10:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:10:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.1:40144 (size: 25.2 KB, free: 912.3 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.2:33266 (size: 25.2 KB, free: 912.3 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.1:40144 (size: 220.0 B, free: 912.3 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.2:33266 (size: 220.0 B, free: 912.3 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.1:40144 (size: 63.1 KB, free: 912.2 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.2:33266 (size: 63.1 KB, free: 912.2 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.1:40144 (size: 24.9 KB, free: 912.2 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.2:33266 (size: 24.9 KB, free: 912.2 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.1.1:40144 (size: 63.1 KB, free: 912.1 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.1:40144 (size: 24.8 KB, free: 912.1 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.2:33266 (size: 24.8 KB, free: 912.2 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.1:40144 (size: 24.8 KB, free: 912.1 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.1.2:33266 (size: 63.1 KB, free: 912.1 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.2:33266 (size: 24.8 KB, free: 912.1 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.2:33266 (size: 100.0 KB, free: 912.0 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.2:33266 (size: 25.0 KB, free: 912.0 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.1:40144 (size: 100.0 KB, free: 912.0 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.2:33266 (size: 546.0 B, free: 912.0 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.1:40144 (size: 25.0 KB, free: 912.0 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.2:33266 (size: 286.4 KB, free: 911.7 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.1:40144 (size: 546.0 B, free: 912.0 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.1:40144 (size: 286.4 KB, free: 911.7 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.1:40144 (size: 25.2 KB, free: 911.7 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.1:40144 (size: 4.0 KB, free: 911.6 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.2:33266 (size: 25.2 KB, free: 911.7 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.2:33266 (size: 4.0 KB, free: 911.6 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.1.1:40144 (size: 62.8 KB, free: 911.6 MB)
18/06/26 03:10:08 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.1.2:33266 (size: 62.8 KB, free: 911.6 MB)
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:08 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.3 (TID 58) in 395 ms on 10.0.1.2 (executor 0) (1/3)
18/06/26 03:10:08 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:10:08 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/36/shuffle_0_2_0.data
18/06/26 03:10:08 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/32/shuffle_0_2_0.index
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:09 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 4.3 (TID 63) in 947 ms on 10.0.1.1 (executor 1) (1/5)
18/06/26 03:10:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:10:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/19/shuffle_1_4_0.data
18/06/26 03:10:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/35/shuffle_1_4_0.index
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:09 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 4.3 (TID 61) in 1709 ms on 10.0.1.1 (executor 1) (2/5)
18/06/26 03:10:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:10:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/17/shuffle_1_2_0.data
18/06/26 03:10:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0d/shuffle_1_2_0.index
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:09 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.3 (TID 59) in 1728 ms on 10.0.1.1 (executor 1) (3/5)
18/06/26 03:10:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:10:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/2b/shuffle_1_0_0.data
18/06/26 03:10:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_1_0_0.index
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:09 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.3 (TID 57) in 1781 ms on 10.0.1.1 (executor 1) (2/3)
18/06/26 03:10:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:10:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/15/shuffle_0_1_0.data
18/06/26 03:10:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_0_1_0.index
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:09 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.3 (TID 60) in 1787 ms on 10.0.1.2 (executor 0) (4/5)
18/06/26 03:10:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:10:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0a/shuffle_1_1_0.data
18/06/26 03:10:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/32/shuffle_1_1_0.index
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:10 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.3 (TID 56) in 1828 ms on 10.0.1.2 (executor 0) (3/3)
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.3, whose tasks have all completed, from pool 
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_0_0_0.data
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/30/shuffle_0_0_0.index
18/06/26 03:10:10 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:376) finished in 1.837 s
18/06/26 03:10:10 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:10:10 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 4)
18/06/26 03:10:10 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 6, ResultStage 7)
18/06/26 03:10:10 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:10 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 4.3 (TID 62) in 1901 ms on 10.0.1.2 (executor 0) (5/5)
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.3, whose tasks have all completed, from pool 
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/08/shuffle_1_3_0.data
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_1_3_0.index
18/06/26 03:10:10 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (processCmd at CliDriver.java:376) finished in 1.912 s
18/06/26 03:10:10 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:10:10 INFO scheduler.DAGScheduler: running: Set()
18/06/26 03:10:10 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 6, ResultStage 7)
18/06/26 03:10:10 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:10:10 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[42] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 374
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 374
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 372
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 372
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 635
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 635
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 630
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 630
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 606
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 606
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 600
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 600
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 337
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 337
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 250
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 250
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 570
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 570
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 389
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 389
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 536
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 536
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 360
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 360
18/06/26 03:10:10 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 377.0 KB, free 358.3 MB)
18/06/26 03:10:10 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 100.0 KB, free 358.2 MB)
18/06/26 03:10:10 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on dc1master-lan1:36696 (size: 100.0 KB, free: 365.5 MB)
18/06/26 03:10:10 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1074
18/06/26 03:10:10 INFO scheduler.DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[42] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: Adding task set 6.3 with 8 tasks
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:10 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.3 (TID 64, 10.0.1.1, executor 1, partition 0, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 03:10:10 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.3 (TID 65, 10.0.1.2, executor 0, partition 1, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 03:10:10 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.3 (TID 66, 10.0.1.1, executor 1, partition 3, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:10 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.3 (TID 67, 10.0.1.2, executor 0, partition 4, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.3 (TID 68, 10.0.1.1, executor 1, partition 5, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.3 (TID 69, 10.0.1.2, executor 0, partition 7, NODE_LOCAL, 7856 bytes)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:10 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.3 (TID 70, 10.0.1.1, executor 1, partition 2, PROCESS_LOCAL, 7856 bytes)
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.3 (TID 71, 10.0.1.2, executor 0, partition 6, PROCESS_LOCAL, 7856 bytes)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 03:10:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 8
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626030946-0058
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.1.1:40144 (size: 100.0 KB, free: 911.5 MB)
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.1.2:33266 (size: 100.0 KB, free: 911.5 MB)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 373
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 628
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 607
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:10:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 03:10:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:10:10 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:35648
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:35648
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:10:10 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.1:35648
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.1:35648
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:10:10 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.2:54060
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.2:54060
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:10:10 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:54060
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:54060
18/06/26 03:10:10 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:11 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.1:35648
18/06/26 03:10:11 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.1:35648
18/06/26 03:10:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 373
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 628
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 607
18/06/26 03:10:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 374
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 635
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 606
18/06/26 03:10:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 372
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 630
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 600
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 337
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 570
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 536
18/06/26 03:10:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 250
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 389
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 0
18/06/26 03:10:11 INFO scheduler.RawMapStatus: 360
18/06/26 03:10:11 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.2:54060
18/06/26 03:10:11 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.2:54060
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:11 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 6.3 (TID 65, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 33266, None), shuffleId=0, mapId=1, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:11 INFO scheduler.TaskSetManager: Task 1.0 in stage 6.3 (TID 65) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:11 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 6 (processCmd at CliDriver.java:376) as failed due to a fetch failure from ShuffleMapStage 5 (processCmd at CliDriver.java:376)
18/06/26 03:10:11 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:376) failed in 1.075 s due to org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:11 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 6.3 (TID 66, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 40144, None), shuffleId=0, mapId=0, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:11 INFO scheduler.TaskSetManager: Task 3.0 in stage 6.3 (TID 66) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:11 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 6.3 (TID 64, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 40144, None), shuffleId=0, mapId=0, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:11 INFO scheduler.TaskSetManager: Task 0.0 in stage 6.3 (TID 64) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:11 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 6.3 (TID 70) in 1065 ms on 10.0.1.1 (executor 1) (4/8)
18/06/26 03:10:11 INFO scheduler.DAGScheduler: Executor lost: 0 (epoch 32)
18/06/26 03:10:11 INFO scheduler.DAGScheduler: Job 4 failed: processCmd at CliDriver.java:376, took 14.537750 s
18/06/26 03:10:11 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:11 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.0.1.2, 33266, None)
18/06/26 03:10:11 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 6.3 (TID 68, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 40144, None), shuffleId=1, mapId=3, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_1_3_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-009d59ba-f5ea-4729-b354-42fe6afabd29/blockmgr-496036f3-d660-48ea-ab94-15a06ac30658/0c/shuffle_1_3_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:11 INFO storage.BlockManagerMaster: Removed 0 successfully in removeExecutor
18/06/26 03:10:11 INFO scheduler.TaskSetManager: Task 5.0 in stage 6.3 (TID 68) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:11 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 0 (epoch 32)
18/06/26 03:10:11 ERROR thriftserver.SparkSQLDriver: Failed in [
select   
         w_warehouse_name
 	,w_warehouse_sq_ft
 	,w_city
 	,w_county
 	,w_state
 	,w_country
        ,ship_carriers
        ,year
 	,sum(jan_sales) as jan_sales
 	,sum(feb_sales) as feb_sales
 	,sum(mar_sales) as mar_sales
 	,sum(apr_sales) as apr_sales
 	,sum(may_sales) as may_sales
 	,sum(jun_sales) as jun_sales
 	,sum(jul_sales) as jul_sales
 	,sum(aug_sales) as aug_sales
 	,sum(sep_sales) as sep_sales
 	,sum(oct_sales) as oct_sales
 	,sum(nov_sales) as nov_sales
 	,sum(dec_sales) as dec_sales
 	,sum(jan_sales/w_warehouse_sq_ft) as jan_sales_per_sq_foot
 	,sum(feb_sales/w_warehouse_sq_ft) as feb_sales_per_sq_foot
 	,sum(mar_sales/w_warehouse_sq_ft) as mar_sales_per_sq_foot
 	,sum(apr_sales/w_warehouse_sq_ft) as apr_sales_per_sq_foot
 	,sum(may_sales/w_warehouse_sq_ft) as may_sales_per_sq_foot
 	,sum(jun_sales/w_warehouse_sq_ft) as jun_sales_per_sq_foot
 	,sum(jul_sales/w_warehouse_sq_ft) as jul_sales_per_sq_foot
 	,sum(aug_sales/w_warehouse_sq_ft) as aug_sales_per_sq_foot
 	,sum(sep_sales/w_warehouse_sq_ft) as sep_sales_per_sq_foot
 	,sum(oct_sales/w_warehouse_sq_ft) as oct_sales_per_sq_foot
 	,sum(nov_sales/w_warehouse_sq_ft) as nov_sales_per_sq_foot
 	,sum(dec_sales/w_warehouse_sq_ft) as dec_sales_per_sq_foot
 	,sum(jan_net) as jan_net
 	,sum(feb_net) as feb_net
 	,sum(mar_net) as mar_net
 	,sum(apr_net) as apr_net
 	,sum(may_net) as may_net
 	,sum(jun_net) as jun_net
 	,sum(jul_net) as jul_net
 	,sum(aug_net) as aug_net
 	,sum(sep_net) as sep_net
 	,sum(oct_net) as oct_net
 	,sum(nov_net) as nov_net
 	,sum(dec_net) as dec_net
 from (
    select 
 	w_warehouse_name
 	,w_warehouse_sq_ft
 	,w_city
 	,w_county
 	,w_state
 	,w_country
 	,concat('DIAMOND', ',', 'AIRBORNE') as ship_carriers
        ,d_year as year
 	,sum(case when d_moy = 1 
 		then ws_sales_price* ws_quantity else 0 end) as jan_sales
 	,sum(case when d_moy = 2 
 		then ws_sales_price* ws_quantity else 0 end) as feb_sales
 	,sum(case when d_moy = 3 
 		then ws_sales_price* ws_quantity else 0 end) as mar_sales
 	,sum(case when d_moy = 4 
 		then ws_sales_price* ws_quantity else 0 end) as apr_sales
 	,sum(case when d_moy = 5 
 		then ws_sales_price* ws_quantity else 0 end) as may_sales
 	,sum(case when d_moy = 6 
 		then ws_sales_price* ws_quantity else 0 end) as jun_sales
 	,sum(case when d_moy = 7 
 		then ws_sales_price* ws_quantity else 0 end) as jul_sales
 	,sum(case when d_moy = 8 
 		then ws_sales_price* ws_quantity else 0 end) as aug_sales
 	,sum(case when d_moy = 9 
 		then ws_sales_price* ws_quantity else 0 end) as sep_sales
 	,sum(case when d_moy = 10 
 		then ws_sales_price* ws_quantity else 0 end) as oct_sales
 	,sum(case when d_moy = 11
 		then ws_sales_price* ws_quantity else 0 end) as nov_sales
 	,sum(case when d_moy = 12
 		then ws_sales_price* ws_quantity else 0 end) as dec_sales
 	,sum(case when d_moy = 1 
 		then ws_net_paid_inc_tax * ws_quantity else 0 end) as jan_net
 	,sum(case when d_moy = 2
 		then ws_net_paid_inc_tax * ws_quantity else 0 end) as feb_net
 	,sum(case when d_moy = 3 
 		then ws_net_paid_inc_tax * ws_quantity else 0 end) as mar_net
 	,sum(case when d_moy = 4 
 		then ws_net_paid_inc_tax * ws_quantity else 0 end) as apr_net
 	,sum(case when d_moy = 5 
 		then ws_net_paid_inc_tax * ws_quantity else 0 end) as may_net
 	,sum(case when d_moy = 6 
 		then ws_net_paid_inc_tax * ws_quantity else 0 end) as jun_net
 	,sum(case when d_moy = 7 
 		then ws_net_paid_inc_tax * ws_quantity else 0 end) as jul_net
 	,sum(case when d_moy = 8 
 		then ws_net_paid_inc_tax * ws_quantity else 0 end) as aug_net
 	,sum(case when d_moy = 9 
 		then ws_net_paid_inc_tax * ws_quantity else 0 end) as sep_net
 	,sum(case when d_moy = 10 
 		then ws_net_paid_inc_tax * ws_quantity else 0 end) as oct_net
 	,sum(case when d_moy = 11
 		then ws_net_paid_inc_tax * ws_quantity else 0 end) as nov_net
 	,sum(case when d_moy = 12
 		then ws_net_paid_inc_tax * ws_quantity else 0 end) as dec_net
     from
          web_sales
         ,warehouse
         ,date_dim
         ,time_dim
 	  ,ship_mode
     where
            web_sales.ws_warehouse_sk =  warehouse.w_warehouse_sk
        and web_sales.ws_sold_date_sk = date_dim.d_date_sk
        and web_sales.ws_sold_time_sk = time_dim.t_time_sk
 	and web_sales.ws_ship_mode_sk = ship_mode.sm_ship_mode_sk
        and d_year = 2002
 	and t_time between 49530 and 49530+28800 
 	and sm_carrier in ('DIAMOND','AIRBORNE')
     group by 
        w_warehouse_name
 	,w_warehouse_sq_ft
 	,w_city
 	,w_county
 	,w_state
 	,w_country
       ,d_year
 union all
    select 
 	w_warehouse_name
 	,w_warehouse_sq_ft
 	,w_city
 	,w_county
 	,w_state
 	,w_country
        ,concat('DIAMOND', ',', 'AIRBORNE') as ship_carriers
       ,d_year as year
 	,sum(case when d_moy = 1 
 		then cs_ext_sales_price* cs_quantity else 0 end) as jan_sales
 	,sum(case when d_moy = 2 
 		then cs_ext_sales_price* cs_quantity else 0 end) as feb_sales
 	,sum(case when d_moy = 3 
 		then cs_ext_sales_price* cs_quantity else 0 end) as mar_sales
 	,sum(case when d_moy = 4 
 		then cs_ext_sales_price* cs_quantity else 0 end) as apr_sales
 	,sum(case when d_moy = 5 
 		then cs_ext_sales_price* cs_quantity else 0 end) as may_sales
 	,sum(case when d_moy = 6 
 		then cs_ext_sales_price* cs_quantity else 0 end) as jun_sales
 	,sum(case when d_moy = 7 
 		then cs_ext_sales_price* cs_quantity else 0 end) as jul_sales
 	,sum(case when d_moy = 8 
 		then cs_ext_sales_price* cs_quantity else 0 end) as aug_sales
 	,sum(case when d_moy = 9 
 		then cs_ext_sales_price* cs_quantity else 0 end) as sep_sales
 	,sum(case when d_moy = 10 
 		then cs_ext_sales_price* cs_quantity else 0 end) as oct_sales
 	,sum(case when d_moy = 11
 		then cs_ext_sales_price* cs_quantity else 0 end) as nov_sales
 	,sum(case when d_moy = 12
 		then cs_ext_sales_price* cs_quantity else 0 end) as dec_sales
 	,sum(case when d_moy = 1 
 		then cs_net_paid_inc_ship_tax * cs_quantity else 0 end) as jan_net
 	,sum(case when d_moy = 2 
 		then cs_net_paid_inc_ship_tax * cs_quantity else 0 end) as feb_net
 	,sum(case when d_moy = 3 
 		then cs_net_paid_inc_ship_tax * cs_quantity else 0 end) as mar_net
 	,sum(case when d_moy = 4 
 		then cs_net_paid_inc_ship_tax * cs_quantity else 0 end) as apr_net
 	,sum(case when d_moy = 5 
 		then cs_net_paid_inc_ship_tax * cs_quantity else 0 end) as may_net
 	,sum(case when d_moy = 6 
 		then cs_net_paid_inc_ship_tax * cs_quantity else 0 end) as jun_net
 	,sum(case when d_moy = 7 
 		then cs_net_paid_inc_ship_tax * cs_quantity else 0 end) as jul_net
 	,sum(case when d_moy = 8 
 		then cs_net_paid_inc_ship_tax * cs_quantity else 0 end) as aug_net
 	,sum(case when d_moy = 9 
 		then cs_net_paid_inc_ship_tax * cs_quantity else 0 end) as sep_net
 	,sum(case when d_moy = 10 
 		then cs_net_paid_inc_ship_tax * cs_quantity else 0 end) as oct_net
 	,sum(case when d_moy = 11
 		then cs_net_paid_inc_ship_tax * cs_quantity else 0 end) as nov_net
 	,sum(case when d_moy = 12
 		then cs_net_paid_inc_ship_tax * cs_quantity else 0 end) as dec_net
     from
          catalog_sales
         ,warehouse
         ,date_dim
         ,time_dim
 	 ,ship_mode
     where
            catalog_sales.cs_warehouse_sk =  warehouse.w_warehouse_sk
        and catalog_sales.cs_sold_date_sk = date_dim.d_date_sk
        and catalog_sales.cs_sold_time_sk = time_dim.t_time_sk
 	and catalog_sales.cs_ship_mode_sk = ship_mode.sm_ship_mode_sk
        and d_year = 2002
 	and t_time between 49530 AND 49530+28800 
 	and sm_carrier in ('DIAMOND','AIRBORNE')
     group by 
        w_warehouse_name
 	,w_warehouse_sq_ft
 	,w_city
 	,w_county
 	,w_state
 	,w_country
       ,d_year
 ) x
 group by 
        w_warehouse_name
 	,w_warehouse_sq_ft
 	,w_city
 	,w_county
 	,w_state
 	,w_country
 	,ship_carriers
       ,year
 order by w_warehouse_name
 limit 100]
org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 6 (processCmd at CliDriver.java:376) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_0_1_0.index 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64) 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30) 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source) 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source) 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_0_1_0.index 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214) 	at java.nio.file.Files.newByteChannel(Files.java:361) 	at java.nio.file.Files.newByteChannel(Files.java:407) 	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241) 	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58) 	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	... 7 more 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1635)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1623)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1622)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1622)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1394)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1853)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1805)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1794)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2030)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2127)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1029)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1011)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1433)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1420)
	at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:324)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:122)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:363)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:198)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:837)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:912)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:923)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:11 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 6.3 (TID 67, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 33266, None), shuffleId=1, mapId=4, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/35/shuffle_1_4_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/35/shuffle_1_4_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:11 INFO scheduler.TaskSetManager: Task 4.0 in stage 6.3 (TID 67) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 6 (processCmd at CliDriver.java:376) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_0_1_0.index 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64) 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30) 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source) 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source) 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/0f/shuffle_0_1_0.index 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214) 	at java.nio.file.Files.newByteChannel(Files.java:361) 	at java.nio.file.Files.newByteChannel(Files.java:407) 	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241) 	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58) 	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	... 7 more 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1635)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1623)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1622)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1622)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1394)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1853)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1805)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1794)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2030)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2127)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1029)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1011)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1433)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1420)
	at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:324)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:122)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:363)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:198)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:837)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:912)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:923)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:11 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 6.3 (TID 69, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 33266, None), shuffleId=1, mapId=4, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/35/shuffle_1_4_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage12.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-14d2a371-53f4-4571-a826-c0a216d8ff46/blockmgr-0b54d82a-5d94-4b82-816c-97ac096e5c63/35/shuffle_1_4_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 03:10:11 INFO scheduler.TaskSetManager: Task 7.0 in stage 6.3 (TID 69) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:10:11 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 6.3 (TID 71) in 1083 ms on 10.0.1.2 (executor 0) (8/8)
18/06/26 03:10:11 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.3, whose tasks have all completed, from pool 
18/06/26 03:10:11 INFO server.AbstractConnector: Stopped Spark@13ca16bf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18/06/26 03:10:11 INFO ui.SparkUI: Stopped Spark web UI at http://dc1master-lan1:4040
18/06/26 03:10:11 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors
18/06/26 03:10:11 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
18/06/26 03:10:11 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/06/26 03:10:11 INFO memory.MemoryStore: MemoryStore cleared
18/06/26 03:10:11 INFO storage.BlockManager: BlockManager stopped
18/06/26 03:10:11 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/06/26 03:10:11 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/06/26 03:10:11 INFO spark.SparkContext: Successfully stopped SparkContext
18/06/26 03:10:11 INFO util.ShutdownHookManager: Shutdown hook called
18/06/26 03:10:11 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-d882858a-46c1-4ffd-8052-6a02188faf52
18/06/26 03:10:11 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3585559a-1436-4131-8f96-7d1f7d40e435
