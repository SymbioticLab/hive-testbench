18/06/26 03:21:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/06/26 03:21:07 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18/06/26 03:21:07 INFO metastore.ObjectStore: ObjectStore, initialize called
18/06/26 03:21:07 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
18/06/26 03:21:07 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored
18/06/26 03:21:09 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
18/06/26 03:21:10 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 03:21:10 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 03:21:10 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 03:21:10 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 03:21:10 INFO DataNucleus.Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18/06/26 03:21:10 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
18/06/26 03:21:10 INFO metastore.ObjectStore: Initialized ObjectStore
18/06/26 03:21:11 INFO metastore.HiveMetaStore: Added admin role in metastore
18/06/26 03:21:11 INFO metastore.HiveMetaStore: Added public role in metastore
18/06/26 03:21:11 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
18/06/26 03:21:11 INFO metastore.HiveMetaStore: 0: get_all_databases
18/06/26 03:21:11 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_all_databases	
18/06/26 03:21:11 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
18/06/26 03:21:11 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
18/06/26 03:21:11 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 03:21:11 INFO metastore.HiveMetaStore: 0: get_functions: db=tpcds_text_2 pat=*
18/06/26 03:21:11 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_functions: db=tpcds_text_2 pat=*	
18/06/26 03:21:11 INFO session.SessionState: Created local directory: /tmp/1d329a00-a526-43dc-a2c3-1ff1dd3cc662_resources
18/06/26 03:21:11 INFO session.SessionState: Created HDFS directory: /tmp/hive/wentingt/1d329a00-a526-43dc-a2c3-1ff1dd3cc662
18/06/26 03:21:11 INFO session.SessionState: Created local directory: /tmp/wentingt/1d329a00-a526-43dc-a2c3-1ff1dd3cc662
18/06/26 03:21:11 INFO session.SessionState: Created HDFS directory: /tmp/hive/wentingt/1d329a00-a526-43dc-a2c3-1ff1dd3cc662/_tmp_space.db
18/06/26 03:21:12 INFO spark.SparkContext: Running Spark version 2.4.0-SNAPSHOT
18/06/26 03:21:12 INFO spark.SparkContext: Submitted application: SparkSQL::10.0.1.253
18/06/26 03:21:12 INFO spark.SecurityManager: Changing view acls to: wentingt
18/06/26 03:21:12 INFO spark.SecurityManager: Changing modify acls to: wentingt
18/06/26 03:21:12 INFO spark.SecurityManager: Changing view acls groups to: 
18/06/26 03:21:12 INFO spark.SecurityManager: Changing modify acls groups to: 
18/06/26 03:21:12 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(wentingt); groups with view permissions: Set(); users  with modify permissions: Set(wentingt); groups with modify permissions: Set()
18/06/26 03:21:12 INFO util.Utils: Successfully started service 'sparkDriver' on port 38754.
18/06/26 03:21:12 INFO spark.SparkEnv: Registering MapOutputTracker
18/06/26 03:21:12 INFO spark.SparkEnv: Registering BlockManagerMaster
18/06/26 03:21:12 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/06/26 03:21:12 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/06/26 03:21:12 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-20959d3f-bb7a-4f9a-bc86-0629fcba8116
18/06/26 03:21:12 INFO memory.MemoryStore: MemoryStore started with capacity 408.9 MB
18/06/26 03:21:12 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/06/26 03:21:12 INFO util.log: Logging initialized @7261ms
18/06/26 03:21:13 INFO server.Server: jetty-9.3.z-SNAPSHOT
18/06/26 03:21:13 INFO server.Server: Started @7330ms
18/06/26 03:21:13 INFO server.AbstractConnector: Started ServerConnector@32e5af53{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18/06/26 03:21:13 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@722787b5{/jobs,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e807e2{/jobs/json,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c995c5d{/jobs/job,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@730bea0{/jobs/job/json,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41a16eb3{/stages,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@677cb96e{/stages/json,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b1252c8{/stages/stage,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51fe7f15{/stages/stage/json,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5873f3f0{/stages/pool,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@684372d0{/stages/pool/json,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63dda940{/storage,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41f964f9{/storage/json,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@652e345{/storage/rdd,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7574d4ad{/storage/rdd/json,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7bede4ea{/environment,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@713999c2{/environment/json,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6060146b{/executors,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33627576{/executors/json,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27bc1d44{/executors/threadDump,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1af677f8{/executors/threadDump/json,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a55fb81{/static,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52f9e8bb{/,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2035d65b{/api,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d1a859c{/jobs/job/kill,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28554ac8{/stages/stage/kill,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://dc1master-lan1:4040
18/06/26 03:21:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 0
18/06/26 03:21:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:13 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://dc1master-lan1:7077...
18/06/26 03:21:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 0
18/06/26 03:21:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:13 INFO client.TransportClientFactory: Successfully created connection to dc1master-lan1/10.0.1.253:7077 after 45 ms (0 ms spent in bootstraps)
18/06/26 03:21:13 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20180626032113-0065
18/06/26 03:21:13 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20180626032113-0065/0 on worker-20180626004527-10.0.1.2-38332 (10.0.1.2:38332) with 10 core(s)
18/06/26 03:21:13 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20180626032113-0065/0 on hostPort 10.0.1.2:38332 with 10 core(s), 2.0 GB RAM
18/06/26 03:21:13 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20180626032113-0065/1 on worker-20180626004527-10.0.1.1-34053 (10.0.1.1:34053) with 10 core(s)
18/06/26 03:21:13 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20180626032113-0065/1 on hostPort 10.0.1.1:34053 with 10 core(s), 2.0 GB RAM
18/06/26 03:21:13 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38214.
18/06/26 03:21:13 INFO netty.NettyBlockTransferService: Server created on dc1master-lan1:38214
18/06/26 03:21:13 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/06/26 03:21:13 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20180626032113-0065/0 is now RUNNING
18/06/26 03:21:13 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20180626032113-0065/1 is now RUNNING
18/06/26 03:21:13 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, dc1master-lan1, 38214, None)
18/06/26 03:21:13 INFO storage.BlockManagerMasterEndpoint: Registering block manager dc1master-lan1:38214 with 408.9 MB RAM, BlockManagerId(driver, dc1master-lan1, 38214, None)
18/06/26 03:21:13 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, dc1master-lan1, 38214, None)
18/06/26 03:21:13 INFO storage.BlockManager: external shuffle service port = 7337
18/06/26 03:21:13 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, dc1master-lan1, 38214, None)
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@774f2992{/metrics/json,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO scheduler.EventLoggingListener: Logging events to hdfs://dc1master:9000/user/wentingt/spark-logs/app-20180626032113-0065
18/06/26 03:21:13 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
18/06/26 03:21:13 INFO internal.SharedState: loading hive config file: file:/users/wentingt/spark-terra/conf/hive-site.xml
18/06/26 03:21:13 INFO internal.SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
18/06/26 03:21:13 INFO internal.SharedState: Warehouse path is '/user/hive/warehouse'.
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a631049{/SQL,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@425b5fe2{/SQL/json,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c815fdc{/SQL/execution,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@769b0752{/SQL/execution/json,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65db548{/static/sql,null,AVAILABLE,@Spark}
18/06/26 03:21:13 INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
18/06/26 03:21:14 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
18/06/26 03:21:14 INFO metastore.HiveMetaStore: 0: get_database: default
18/06/26 03:21:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: default	
18/06/26 03:21:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 0
18/06/26 03:21:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 0
18/06/26 03:21:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:14 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
18/06/26 03:21:14 INFO metastore.HiveMetaStore: 0: get_database: global_temp
18/06/26 03:21:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: global_temp	
18/06/26 03:21:14 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
18/06/26 03:21:14 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:21:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=catalog_sales
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=catalog_sales	
18/06/26 03:21:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 0
18/06/26 03:21:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 0
18/06/26 03:21:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:15 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.1.2:55800) with ID 0
18/06/26 03:21:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:15 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.2:35775 with 912.3 MB RAM, BlockManagerId(0, 10.0.1.2, 35775, None)
18/06/26 03:21:15 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.1.1:58478) with ID 1
18/06/26 03:21:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=item
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=item	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=date_dim
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=date_dim	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=catalog_returns
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=catalog_returns	
18/06/26 03:21:15 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.1:43748 with 912.3 MB RAM, BlockManagerId(1, 10.0.1.1, 43748, None)
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=item
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=item	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=date_dim
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=date_dim	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_returns
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_returns	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=web_sales
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=web_sales	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=item
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=item	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=date_dim
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=date_dim	
18/06/26 03:21:15 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=web_returns
18/06/26 03:21:15 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=web_returns	
18/06/26 03:21:16 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:16 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:16 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:16 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:17 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:17 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:17 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:17 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:17 WARN util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
18/06/26 03:21:18 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:18 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:18 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:18 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:19 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:19 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:19 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:19 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:19 INFO codegen.CodeGenerator: Code generated in 257.624303 ms
18/06/26 03:21:19 INFO codegen.CodeGenerator: Code generated in 28.102159 ms
18/06/26 03:21:19 INFO codegen.CodeGenerator: Code generated in 32.531153 ms
18/06/26 03:21:19 INFO codegen.CodeGenerator: Code generated in 33.249288 ms
18/06/26 03:21:19 INFO codegen.CodeGenerator: Code generated in 37.616094 ms
18/06/26 03:21:19 INFO codegen.CodeGenerator: Code generated in 37.768654 ms
18/06/26 03:21:19 INFO codegen.CodeGenerator: Code generated in 87.661072 ms
18/06/26 03:21:19 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 291.1 KB, free 407.5 MB)
18/06/26 03:21:19 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 291.1 KB, free 407.5 MB)
18/06/26 03:21:19 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 291.0 KB, free 407.5 MB)
18/06/26 03:21:19 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 291.4 KB, free 407.5 MB)
18/06/26 03:21:19 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 291.3 KB, free 407.5 MB)
18/06/26 03:21:19 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.0 KB, free 407.4 MB)
18/06/26 03:21:19 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 25.1 KB, free 407.4 MB)
18/06/26 03:21:19 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.0 KB, free 407.4 MB)
18/06/26 03:21:19 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 25.0 KB, free 407.4 MB)
18/06/26 03:21:19 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 25.0 KB, free 407.4 MB)
18/06/26 03:21:19 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on dc1master-lan1:38214 (size: 25.1 KB, free: 408.9 MB)
18/06/26 03:21:19 INFO spark.SparkContext: Created broadcast 3 from 
18/06/26 03:21:19 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on dc1master-lan1:38214 (size: 25.0 KB, free: 408.9 MB)
18/06/26 03:21:19 INFO spark.SparkContext: Created broadcast 2 from 
18/06/26 03:21:19 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on dc1master-lan1:38214 (size: 25.0 KB, free: 408.8 MB)
18/06/26 03:21:19 INFO spark.SparkContext: Created broadcast 4 from 
18/06/26 03:21:19 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on dc1master-lan1:38214 (size: 25.0 KB, free: 408.8 MB)
18/06/26 03:21:19 INFO spark.SparkContext: Created broadcast 1 from 
18/06/26 03:21:19 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on dc1master-lan1:38214 (size: 25.0 KB, free: 408.8 MB)
18/06/26 03:21:19 INFO spark.SparkContext: Created broadcast 0 from 
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 22
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 11
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 26
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 10
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 15
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 13
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 36
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 12
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 24
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 31
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 14
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 34
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 29
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 20
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 25
18/06/26 03:21:19 INFO spark.ContextCleaner: Cleaned accumulator 8
18/06/26 03:21:19 INFO codegen.CodeGenerator: Code generated in 34.116895 ms
18/06/26 03:21:19 INFO codegen.CodeGenerator: Code generated in 58.919324 ms
18/06/26 03:21:20 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:21:20 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:21:20 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:21:20 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:21:20 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:21:20 INFO codegen.CodeGenerator: Code generated in 89.487546 ms
18/06/26 03:21:20 INFO spark.SparkContext: Starting job: run at ThreadPoolExecutor.java:1142
18/06/26 03:21:20 INFO spark.SparkContext: Starting job: run at ThreadPoolExecutor.java:1142
18/06/26 03:21:20 INFO spark.SparkContext: Starting job: run at ThreadPoolExecutor.java:1142
18/06/26 03:21:20 INFO spark.SparkContext: Starting job: run at ThreadPoolExecutor.java:1142
18/06/26 03:21:20 INFO spark.SparkContext: Starting job: run at ThreadPoolExecutor.java:1142
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Got job 0 (run at ThreadPoolExecutor.java:1142) with 2 output partitions
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1142)
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Missing parents: List()
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[29] at run at ThreadPoolExecutor.java:1142), which has no missing parents
18/06/26 03:21:20 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.9 KB, free 407.3 MB)
18/06/26 03:21:20 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.2 KB, free 407.3 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on dc1master-lan1:38214 (size: 6.2 KB, free: 408.8 MB)
18/06/26 03:21:20 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[29] at run at ThreadPoolExecutor.java:1142) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Got job 3 (run at ThreadPoolExecutor.java:1142) with 2 output partitions
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (run at ThreadPoolExecutor.java:1142)
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Missing parents: List()
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[26] at run at ThreadPoolExecutor.java:1142), which has no missing parents
18/06/26 03:21:20 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.6 KB, free 407.3 MB)
18/06/26 03:21:20 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.9 KB, free 407.3 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on dc1master-lan1:38214 (size: 5.9 KB, free: 408.8 MB)
18/06/26 03:21:20 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[26] at run at ThreadPoolExecutor.java:1142) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
18/06/26 03:21:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.0.1.1, executor 1, partition 0, ANY, 7922 bytes)
18/06/26 03:21:20 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.0.1.2, executor 0, partition 1, ANY, 7922 bytes)
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Got job 4 (run at ThreadPoolExecutor.java:1142) with 2 output partitions
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (run at ThreadPoolExecutor.java:1142)
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Missing parents: List()
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[28] at run at ThreadPoolExecutor.java:1142), which has no missing parents
18/06/26 03:21:20 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 11.6 KB, free 407.3 MB)
18/06/26 03:21:20 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.9 KB, free 407.3 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on dc1master-lan1:38214 (size: 5.9 KB, free: 408.8 MB)
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:20 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[28] at run at ThreadPoolExecutor.java:1142) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
18/06/26 03:21:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, 10.0.1.2, executor 0, partition 0, ANY, 7915 bytes)
18/06/26 03:21:20 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, 10.0.1.1, executor 1, partition 1, ANY, 7915 bytes)
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Got job 2 (run at ThreadPoolExecutor.java:1142) with 2 output partitions
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (run at ThreadPoolExecutor.java:1142)
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Missing parents: List()
18/06/26 03:21:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, 10.0.1.2, executor 0, partition 0, ANY, 7915 bytes)
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[25] at run at ThreadPoolExecutor.java:1142), which has no missing parents
18/06/26 03:21:20 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, 10.0.1.1, executor 1, partition 1, ANY, 7915 bytes)
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 03:21:20 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 13.2 KB, free 407.3 MB)
18/06/26 03:21:20 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.4 KB, free 407.3 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on dc1master-lan1:38214 (size: 6.4 KB, free: 408.8 MB)
18/06/26 03:21:20 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[25] at run at ThreadPoolExecutor.java:1142) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Got job 1 (run at ThreadPoolExecutor.java:1142) with 2 output partitions
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (run at ThreadPoolExecutor.java:1142)
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Missing parents: List()
18/06/26 03:21:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6, 10.0.1.1, executor 1, partition 0, ANY, 7911 bytes)
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[27] at run at ThreadPoolExecutor.java:1142), which has no missing parents
18/06/26 03:21:20 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 7, 10.0.1.2, executor 0, partition 1, ANY, 7911 bytes)
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 03:21:20 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 12.7 KB, free 407.3 MB)
18/06/26 03:21:20 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.2 KB, free 407.3 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on dc1master-lan1:38214 (size: 6.2 KB, free: 408.7 MB)
18/06/26 03:21:20 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:20 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[27] at run at ThreadPoolExecutor.java:1142) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 8, 10.0.1.1, executor 1, partition 0, ANY, 7918 bytes)
18/06/26 03:21:20 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 9, 10.0.1.2, executor 0, partition 1, ANY, 7918 bytes)
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:20 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 03:21:20 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 0
18/06/26 03:21:20 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 0
18/06/26 03:21:20 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 1
18/06/26 03:21:20 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 1
18/06/26 03:21:20 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 2
18/06/26 03:21:20 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 2
18/06/26 03:21:20 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 3
18/06/26 03:21:20 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 3
18/06/26 03:21:20 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:21:20 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.1.1:43748 (size: 5.9 KB, free: 912.3 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.1.2:35775 (size: 5.9 KB, free: 912.3 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.1:43748 (size: 6.2 KB, free: 912.3 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.1:43748 (size: 6.4 KB, free: 912.3 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.1.2:35775 (size: 6.2 KB, free: 912.3 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.1.1:43748 (size: 5.9 KB, free: 912.3 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.1.2:35775 (size: 5.9 KB, free: 912.3 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.1.1:43748 (size: 6.2 KB, free: 912.3 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 912.2 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.2:35775 (size: 6.4 KB, free: 912.3 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.2:35775 (size: 6.2 KB, free: 912.3 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 912.2 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 912.2 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 912.2 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 912.2 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 912.2 MB)
18/06/26 03:21:20 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 912.2 MB)
18/06/26 03:21:21 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 912.1 MB)
18/06/26 03:21:21 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 912.2 MB)
18/06/26 03:21:21 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 912.1 MB)
18/06/26 03:21:21 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:21 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:21 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:21 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 2535 ms on 10.0.1.2 (executor 0) (1/2)
18/06/26 03:21:22 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 2546 ms on 10.0.1.2 (executor 0) (1/2)
18/06/26 03:21:22 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 7) in 2532 ms on 10.0.1.2 (executor 0) (1/2)
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 2533 ms on 10.0.1.1 (executor 1) (2/2)
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
18/06/26 03:21:22 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 2550 ms on 10.0.1.1 (executor 1) (2/2)
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 2546 ms on 10.0.1.1 (executor 1) (2/2)
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
18/06/26 03:21:22 INFO scheduler.DAGScheduler: ResultStage 3 (run at ThreadPoolExecutor.java:1142) finished in 2.550 s
18/06/26 03:21:22 INFO scheduler.DAGScheduler: ResultStage 1 (run at ThreadPoolExecutor.java:1142) finished in 2.603 s
18/06/26 03:21:22 INFO scheduler.DAGScheduler: Job 2 finished: run at ThreadPoolExecutor.java:1142, took 2.710373 s
18/06/26 03:21:22 INFO scheduler.DAGScheduler: Job 3 finished: run at ThreadPoolExecutor.java:1142, took 2.710444 s
18/06/26 03:21:22 INFO scheduler.DAGScheduler: ResultStage 2 (run at ThreadPoolExecutor.java:1142) finished in 2.573 s
18/06/26 03:21:22 INFO scheduler.DAGScheduler: Job 4 finished: run at ThreadPoolExecutor.java:1142, took 2.711834 s
18/06/26 03:21:22 INFO codegen.CodeGenerator: Code generated in 12.142909 ms
18/06/26 03:21:22 INFO codegen.CodeGenerator: Code generated in 13.326955 ms
18/06/26 03:21:22 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 1026.9 KB, free 405.3 MB)
18/06/26 03:21:22 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 1026.9 KB, free 405.3 MB)
18/06/26 03:21:22 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 1152.0 KB, free 404.1 MB)
18/06/26 03:21:22 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.5 KB, free 404.1 MB)
18/06/26 03:21:22 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.5 KB, free 404.1 MB)
18/06/26 03:21:22 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on dc1master-lan1:38214 (size: 3.5 KB, free: 408.7 MB)
18/06/26 03:21:22 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 69.3 KB, free 404.1 MB)
18/06/26 03:21:22 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on dc1master-lan1:38214 (size: 3.5 KB, free: 408.7 MB)
18/06/26 03:21:22 INFO spark.SparkContext: Created broadcast 11 from run at ThreadPoolExecutor.java:1142
18/06/26 03:21:22 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on dc1master-lan1:38214 (size: 69.3 KB, free: 408.7 MB)
18/06/26 03:21:22 INFO spark.SparkContext: Created broadcast 10 from run at ThreadPoolExecutor.java:1142
18/06/26 03:21:22 INFO spark.SparkContext: Created broadcast 12 from run at ThreadPoolExecutor.java:1142
18/06/26 03:21:22 INFO storage.BlockManagerInfo: Added taskresult_8 in memory on 10.0.1.1:43748 (size: 1242.0 KB, free: 910.9 MB)
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:22 INFO storage.BlockManagerInfo: Added taskresult_9 in memory on 10.0.1.2:35775 (size: 1239.6 KB, free: 910.9 MB)
18/06/26 03:21:22 INFO client.TransportClientFactory: Successfully created connection to /10.0.1.1:43748 after 2 ms (0 ms spent in bootstraps)
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:22 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:23 INFO client.TransportClientFactory: Successfully created connection to /10.0.1.2:35775 after 4 ms (0 ms spent in bootstraps)
18/06/26 03:21:23 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 8) in 2732 ms on 10.0.1.1 (executor 1) (1/2)
18/06/26 03:21:23 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 9) in 2731 ms on 10.0.1.2 (executor 0) (2/2)
18/06/26 03:21:23 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
18/06/26 03:21:23 INFO scheduler.DAGScheduler: ResultStage 4 (run at ThreadPoolExecutor.java:1142) finished in 2.740 s
18/06/26 03:21:23 INFO scheduler.DAGScheduler: Job 1 finished: run at ThreadPoolExecutor.java:1142, took 2.902355 s
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed taskresult_9 on 10.0.1.2:35775 in memory (size: 1239.6 KB, free: 912.1 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed taskresult_8 on 10.0.1.1:43748 in memory (size: 1242.0 KB, free: 912.1 MB)
18/06/26 03:21:23 INFO codegen.CodeGenerator: Code generated in 10.528349 ms
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Added taskresult_0 in memory on 10.0.1.1:43748 (size: 2.4 MB, free: 909.7 MB)
18/06/26 03:21:23 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:23 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:23 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:23 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:23 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2886 ms on 10.0.1.1 (executor 1) (1/2)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed taskresult_0 on 10.0.1.1:43748 in memory (size: 2.4 MB, free: 912.1 MB)
18/06/26 03:21:23 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:23 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:23 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:23 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Added taskresult_1 in memory on 10.0.1.2:35775 (size: 2.4 MB, free: 909.7 MB)
18/06/26 03:21:23 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:23 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:23 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:23 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:23 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2939 ms on 10.0.1.2 (executor 0) (2/2)
18/06/26 03:21:23 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/06/26 03:21:23 INFO scheduler.DAGScheduler: ResultStage 0 (run at ThreadPoolExecutor.java:1142) finished in 3.021 s
18/06/26 03:21:23 INFO scheduler.DAGScheduler: Job 0 finished: run at ThreadPoolExecutor.java:1142, took 3.072370 s
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed taskresult_1 on 10.0.1.2:35775 in memory (size: 2.4 MB, free: 912.1 MB)
18/06/26 03:21:23 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 19.0 MB, free 385.1 MB)
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 239
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 227
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 217
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 269
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 224
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 214
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 240
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 263
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 264
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 225
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 208
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 237
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on dc1master-lan1:38214 in memory (size: 6.2 KB, free: 408.7 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.1.1:43748 in memory (size: 6.2 KB, free: 912.2 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.1.2:35775 in memory (size: 6.2 KB, free: 912.2 MB)
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 213
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 230
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 229
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 220
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 253
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on dc1master-lan1:38214 in memory (size: 5.9 KB, free: 408.7 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.1.2:35775 in memory (size: 5.9 KB, free: 912.2 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.1.1:43748 in memory (size: 5.9 KB, free: 912.2 MB)
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 243
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 207
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 221
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 222
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 277
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 236
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on dc1master-lan1:38214 in memory (size: 6.2 KB, free: 408.7 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.1.1:43748 in memory (size: 6.2 KB, free: 912.2 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.1.2:35775 in memory (size: 6.2 KB, free: 912.2 MB)
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 257
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 265
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 215
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 238
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 268
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on dc1master-lan1:38214 in memory (size: 5.9 KB, free: 408.7 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.1.1:43748 in memory (size: 5.9 KB, free: 912.2 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.1.2:35775 in memory (size: 5.9 KB, free: 912.2 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on dc1master-lan1:38214 in memory (size: 6.4 KB, free: 408.7 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.1.2:35775 in memory (size: 6.4 KB, free: 912.2 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.1.1:43748 in memory (size: 6.4 KB, free: 912.2 MB)
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 231
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 223
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 271
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 248
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 270
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 266
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 226
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 211
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 216
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 247
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 209
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 232
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 262
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 273
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 234
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 218
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 261
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 276
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 205
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 278
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 260
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 244
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 255
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 258
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 233
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 256
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 250
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 272
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 251
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 254
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 212
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 275
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 245
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 259
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 252
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 242
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 235
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 274
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 246
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 279
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 210
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 206
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 228
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 267
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 241
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 219
18/06/26 03:21:23 INFO spark.ContextCleaner: Cleaned accumulator 249
18/06/26 03:21:23 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.5 MB, free 381.6 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on dc1master-lan1:38214 (size: 3.5 MB, free: 405.2 MB)
18/06/26 03:21:23 INFO spark.SparkContext: Created broadcast 13 from run at ThreadPoolExecutor.java:1142
18/06/26 03:21:23 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 39.0 MB, free 342.6 MB)
18/06/26 03:21:23 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.0 MB, free 338.6 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on dc1master-lan1:38214 (size: 4.0 MB, free: 401.2 MB)
18/06/26 03:21:23 INFO memory.MemoryStore: Block broadcast_14_piece1 stored as bytes in memory (estimated size 3.1 MB, free 335.5 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Added broadcast_14_piece1 in memory on dc1master-lan1:38214 (size: 3.1 MB, free: 398.0 MB)
18/06/26 03:21:23 INFO spark.SparkContext: Created broadcast 14 from run at ThreadPoolExecutor.java:1142
18/06/26 03:21:23 INFO codegen.CodeGenerator: Code generated in 41.854245 ms
18/06/26 03:21:23 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 291.7 KB, free 335.2 MB)
18/06/26 03:21:23 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 25.1 KB, free 335.2 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on dc1master-lan1:38214 (size: 25.1 KB, free: 398.0 MB)
18/06/26 03:21:23 INFO spark.SparkContext: Created broadcast 15 from 
18/06/26 03:21:23 INFO codegen.CodeGenerator: Code generated in 14.639193 ms
18/06/26 03:21:23 INFO codegen.CodeGenerator: Code generated in 10.992293 ms
18/06/26 03:21:23 INFO codegen.CodeGenerator: Code generated in 24.873352 ms
18/06/26 03:21:23 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 291.2 KB, free 334.9 MB)
18/06/26 03:21:23 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 25.1 KB, free 334.8 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on dc1master-lan1:38214 (size: 25.1 KB, free: 398.0 MB)
18/06/26 03:21:23 INFO spark.SparkContext: Created broadcast 16 from 
18/06/26 03:21:23 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:21:23 INFO codegen.CodeGenerator: Code generated in 14.208236 ms
18/06/26 03:21:23 INFO codegen.CodeGenerator: Code generated in 13.67831 ms
18/06/26 03:21:23 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 291.0 KB, free 334.6 MB)
18/06/26 03:21:23 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 25.1 KB, free 334.5 MB)
18/06/26 03:21:23 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on dc1master-lan1:38214 (size: 25.1 KB, free: 397.9 MB)
18/06/26 03:21:23 INFO spark.SparkContext: Created broadcast 17 from 
18/06/26 03:21:24 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 291
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 299
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 197
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 198
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 187
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 200
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 283
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 184
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 195
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 285
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 284
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 201
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 202
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 185
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 295
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 288
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 289
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 294
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 280
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 297
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 189
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 182
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 188
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 204
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 191
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 302
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 300
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 281
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 186
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 282
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 203
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 293
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 292
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 290
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 287
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 190
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 196
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 194
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 303
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 304
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 183
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 193
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 181
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 286
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 296
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 199
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 298
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 192
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 301
18/06/26 03:21:24 INFO spark.ContextCleaner: Cleaned accumulator 180
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:24 INFO codegen.CodeGenerator: Code generated in 22.979774 ms
18/06/26 03:21:24 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 291.7 KB, free 334.3 MB)
18/06/26 03:21:24 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 25.2 KB, free 334.2 MB)
18/06/26 03:21:24 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on dc1master-lan1:38214 (size: 25.2 KB, free: 397.9 MB)
18/06/26 03:21:24 INFO spark.SparkContext: Created broadcast 18 from 
18/06/26 03:21:24 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:21:24 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:21:24 INFO codegen.CodeGenerator: Code generated in 14.092806 ms
18/06/26 03:21:24 INFO codegen.CodeGenerator: Code generated in 18.690824 ms
18/06/26 03:21:24 INFO codegen.CodeGenerator: Code generated in 53.260296 ms
18/06/26 03:21:24 INFO codegen.CodeGenerator: Code generated in 19.981554 ms
18/06/26 03:21:24 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 291.7 KB, free 333.9 MB)
18/06/26 03:21:24 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 25.1 KB, free 333.9 MB)
18/06/26 03:21:24 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on dc1master-lan1:38214 (size: 25.1 KB, free: 397.9 MB)
18/06/26 03:21:24 INFO spark.SparkContext: Created broadcast 19 from 
18/06/26 03:21:24 INFO codegen.CodeGenerator: Code generated in 8.330188 ms
18/06/26 03:21:24 INFO codegen.CodeGenerator: Code generated in 8.795553 ms
18/06/26 03:21:24 INFO codegen.CodeGenerator: Code generated in 53.787733 ms
18/06/26 03:21:24 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 291.2 KB, free 333.6 MB)
18/06/26 03:21:24 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 25.1 KB, free 333.6 MB)
18/06/26 03:21:24 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on dc1master-lan1:38214 (size: 25.1 KB, free: 397.9 MB)
18/06/26 03:21:24 INFO spark.SparkContext: Created broadcast 20 from 
18/06/26 03:21:24 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:21:24 INFO codegen.CodeGenerator: Code generated in 12.385797 ms
18/06/26 03:21:24 INFO codegen.CodeGenerator: Code generated in 22.193436 ms
18/06/26 03:21:24 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 291.7 KB, free 333.3 MB)
18/06/26 03:21:24 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 25.2 KB, free 333.3 MB)
18/06/26 03:21:24 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on dc1master-lan1:38214 (size: 25.2 KB, free: 397.8 MB)
18/06/26 03:21:24 INFO spark.SparkContext: Created broadcast 21 from 
18/06/26 03:21:24 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:21:24 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 03:21:24 INFO spark.SparkContext: Starting job: processCmd at CliDriver.java:376
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Registering RDD 40 (processCmd at CliDriver.java:376)
18/06/26 03:21:24 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerShuffle id:0
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Registering RDD 48 (processCmd at CliDriver.java:376)
18/06/26 03:21:24 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerShuffle id:1
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Registering RDD 60 (processCmd at CliDriver.java:376)
18/06/26 03:21:24 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerShuffle id:2
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Registering RDD 63 (processCmd at CliDriver.java:376)
18/06/26 03:21:24 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerShuffle id:3
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Registering RDD 76 (processCmd at CliDriver.java:376)
18/06/26 03:21:24 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerShuffle id:4
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Registering RDD 89 (processCmd at CliDriver.java:376)
18/06/26 03:21:24 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerShuffle id:5
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Registering RDD 92 (processCmd at CliDriver.java:376)
18/06/26 03:21:24 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerShuffle id:6
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Got job 5 (processCmd at CliDriver.java:376) with 4 output partitions
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Final stage: ResultStage 12 (processCmd at CliDriver.java:376)
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 11, ShuffleMapStage 8)
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 11, ShuffleMapStage 8)
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[40] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:24 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 19.0 KB, free 333.3 MB)
18/06/26 03:21:24 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 8.4 KB, free 333.3 MB)
18/06/26 03:21:24 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on dc1master-lan1:38214 (size: 8.4 KB, free: 397.8 MB)
18/06/26 03:21:24 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[40] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 6 tasks
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[48] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:24 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 13.7 KB, free 333.3 MB)
18/06/26 03:21:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 10, 10.0.1.1, executor 1, partition 0, ANY, 7907 bytes)
18/06/26 03:21:24 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 11, 10.0.1.2, executor 0, partition 1, ANY, 7907 bytes)
18/06/26 03:21:24 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 12, 10.0.1.1, executor 1, partition 2, ANY, 7907 bytes)
18/06/26 03:21:24 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 5.0 (TID 13, 10.0.1.2, executor 0, partition 3, ANY, 7907 bytes)
18/06/26 03:21:24 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 5.0 (TID 14, 10.0.1.1, executor 1, partition 4, ANY, 7907 bytes)
18/06/26 03:21:24 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 6.8 KB, free 333.3 MB)
18/06/26 03:21:24 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 5.0 (TID 15, 10.0.1.2, executor 0, partition 5, ANY, 7907 bytes)
18/06/26 03:21:24 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on dc1master-lan1:38214 (size: 6.8 KB, free: 397.8 MB)
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 6
18/06/26 03:21:24 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[48] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 2 tasks
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[76] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 16, 10.0.1.2, executor 0, partition 0, ANY, 7909 bytes)
18/06/26 03:21:24 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 19.0 KB, free 333.2 MB)
18/06/26 03:21:24 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 17, 10.0.1.1, executor 1, partition 1, ANY, 7909 bytes)
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 03:21:24 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 8.4 KB, free 333.2 MB)
18/06/26 03:21:24 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on dc1master-lan1:38214 (size: 8.4 KB, free: 397.8 MB)
18/06/26 03:21:24 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:24 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[76] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: Adding task set 9.0 with 6 tasks
18/06/26 03:21:24 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:24 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:24 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:24 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:24 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:24 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:24 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:21:24 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:21:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 18, 10.0.1.2, executor 0, partition 0, ANY, 7907 bytes)
18/06/26 03:21:24 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:24 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 19, 10.0.1.1, executor 1, partition 1, ANY, 7907 bytes)
18/06/26 03:21:24 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:24 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 9.0 (TID 20, 10.0.1.2, executor 0, partition 2, ANY, 7907 bytes)
18/06/26 03:21:24 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:24 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 9.0 (TID 21, 10.0.1.1, executor 1, partition 3, ANY, 7907 bytes)
18/06/26 03:21:24 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:24 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 9.0 (TID 22, 10.0.1.2, executor 0, partition 4, ANY, 7907 bytes)
18/06/26 03:21:24 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:24 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 9.0 (TID 23, 10.0.1.1, executor 1, partition 5, ANY, 7907 bytes)
18/06/26 03:21:24 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:24 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 6
18/06/26 03:21:24 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 912.2 MB)
18/06/26 03:21:24 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 912.2 MB)
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.1.1:43748 (size: 6.8 KB, free: 912.2 MB)
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 912.2 MB)
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 912.1 MB)
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.1.2:35775 (size: 6.8 KB, free: 912.2 MB)
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 912.2 MB)
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 912.1 MB)
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 912.1 MB)
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 912.1 MB)
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 912.1 MB)
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 912.1 MB)
18/06/26 03:21:25 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:25 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:25 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:25 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.1:43748 (size: 69.3 KB, free: 912.0 MB)
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.1:43748 (size: 3.5 KB, free: 912.0 MB)
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.1:43748 (size: 3.5 KB, free: 912.0 MB)
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.2:35775 (size: 69.3 KB, free: 912.0 MB)
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.2:35775 (size: 3.5 KB, free: 912.0 MB)
18/06/26 03:21:25 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.2:35775 (size: 3.5 KB, free: 912.0 MB)
18/06/26 03:21:26 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:26 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:26 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:26 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:26 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 17) in 1121 ms on 10.0.1.1 (executor 1) (1/2)
18/06/26 03:21:26 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:26 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0a/shuffle_1_1_0.data
18/06/26 03:21:26 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/32/shuffle_1_1_0.index
18/06/26 03:21:26 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:26 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:26 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:26 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 16) in 1173 ms on 10.0.1.2 (executor 0) (2/2)
18/06/26 03:21:26 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
18/06/26 03:21:26 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:26 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/2b/shuffle_1_0_0.data
18/06/26 03:21:26 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_1_0_0.index
18/06/26 03:21:26 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:376) finished in 1.184 s
18/06/26 03:21:26 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:21:26 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 9, ShuffleMapStage 5)
18/06/26 03:21:26 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 12, ShuffleMapStage 10, ShuffleMapStage 7, ShuffleMapStage 11, ShuffleMapStage 8)
18/06/26 03:21:26 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:21:26 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:26 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:26 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:26 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 5.0 (TID 15) in 2374 ms on 10.0.1.2 (executor 0) (1/6)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/27/shuffle_0_5_0.data
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/35/shuffle_0_5_0.index
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 9.0 (TID 23) in 2401 ms on 10.0.1.1 (executor 1) (1/6)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/1d/shuffle_4_5_0.data
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/39/shuffle_4_5_0.index
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 5.0 (TID 13) in 2658 ms on 10.0.1.2 (executor 0) (2/6)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/29/shuffle_0_3_0.data
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0d/shuffle_0_3_0.index
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 9.0 (TID 22) in 2656 ms on 10.0.1.2 (executor 0) (2/6)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/3c/shuffle_4_4_0.data
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/08/shuffle_4_4_0.index
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 12) in 2691 ms on 10.0.1.1 (executor 1) (3/6)
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/36/shuffle_0_2_0.data
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/32/shuffle_0_2_0.index
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 10) in 2695 ms on 10.0.1.1 (executor 1) (4/6)
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 11) in 2694 ms on 10.0.1.2 (executor 0) (5/6)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0c/shuffle_0_0_0.data
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/30/shuffle_0_0_0.index
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/15/shuffle_0_1_0.data
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_0_1_0.index
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 5.0 (TID 14) in 2699 ms on 10.0.1.1 (executor 1) (6/6)
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/38/shuffle_0_4_0.data
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0c/shuffle_0_4_0.index
18/06/26 03:21:27 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:376) finished in 2.717 s
18/06/26 03:21:27 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:21:27 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 9)
18/06/26 03:21:27 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 12, ShuffleMapStage 10, ShuffleMapStage 7, ShuffleMapStage 11, ShuffleMapStage 8)
18/06/26 03:21:27 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:21:27 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[60] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140711
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140711
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143907
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143907
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138095
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138095
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 137127
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 137127
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 121819
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 121819
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144976
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144976
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142734
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142734
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140701
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140701
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139040
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139040
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 123992
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 123992
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142253
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142253
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139836
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139836
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140708
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140708
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 137187
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 137187
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 117413
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 117413
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143012
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143012
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144814
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144814
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140990
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140990
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138697
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138697
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 116604
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 116604
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 18) in 2707 ms on 10.0.1.2 (executor 0) (3/6)
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 9.0 (TID 21) in 2709 ms on 10.0.1.1 (executor 1) (4/6)
18/06/26 03:21:27 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 126.5 KB, free 333.1 MB)
18/06/26 03:21:27 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 48.0 KB, free 333.1 MB)
18/06/26 03:21:27 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on dc1master-lan1:38214 (size: 48.0 KB, free: 397.8 MB)
18/06/26 03:21:27 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:27 INFO scheduler.DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[60] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 12 tasks
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/38/shuffle_4_0_0.data
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0c/shuffle_4_0_0.index
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/25/shuffle_4_3_0.data
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/37/shuffle_4_3_0.index
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 7.0 (TID 24, 10.0.1.1, executor 1, partition 5, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 7.0 (TID 25, 10.0.1.2, executor 0, partition 6, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 7.0 (TID 26, 10.0.1.1, executor 1, partition 7, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 7.0 (TID 27, 10.0.1.2, executor 0, partition 8, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 4
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 19) in 2723 ms on 10.0.1.1 (executor 1) (5/6)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 9.0 (TID 20) in 2724 ms on 10.0.1.2 (executor 0) (6/6)
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/27/shuffle_4_1_0.data
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/35/shuffle_4_1_0.index
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/3a/shuffle_4_2_0.data
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0a/shuffle_4_2_0.index
18/06/26 03:21:27 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (processCmd at CliDriver.java:376) finished in 2.734 s
18/06/26 03:21:27 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:21:27 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 7)
18/06/26 03:21:27 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 12, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 8)
18/06/26 03:21:27 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:21:27 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[89] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 141775
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 141775
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142237
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142237
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143487
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143487
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142800
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142800
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 116037
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 116037
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142735
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142735
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140513
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140513
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142608
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142608
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 141095
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 141095
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 117229
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 117229
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143225
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143225
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144994
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144994
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 141700
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 141700
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143486
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143486
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 114415
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 114415
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143173
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143173
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 135781
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 135781
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145177
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145177
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140068
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140068
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 117509
18/06/26 03:21:27 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 117509
18/06/26 03:21:27 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.1.1:43748 (size: 48.0 KB, free: 912.0 MB)
18/06/26 03:21:27 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.1.2:35775 (size: 48.0 KB, free: 912.0 MB)
18/06/26 03:21:27 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 127.2 KB, free 332.9 MB)
18/06/26 03:21:27 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 48.2 KB, free 332.9 MB)
18/06/26 03:21:27 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on dc1master-lan1:38214 (size: 48.2 KB, free: 397.7 MB)
18/06/26 03:21:27 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:27 INFO scheduler.DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[89] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: Adding task set 10.0 with 12 tasks
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 10.0 (TID 28, 10.0.1.2, executor 0, partition 5, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 10.0 (TID 29, 10.0.1.1, executor 1, partition 6, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 10.0 (TID 30, 10.0.1.2, executor 0, partition 7, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 10.0 (TID 31, 10.0.1.1, executor 1, partition 8, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 4
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:27 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:27 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:27 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.1.2:35775 (size: 48.2 KB, free: 911.9 MB)
18/06/26 03:21:27 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.1.1:43748 (size: 48.2 KB, free: 911.9 MB)
18/06/26 03:21:27 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:27 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:27 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:27 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:27 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:27 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:27 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:27 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:27 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:28 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:28 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:28 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:28 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:28 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:29 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:29 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:29 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:29 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:29 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:29 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:29 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:29 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:29 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:29 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:29 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:29 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:29 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:30 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:30 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:30 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:30 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:30 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:30 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:30 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:30 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:30 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:30 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:30 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:30 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:30 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:31 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:31 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:31 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 32, 10.0.1.1, executor 1, partition 0, ANY, 8018 bytes)
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 33, 10.0.1.2, executor 0, partition 1, ANY, 8018 bytes)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 7.0 (TID 34, 10.0.1.1, executor 1, partition 2, ANY, 8018 bytes)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 7.0 (TID 35, 10.0.1.2, executor 0, partition 3, ANY, 8018 bytes)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:31 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 7.0 (TID 36, 10.0.1.1, executor 1, partition 4, ANY, 8018 bytes)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 7.0 (TID 37, 10.0.1.2, executor 0, partition 9, ANY, 8014 bytes)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 7.0 (TID 38, 10.0.1.1, executor 1, partition 10, ANY, 8014 bytes)
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 03:21:31 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 7.0 (TID 39, 10.0.1.2, executor 0, partition 11, ANY, 8014 bytes)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 03:21:31 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 40, 10.0.1.1, executor 1, partition 0, ANY, 8018 bytes)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 41, 10.0.1.2, executor 0, partition 1, ANY, 8018 bytes)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 03:21:31 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 10.0 (TID 42, 10.0.1.1, executor 1, partition 2, ANY, 8018 bytes)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 10.0 (TID 43, 10.0.1.2, executor 0, partition 3, ANY, 8018 bytes)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 03:21:31 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:31 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 03:21:31 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 03:21:31 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 03:21:31 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 03:21:31 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:31 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:31 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 911.9 MB)
18/06/26 03:21:31 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 911.9 MB)
18/06/26 03:21:31 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 911.9 MB)
18/06/26 03:21:31 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 911.9 MB)
18/06/26 03:21:31 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.1:43748 (size: 25.2 KB, free: 911.8 MB)
18/06/26 03:21:31 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.2:35775 (size: 25.2 KB, free: 911.8 MB)
18/06/26 03:21:31 INFO storage.BlockManagerInfo: Added broadcast_14_piece1 in memory on 10.0.1.1:43748 (size: 3.1 MB, free: 908.7 MB)
18/06/26 03:21:31 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.1:43748 (size: 4.0 MB, free: 904.7 MB)
18/06/26 03:21:31 INFO storage.BlockManagerInfo: Added broadcast_14_piece1 in memory on 10.0.1.2:35775 (size: 3.1 MB, free: 908.7 MB)
18/06/26 03:21:31 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.2:35775 (size: 4.0 MB, free: 904.7 MB)
18/06/26 03:21:31 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.1:43748 (size: 3.5 MB, free: 901.2 MB)
18/06/26 03:21:31 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.2:35775 (size: 3.5 MB, free: 901.2 MB)
18/06/26 03:21:31 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:31 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:31 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:31 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:31 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:31 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:31 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:32 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 10.0 (TID 44, 10.0.1.2, executor 0, partition 4, ANY, 8018 bytes)
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 1
18/06/26 03:21:32 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:32 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:32 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:32 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:32 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 03:21:32 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 7.0 (TID 39) in 1051 ms on 10.0.1.2 (executor 0) (1/12)
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 03:21:32 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/2c/shuffle_2_11_0.data
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/18/shuffle_2_11_0.index
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:32 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 10.0 (TID 45, 10.0.1.1, executor 1, partition 9, ANY, 8014 bytes)
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 1
18/06/26 03:21:32 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:32 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:32 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:32 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:32 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 03:21:32 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:32 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 7.0 (TID 36) in 1353 ms on 10.0.1.1 (executor 1) (2/12)
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/3a/shuffle_2_4_0.data
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0a/shuffle_2_4_0.index
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:32 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.1:43748 (size: 25.2 KB, free: 901.1 MB)
18/06/26 03:21:32 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:32 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:32 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:32 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### false
18/06/26 03:21:32 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:32 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:32 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 142995
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 144511
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 145163
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 143544
18/06/26 03:21:32 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 140711
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 144976
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 142253
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 143012
18/06/26 03:21:32 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 143907
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 142734
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 139836
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 144814
18/06/26 03:21:32 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 138095
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 140701
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 140708
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 140990
18/06/26 03:21:32 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 137127
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 139040
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 137187
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 138697
18/06/26 03:21:32 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 121819
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 123992
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 117413
18/06/26 03:21:32 INFO scheduler.RawMapStatus: 116604
18/06/26 03:21:32 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.1:58478
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.1:58478
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:32 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 10.0 (TID 46, 10.0.1.2, executor 0, partition 10, ANY, 8014 bytes)
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:32 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 1
18/06/26 03:21:32 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:32 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:32 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:32 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:32 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:32 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 10.0 (TID 44) in 781 ms on 10.0.1.2 (executor 0) (1/12)
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 03:21:32 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/1d/shuffle_5_4_0.data
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/39/shuffle_5_4_0.index
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:32 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:32 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.2:55800
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.2:55800
18/06/26 03:21:32 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:33 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.2:35775 (size: 25.2 KB, free: 901.1 MB)
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 10.0 (TID 47, 10.0.1.1, executor 1, partition 11, ANY, 8014 bytes)
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 1
18/06/26 03:21:33 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:33 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:33 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:33 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:33 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:33 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 7.0 (TID 38) in 2141 ms on 10.0.1.1 (executor 1) (3/12)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0b/shuffle_2_10_0.data
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/27/shuffle_2_10_0.index
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 03:21:33 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:33 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 40) in 2158 ms on 10.0.1.1 (executor 1) (2/12)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/19/shuffle_5_0_0.data
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/35/shuffle_5_0_0.index
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 41) in 2183 ms on 10.0.1.2 (executor 0) (3/12)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/06/shuffle_5_1_0.data
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0a/shuffle_5_1_0.index
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 32) in 2209 ms on 10.0.1.1 (executor 1) (4/12)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0a/shuffle_2_0_0.data
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/32/shuffle_2_0_0.index
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 7.0 (TID 34) in 2212 ms on 10.0.1.1 (executor 1) (5/12)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/38/shuffle_2_2_0.data
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0c/shuffle_2_2_0.index
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 10.0 (TID 42) in 2223 ms on 10.0.1.1 (executor 1) (4/12)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/1b/shuffle_5_2_0.data
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/37/shuffle_5_2_0.index
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 10.0 (TID 43) in 2288 ms on 10.0.1.2 (executor 0) (5/12)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/04/shuffle_5_3_0.data
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/08/shuffle_5_3_0.index
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 7.0 (TID 37) in 2301 ms on 10.0.1.2 (executor 0) (6/12)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/1f/shuffle_2_9_0.data
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/05/shuffle_2_9_0.index
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 7.0 (TID 35) in 2311 ms on 10.0.1.2 (executor 0) (7/12)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/27/shuffle_2_3_0.data
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/35/shuffle_2_3_0.index
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 33) in 2364 ms on 10.0.1.2 (executor 0) (8/12)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/29/shuffle_2_1_0.data
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0d/shuffle_2_1_0.index
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:33 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:33 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 10.0 (TID 47) in 371 ms on 10.0.1.1 (executor 1) (6/12)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/29/shuffle_5_11_0.data
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/3b/shuffle_5_11_0.index
18/06/26 03:21:33 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:33 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:33 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:33 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.1:58478
18/06/26 03:21:33 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:33 INFO scheduler.RawMapStatus: 1289372
18/06/26 03:21:33 INFO scheduler.RawMapStatus: 1282786
18/06/26 03:21:33 INFO scheduler.RawMapStatus: 1285962
18/06/26 03:21:33 INFO scheduler.RawMapStatus: 1291904
18/06/26 03:21:33 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:33 INFO scheduler.RawMapStatus: 1282137
18/06/26 03:21:33 INFO scheduler.RawMapStatus: 1284311
18/06/26 03:21:33 INFO scheduler.RawMapStatus: 1280329
18/06/26 03:21:33 INFO scheduler.RawMapStatus: 1283523
18/06/26 03:21:33 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.2:55800
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:34 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 7.0 (TID 24, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 43748, None), shuffleId=1, mapId=0, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_1_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_1_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Task 5.0 in stage 7.0 (TID 24) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:34 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 7.0 (TID 26, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 43748, None), shuffleId=1, mapId=0, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_1_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_1_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Task 7.0 in stage 7.0 (TID 26) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:34 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 7 (processCmd at CliDriver.java:376) as failed due to a fetch failure from ShuffleMapStage 6 (processCmd at CliDriver.java:376)
18/06/26 03:21:34 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (processCmd at CliDriver.java:376) failed in 6.441 s due to org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_1_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_1_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

18/06/26 03:21:34 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 6 (processCmd at CliDriver.java:376) and ShuffleMapStage 7 (processCmd at CliDriver.java:376) due to fetch failure
18/06/26 03:21:34 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 2)
18/06/26 03:21:34 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
18/06/26 03:21:34 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 10.0.1.1, 43748, None)
18/06/26 03:21:34 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor
18/06/26 03:21:34 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 1 (epoch 2)
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 10.0 (TID 45) in 1634 ms on 10.0.1.1 (executor 1) (7/12)
18/06/26 03:21:34 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:34 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/02/shuffle_5_9_0.data
18/06/26 03:21:34 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/3e/shuffle_5_9_0.index
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:34 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 7.0 (TID 25, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 35775, None), shuffleId=1, mapId=1, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/32/shuffle_1_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/32/shuffle_1_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Task 6.0 in stage 7.0 (TID 25) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:34 INFO scheduler.DAGScheduler: Executor lost: 0 (epoch 2)
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:34 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
18/06/26 03:21:34 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.0.1.2, 35775, None)
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:34 INFO storage.BlockManagerMaster: Removed 0 successfully in removeExecutor
18/06/26 03:21:34 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 7.0 (TID 27, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 35775, None), shuffleId=1, mapId=1, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/32/shuffle_1_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/32/shuffle_1_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:34 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 0 (epoch 2)
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Task 8.0 in stage 7.0 (TID 27) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
18/06/26 03:21:34 INFO scheduler.DAGScheduler: Resubmitting failed stages
18/06/26 03:21:34 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[48] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:34 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 13.7 KB, free 332.9 MB)
18/06/26 03:21:34 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 6.8 KB, free 332.9 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on dc1master-lan1:38214 (size: 6.8 KB, free: 397.7 MB)
18/06/26 03:21:34 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:34 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[48] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: Adding task set 6.1 with 2 tasks
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[40] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.1 (TID 48, 10.0.1.2, executor 0, partition 0, ANY, 7909 bytes)
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.1 (TID 49, 10.0.1.1, executor 1, partition 1, ANY, 7909 bytes)
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 03:21:34 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 19.0 KB, free 332.8 MB)
18/06/26 03:21:34 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 8.4 KB, free 332.8 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on dc1master-lan1:38214 (size: 8.4 KB, free: 397.7 MB)
18/06/26 03:21:34 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:34 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[40] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: Adding task set 5.1 with 6 tasks
18/06/26 03:21:34 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:34 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.1 (TID 50, 10.0.1.1, executor 1, partition 0, ANY, 7907 bytes)
18/06/26 03:21:34 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.1 (TID 51, 10.0.1.2, executor 0, partition 1, ANY, 7907 bytes)
18/06/26 03:21:34 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.1 (TID 52, 10.0.1.1, executor 1, partition 2, ANY, 7907 bytes)
18/06/26 03:21:34 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 5.1 (TID 53, 10.0.1.2, executor 0, partition 3, ANY, 7907 bytes)
18/06/26 03:21:34 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 5.1 (TID 54, 10.0.1.1, executor 1, partition 4, ANY, 7907 bytes)
18/06/26 03:21:34 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 5.1 (TID 55, 10.0.1.2, executor 0, partition 5, ANY, 7907 bytes)
18/06/26 03:21:34 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 6
18/06/26 03:21:34 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.2:35775 with 912.3 MB RAM, BlockManagerId(0, 10.0.1.2, 35775, None)
18/06/26 03:21:34 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.1:43748 with 912.3 MB RAM, BlockManagerId(1, 10.0.1.1, 43748, None)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.2:35775 (size: 25.2 KB, free: 912.3 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.1:43748 (size: 25.2 KB, free: 912.3 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.2:35775 (size: 3.5 MB, free: 908.7 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.1:43748 (size: 3.5 MB, free: 908.7 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 908.7 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 908.7 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 908.7 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 908.7 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.2:35775 (size: 25.2 KB, free: 908.7 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.1:43748 (size: 25.2 KB, free: 908.7 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 908.6 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 908.6 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 908.6 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 908.6 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 908.6 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 908.6 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 908.6 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 908.6 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 908.5 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 908.5 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 908.6 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.1:43748 (size: 4.0 MB, free: 904.5 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.1.1:43748 (size: 6.8 KB, free: 904.5 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 908.6 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 908.5 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 904.5 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 908.5 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 904.5 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.2:35775 (size: 4.0 MB, free: 904.5 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.1:43748 (size: 3.5 KB, free: 904.5 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.1.2:35775 (size: 6.8 KB, free: 904.5 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_14_piece1 in memory on 10.0.1.1:43748 (size: 3.1 MB, free: 901.3 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.1:43748 (size: 69.3 KB, free: 901.3 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 904.5 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.1.1:43748 (size: 48.0 KB, free: 901.2 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 904.5 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.1.1:43748 (size: 48.2 KB, free: 901.2 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.2:35775 (size: 3.5 KB, free: 904.5 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 901.1 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_14_piece1 in memory on 10.0.1.2:35775 (size: 3.1 MB, free: 901.3 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.1:43748 (size: 3.5 KB, free: 901.1 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.2:35775 (size: 69.3 KB, free: 901.3 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 901.1 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.1.2:35775 (size: 48.0 KB, free: 901.2 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.1.1:43748 (size: 6.8 KB, free: 901.1 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.1.2:35775 (size: 48.2 KB, free: 901.2 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 901.1 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.2:35775 (size: 3.5 KB, free: 901.1 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 901.1 MB)
18/06/26 03:21:34 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.1.2:35775 (size: 6.8 KB, free: 901.1 MB)
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 10.0 (TID 46) in 1646 ms on 10.0.1.2 (executor 0) (8/12)
18/06/26 03:21:34 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:34 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/38/shuffle_5_10_0.data
18/06/26 03:21:34 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/04/shuffle_5_10_0.index
18/06/26 03:21:34 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:34 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:34 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 10.0 (TID 31, 10.0.1.1, executor 1): FetchFailed(null, shuffleId=4, mapId=-1, reduceId=3, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 4
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1414)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1410)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:62)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

)
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Task 8.0 in stage 10.0 (TID 31) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:34 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 10 (processCmd at CliDriver.java:376) as failed due to a fetch failure from ShuffleMapStage 9 (processCmd at CliDriver.java:376)
18/06/26 03:21:34 INFO scheduler.DAGScheduler: ShuffleMapStage 10 (processCmd at CliDriver.java:376) failed in 7.207 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 4
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1414)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1410)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:62)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 9 (processCmd at CliDriver.java:376) and ShuffleMapStage 10 (processCmd at CliDriver.java:376) due to fetch failure
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:34 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 10.0 (TID 29, 10.0.1.1, executor 1): FetchFailed(null, shuffleId=4, mapId=-1, reduceId=1, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 4
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1414)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1410)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:62)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

)
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Task 6.0 in stage 10.0 (TID 29) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:34 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 10.0 (TID 30, 10.0.1.2, executor 0): FetchFailed(null, shuffleId=4, mapId=-1, reduceId=2, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 4
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1414)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1410)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:62)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

)
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Task 7.0 in stage 10.0 (TID 30) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:34 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 10.0 (TID 28, 10.0.1.2, executor 0): FetchFailed(null, shuffleId=4, mapId=-1, reduceId=0, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 4
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1414)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1410)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:62)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

)
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Task 5.0 in stage 10.0 (TID 28) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.1 (TID 48) in 639 ms on 10.0.1.2 (executor 0) (1/2)
18/06/26 03:21:34 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:34 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/2b/shuffle_1_0_0.data
18/06/26 03:21:34 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_1_0_0.index
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:34 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.1 (TID 49) in 672 ms on 10.0.1.1 (executor 1) (2/2)
18/06/26 03:21:34 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.1, whose tasks have all completed, from pool 
18/06/26 03:21:34 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:34 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0a/shuffle_1_1_0.data
18/06/26 03:21:34 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/32/shuffle_1_1_0.index
18/06/26 03:21:34 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:376) finished in 0.681 s
18/06/26 03:21:34 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:21:34 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 5)
18/06/26 03:21:34 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 12, ShuffleMapStage 7, ShuffleMapStage 11, ShuffleMapStage 8)
18/06/26 03:21:34 INFO scheduler.DAGScheduler: failed: Set(ShuffleMapStage 9, ShuffleMapStage 10)
18/06/26 03:21:35 INFO scheduler.DAGScheduler: Resubmitting failed stages
18/06/26 03:21:35 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[76] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:35 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 19.0 KB, free 332.8 MB)
18/06/26 03:21:35 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 8.4 KB, free 332.8 MB)
18/06/26 03:21:35 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on dc1master-lan1:38214 (size: 8.4 KB, free: 397.7 MB)
18/06/26 03:21:35 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:35 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[76] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
18/06/26 03:21:35 INFO scheduler.TaskSchedulerImpl: Adding task set 9.1 with 6 tasks
18/06/26 03:21:35 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:35 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:35 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.1 (TID 56, 10.0.1.2, executor 0, partition 0, ANY, 7907 bytes)
18/06/26 03:21:35 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:35 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.1 (TID 57, 10.0.1.1, executor 1, partition 1, ANY, 7907 bytes)
18/06/26 03:21:35 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:35 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 9.1 (TID 58, 10.0.1.2, executor 0, partition 2, ANY, 7907 bytes)
18/06/26 03:21:35 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:35 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 9.1 (TID 59, 10.0.1.1, executor 1, partition 3, ANY, 7907 bytes)
18/06/26 03:21:35 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:35 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 9.1 (TID 60, 10.0.1.2, executor 0, partition 4, ANY, 7907 bytes)
18/06/26 03:21:35 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:35 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 9.1 (TID 61, 10.0.1.1, executor 1, partition 5, ANY, 7907 bytes)
18/06/26 03:21:35 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:35 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:35 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 6
18/06/26 03:21:35 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 901.1 MB)
18/06/26 03:21:35 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 901.1 MB)
18/06/26 03:21:35 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:35 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:35 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:35 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 5.1 (TID 55) in 1866 ms on 10.0.1.2 (executor 0) (1/6)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/27/shuffle_0_5_0.data
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/35/shuffle_0_5_0.index
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 5.1 (TID 53) in 2166 ms on 10.0.1.2 (executor 0) (2/6)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/29/shuffle_0_3_0.data
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0d/shuffle_0_3_0.index
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.1 (TID 51) in 2263 ms on 10.0.1.2 (executor 0) (3/6)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/15/shuffle_0_1_0.data
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_0_1_0.index
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.1 (TID 50) in 2282 ms on 10.0.1.1 (executor 1) (4/6)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0c/shuffle_0_0_0.data
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/30/shuffle_0_0_0.index
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 5.1 (TID 54) in 2289 ms on 10.0.1.1 (executor 1) (5/6)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/38/shuffle_0_4_0.data
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0c/shuffle_0_4_0.index
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.1 (TID 52) in 2335 ms on 10.0.1.1 (executor 1) (6/6)
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.1, whose tasks have all completed, from pool 
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/36/shuffle_0_2_0.data
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/32/shuffle_0_2_0.index
18/06/26 03:21:36 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:376) finished in 2.342 s
18/06/26 03:21:36 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:21:36 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 9)
18/06/26 03:21:36 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 12, ShuffleMapStage 10, ShuffleMapStage 7, ShuffleMapStage 11, ShuffleMapStage 8)
18/06/26 03:21:36 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:21:36 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[60] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140711
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140711
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143907
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143907
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138095
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138095
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 137127
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 137127
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 121819
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 121819
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144976
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144976
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142734
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142734
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140701
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140701
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139040
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139040
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 123992
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 123992
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142253
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142253
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139836
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139836
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140708
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140708
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 137187
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 137187
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 117413
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 117413
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143012
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143012
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144814
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144814
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140990
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140990
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138697
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138697
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 116604
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 116604
18/06/26 03:21:36 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 126.5 KB, free 332.7 MB)
18/06/26 03:21:36 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 48.1 KB, free 332.6 MB)
18/06/26 03:21:36 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on dc1master-lan1:38214 (size: 48.1 KB, free: 397.7 MB)
18/06/26 03:21:36 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:36 INFO scheduler.DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[60] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: Adding task set 7.1 with 12 tasks
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:36 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 7.1 (TID 62, 10.0.1.1, executor 1, partition 5, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:36 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 7.1 (TID 63, 10.0.1.2, executor 0, partition 6, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:36 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 7.1 (TID 64, 10.0.1.1, executor 1, partition 7, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:36 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 7.1 (TID 65, 10.0.1.2, executor 0, partition 8, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 4
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:36 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:36 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:36 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:36 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:36 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.1.1:43748 (size: 48.1 KB, free: 901.1 MB)
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:36 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.1.2:35775 (size: 48.1 KB, free: 901.1 MB)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:36 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:36 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:36 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:36 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:36 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:36 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:36 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:37 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 9.1 (TID 61) in 1955 ms on 10.0.1.1 (executor 1) (1/6)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/1d/shuffle_4_5_0.data
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/39/shuffle_4_5_0.index
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:37 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.1 (TID 57) in 2211 ms on 10.0.1.1 (executor 1) (2/6)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/27/shuffle_4_1_0.data
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/35/shuffle_4_1_0.index
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:37 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 9.1 (TID 59) in 2213 ms on 10.0.1.1 (executor 1) (3/6)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/25/shuffle_4_3_0.data
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/37/shuffle_4_3_0.index
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:37 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 9.1 (TID 60) in 2238 ms on 10.0.1.2 (executor 0) (4/6)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/3c/shuffle_4_4_0.data
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/08/shuffle_4_4_0.index
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:37 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 9.1 (TID 58) in 2247 ms on 10.0.1.2 (executor 0) (5/6)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/3a/shuffle_4_2_0.data
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0a/shuffle_4_2_0.index
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:37 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.1 (TID 56) in 2294 ms on 10.0.1.2 (executor 0) (6/6)
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.1, whose tasks have all completed, from pool 
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/38/shuffle_4_0_0.data
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0c/shuffle_4_0_0.index
18/06/26 03:21:37 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (processCmd at CliDriver.java:376) finished in 2.301 s
18/06/26 03:21:37 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:21:37 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 7)
18/06/26 03:21:37 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 12, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 8)
18/06/26 03:21:37 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:21:37 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[89] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 141775
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 141775
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142237
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142237
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143487
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143487
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142800
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142800
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 116037
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 116037
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142735
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142735
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140513
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140513
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142608
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142608
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 141095
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 141095
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 117229
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 117229
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143225
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143225
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144994
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144994
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 141700
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 141700
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143486
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143486
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 114415
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 114415
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143173
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143173
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 135781
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 135781
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145177
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145177
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140068
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140068
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 117509
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 117509
18/06/26 03:21:37 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 127.2 KB, free 332.5 MB)
18/06/26 03:21:37 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 48.4 KB, free 332.5 MB)
18/06/26 03:21:37 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on dc1master-lan1:38214 (size: 48.4 KB, free: 397.6 MB)
18/06/26 03:21:37 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:37 INFO scheduler.DAGScheduler: Submitting 10 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[89] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 11))
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: Adding task set 10.1 with 10 tasks
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:37 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 10.1 (TID 66, 10.0.1.1, executor 1, partition 5, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:37 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 10.1 (TID 67, 10.0.1.2, executor 0, partition 6, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:37 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 10.1 (TID 68, 10.0.1.1, executor 1, partition 7, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:37 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 10.1 (TID 69, 10.0.1.2, executor 0, partition 8, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 4
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:37 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:37 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:37 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:37 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:37 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:37 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.1.2:35775 (size: 48.4 KB, free: 901.0 MB)
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:37 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:37 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:37 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.1.1:43748 (size: 48.4 KB, free: 901.0 MB)
18/06/26 03:21:37 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:37 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:37 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 142995
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 144511
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 145163
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 143544
18/06/26 03:21:37 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 140711
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 144976
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 142253
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 143012
18/06/26 03:21:37 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 143907
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 142734
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 139836
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 144814
18/06/26 03:21:37 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 138095
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 140701
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 140708
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 140990
18/06/26 03:21:37 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 137127
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 139040
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 137187
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 138697
18/06/26 03:21:37 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 121819
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 123992
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 117413
18/06/26 03:21:37 INFO scheduler.RawMapStatus: 116604
18/06/26 03:21:37 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:37 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.2:55800
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.2:55800
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:37 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.1:58478
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.1:58478
18/06/26 03:21:37 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:38 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:38 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 143863
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 142026
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 138876
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 139953
18/06/26 03:21:38 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 141775
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 142735
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 143225
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 143173
18/06/26 03:21:38 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 142237
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 140513
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 144994
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 135781
18/06/26 03:21:38 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 143487
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 142608
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 141700
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 145177
18/06/26 03:21:38 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 142800
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 141095
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 143486
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 140068
18/06/26 03:21:38 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 116037
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 117229
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 114415
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 117509
18/06/26 03:21:38 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:38 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.2:55800
18/06/26 03:21:38 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 1289372
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 1282786
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 1285962
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 1291904
18/06/26 03:21:38 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 1282137
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 1284311
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 1280329
18/06/26 03:21:38 INFO scheduler.RawMapStatus: 1283523
18/06/26 03:21:38 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.1:58478
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:38 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 7.1 (TID 64, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 43748, None), shuffleId=1, mapId=0, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_1_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_1_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:38 INFO scheduler.TaskSetManager: Task 7.0 in stage 7.1 (TID 64) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:38 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 7 (processCmd at CliDriver.java:376) as failed due to a fetch failure from ShuffleMapStage 6 (processCmd at CliDriver.java:376)
18/06/26 03:21:38 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (processCmd at CliDriver.java:376) failed in 2.054 s due to org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_1_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_1_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:38 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 6 (processCmd at CliDriver.java:376) and ShuffleMapStage 7 (processCmd at CliDriver.java:376) due to fetch failure
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:38 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 11)
18/06/26 03:21:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:38 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 7.1 (TID 62, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 43748, None), shuffleId=1, mapId=0, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_1_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0f/shuffle_1_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:38 INFO scheduler.TaskSetManager: Task 5.0 in stage 7.1 (TID 62) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:38 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 10.0.1.1, 43748, None)
18/06/26 03:21:38 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor
18/06/26 03:21:38 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 1 (epoch 11)
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:38 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 7.1 (TID 63, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 35775, None), shuffleId=1, mapId=1, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/32/shuffle_1_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/32/shuffle_1_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:38 INFO scheduler.TaskSetManager: Task 6.0 in stage 7.1 (TID 63) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:38 INFO scheduler.DAGScheduler: Executor lost: 0 (epoch 11)
18/06/26 03:21:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
18/06/26 03:21:38 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.0.1.2, 35775, None)
18/06/26 03:21:38 INFO storage.BlockManagerMaster: Removed 0 successfully in removeExecutor
18/06/26 03:21:38 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 0 (epoch 11)
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:38 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 7.1 (TID 65, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 35775, None), shuffleId=1, mapId=1, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/32/shuffle_1_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/32/shuffle_1_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:38 INFO scheduler.TaskSetManager: Task 8.0 in stage 7.1 (TID 65) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.1, whose tasks have all completed, from pool 
18/06/26 03:21:38 INFO scheduler.DAGScheduler: Resubmitting failed stages
18/06/26 03:21:38 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[48] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:38 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 13.7 KB, free 332.5 MB)
18/06/26 03:21:38 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 6.8 KB, free 332.5 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on dc1master-lan1:38214 (size: 6.8 KB, free: 397.6 MB)
18/06/26 03:21:38 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:38 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[48] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: Adding task set 6.2 with 2 tasks
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:38 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[40] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:38 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.2 (TID 70, 10.0.1.1, executor 1, partition 0, ANY, 7909 bytes)
18/06/26 03:21:38 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.2 (TID 71, 10.0.1.2, executor 0, partition 1, ANY, 7909 bytes)
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 03:21:38 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 19.0 KB, free 332.4 MB)
18/06/26 03:21:38 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 8.4 KB, free 332.4 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on dc1master-lan1:38214 (size: 8.4 KB, free: 397.6 MB)
18/06/26 03:21:38 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:38 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[40] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: Adding task set 5.2 with 6 tasks
18/06/26 03:21:38 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:38 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:21:38 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.2 (TID 72, 10.0.1.2, executor 0, partition 0, ANY, 7907 bytes)
18/06/26 03:21:38 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:38 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.2 (TID 73, 10.0.1.1, executor 1, partition 1, ANY, 7907 bytes)
18/06/26 03:21:38 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:38 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.2 (TID 74, 10.0.1.2, executor 0, partition 2, ANY, 7907 bytes)
18/06/26 03:21:38 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:38 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 5.2 (TID 75, 10.0.1.1, executor 1, partition 3, ANY, 7907 bytes)
18/06/26 03:21:38 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:38 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 5.2 (TID 76, 10.0.1.2, executor 0, partition 4, ANY, 7907 bytes)
18/06/26 03:21:38 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:38 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 5.2 (TID 77, 10.0.1.1, executor 1, partition 5, ANY, 7907 bytes)
18/06/26 03:21:38 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:38 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 6
18/06/26 03:21:38 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.1:43748 with 912.3 MB RAM, BlockManagerId(1, 10.0.1.1, 43748, None)
18/06/26 03:21:38 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.2:35775 with 912.3 MB RAM, BlockManagerId(0, 10.0.1.2, 35775, None)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.1:43748 (size: 25.2 KB, free: 912.3 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.2:35775 (size: 25.2 KB, free: 912.3 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.1:43748 (size: 3.5 MB, free: 908.7 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 908.7 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.2:35775 (size: 3.5 MB, free: 908.7 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 908.7 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 908.7 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 908.7 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.1.2:35775 (size: 6.8 KB, free: 908.7 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.2:35775 (size: 25.2 KB, free: 908.7 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.1.1:43748 (size: 6.8 KB, free: 908.7 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.1:43748 (size: 25.2 KB, free: 908.7 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 908.6 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 908.6 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 908.6 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 908.6 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 908.6 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 908.6 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 908.6 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 908.6 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 908.6 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 908.6 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 908.6 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 908.6 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 908.5 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 908.5 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 908.5 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 908.5 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.1:43748 (size: 4.0 MB, free: 904.5 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.1.1:43748 (size: 48.1 KB, free: 904.5 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.1.1:43748 (size: 6.8 KB, free: 904.5 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 908.5 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.1.1:43748 (size: 48.4 KB, free: 904.4 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 908.5 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 904.4 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.2:35775 (size: 4.0 MB, free: 904.5 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 904.4 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.1.2:35775 (size: 48.1 KB, free: 904.5 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.1.2:35775 (size: 6.8 KB, free: 904.5 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.1:43748 (size: 3.5 KB, free: 904.4 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.1.2:35775 (size: 48.4 KB, free: 904.4 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_14_piece1 in memory on 10.0.1.1:43748 (size: 3.1 MB, free: 901.2 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.1:43748 (size: 69.3 KB, free: 901.2 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 904.4 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.1.1:43748 (size: 48.0 KB, free: 901.1 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 904.4 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.1.1:43748 (size: 48.2 KB, free: 901.1 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 901.0 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.2:35775 (size: 3.5 KB, free: 904.4 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 901.0 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_14_piece1 in memory on 10.0.1.2:35775 (size: 3.1 MB, free: 901.2 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.1:43748 (size: 3.5 KB, free: 901.0 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.2:35775 (size: 69.3 KB, free: 901.2 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.1.1:43748 (size: 6.8 KB, free: 901.0 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.1.2:35775 (size: 48.0 KB, free: 901.1 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.1.2:35775 (size: 48.2 KB, free: 901.1 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 901.0 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 901.0 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.2:35775 (size: 3.5 KB, free: 901.0 MB)
18/06/26 03:21:38 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.1.2:35775 (size: 6.8 KB, free: 901.0 MB)
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:39 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.2:55800
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:39 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 10.1 (TID 67, 10.0.1.2, executor 0): FetchFailed(null, shuffleId=1, mapId=-1, reduceId=1, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1414)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1410)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:62)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

)
18/06/26 03:21:39 INFO scheduler.TaskSetManager: Task 6.0 in stage 10.1 (TID 67) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:39 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 10 (processCmd at CliDriver.java:376) as failed due to a fetch failure from ShuffleMapStage 6 (processCmd at CliDriver.java:376)
18/06/26 03:21:39 INFO scheduler.DAGScheduler: ShuffleMapStage 10 (processCmd at CliDriver.java:376) failed in 2.057 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1414)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1410)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:62)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:39 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 6 (processCmd at CliDriver.java:376) and ShuffleMapStage 10 (processCmd at CliDriver.java:376) due to fetch failure
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:39 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 10.1 (TID 69, 10.0.1.2, executor 0): FetchFailed(null, shuffleId=1, mapId=-1, reduceId=3, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1414)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1410)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:62)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

)
18/06/26 03:21:39 INFO scheduler.TaskSetManager: Task 8.0 in stage 10.1 (TID 69) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:39 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.1:58478
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:39 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 10.1 (TID 66, 10.0.1.1, executor 1): FetchFailed(null, shuffleId=1, mapId=-1, reduceId=0, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1414)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1410)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:62)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

)
18/06/26 03:21:39 INFO scheduler.TaskSetManager: Task 5.0 in stage 10.1 (TID 66) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:39 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 10.1 (TID 68, 10.0.1.1, executor 1): FetchFailed(null, shuffleId=1, mapId=-1, reduceId=2, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1414)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1410)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:62)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

)
18/06/26 03:21:39 INFO scheduler.TaskSetManager: Task 7.0 in stage 10.1 (TID 68) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 10.1, whose tasks have all completed, from pool 
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:39 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.2 (TID 70) in 696 ms on 10.0.1.1 (executor 1) (1/2)
18/06/26 03:21:39 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:39 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/2b/shuffle_1_0_0.data
18/06/26 03:21:39 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0f/shuffle_1_0_0.index
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:39 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.2 (TID 71) in 701 ms on 10.0.1.2 (executor 0) (2/2)
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.2, whose tasks have all completed, from pool 
18/06/26 03:21:39 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:39 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0a/shuffle_1_1_0.data
18/06/26 03:21:39 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index
18/06/26 03:21:39 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:376) finished in 0.708 s
18/06/26 03:21:39 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:21:39 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 5)
18/06/26 03:21:39 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 12, ShuffleMapStage 7, ShuffleMapStage 11, ShuffleMapStage 8)
18/06/26 03:21:39 INFO scheduler.DAGScheduler: failed: Set(ShuffleMapStage 6, ShuffleMapStage 10)
18/06/26 03:21:39 INFO scheduler.DAGScheduler: Resubmitting failed stages
18/06/26 03:21:39 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[48] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:39 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 13.7 KB, free 332.4 MB)
18/06/26 03:21:39 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 6.8 KB, free 332.4 MB)
18/06/26 03:21:39 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on dc1master-lan1:38214 (size: 6.8 KB, free: 397.6 MB)
18/06/26 03:21:39 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:39 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:376) finished in Unknown s
18/06/26 03:21:39 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[76] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:39 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 19.0 KB, free 332.4 MB)
18/06/26 03:21:39 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 8.4 KB, free 332.4 MB)
18/06/26 03:21:39 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on dc1master-lan1:38214 (size: 8.4 KB, free: 397.6 MB)
18/06/26 03:21:39 INFO spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:39 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[76] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: Adding task set 9.2 with 6 tasks
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:39 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.2 (TID 78, 10.0.1.1, executor 1, partition 0, ANY, 7907 bytes)
18/06/26 03:21:39 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:39 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.2 (TID 79, 10.0.1.2, executor 0, partition 1, ANY, 7907 bytes)
18/06/26 03:21:39 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:39 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 9.2 (TID 80, 10.0.1.1, executor 1, partition 2, ANY, 7907 bytes)
18/06/26 03:21:39 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:39 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 9.2 (TID 81, 10.0.1.2, executor 0, partition 3, ANY, 7907 bytes)
18/06/26 03:21:39 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:39 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 9.2 (TID 82, 10.0.1.1, executor 1, partition 4, ANY, 7907 bytes)
18/06/26 03:21:39 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:39 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 9.2 (TID 83, 10.0.1.2, executor 0, partition 5, ANY, 7907 bytes)
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:39 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 6
18/06/26 03:21:39 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:39 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 901.0 MB)
18/06/26 03:21:39 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 901.0 MB)
18/06/26 03:21:40 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:40 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:40 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:40 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:40 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:40 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:40 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:40 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:40 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 5.2 (TID 77) in 1913 ms on 10.0.1.1 (executor 1) (1/6)
18/06/26 03:21:40 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:40 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/27/shuffle_0_5_0.data
18/06/26 03:21:40 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/35/shuffle_0_5_0.index
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.2 (TID 73) in 2264 ms on 10.0.1.1 (executor 1) (2/6)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/15/shuffle_0_1_0.data
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0f/shuffle_0_1_0.index
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 338
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 475
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 348
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 385
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 424
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 489
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 442
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 428
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 342
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 311
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 363
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 413
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 448
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 323
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 5.2 (TID 75) in 2310 ms on 10.0.1.1 (executor 1) (3/6)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/29/shuffle_0_3_0.data
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0d/shuffle_0_3_0.index
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on dc1master-lan1:38214 in memory (size: 6.8 KB, free: 397.6 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.1.2:35775 in memory (size: 6.8 KB, free: 901.0 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.1.1:43748 in memory (size: 6.8 KB, free: 901.0 MB)
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 314
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 461
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 418
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 401
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 366
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 476
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 361
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 455
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 378
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 364
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 310
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 395
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 334
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 354
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 370
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 450
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on dc1master-lan1:38214 in memory (size: 8.4 KB, free: 397.6 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.1.1:43748 in memory (size: 8.4 KB, free: 901.0 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.1.2:35775 in memory (size: 8.4 KB, free: 901.0 MB)
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 356
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 373
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 399
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 353
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 482
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on dc1master-lan1:38214 in memory (size: 48.4 KB, free: 397.6 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.0.1.1:43748 in memory (size: 48.4 KB, free: 901.1 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.0.1.2:35775 in memory (size: 48.4 KB, free: 901.1 MB)
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 339
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 563
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 397
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 367
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 352
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 443
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 305
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 403
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 345
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 328
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 572
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 559
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 306
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 470
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 321
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 377
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 416
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 315
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 557
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 575
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 335
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 375
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 320
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 376
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 360
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on dc1master-lan1:38214 in memory (size: 48.2 KB, free: 397.7 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.1.1:43748 in memory (size: 48.2 KB, free: 901.1 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.1.2:35775 in memory (size: 48.2 KB, free: 901.1 MB)
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 568
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 400
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 479
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 382
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 425
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 368
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 472
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 343
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 319
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 445
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 471
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 480
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 396
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 497
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 423
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 453
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on dc1master-lan1:38214 in memory (size: 48.0 KB, free: 397.7 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.1.1:43748 in memory (size: 48.0 KB, free: 901.1 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.1.2:35775 in memory (size: 48.0 KB, free: 901.1 MB)
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 487
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 464
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 409
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 432
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 565
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 576
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 564
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 488
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 486
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 431
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 571
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 322
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 332
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 420
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 577
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 324
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on dc1master-lan1:38214 in memory (size: 6.8 KB, free: 397.7 MB)
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 327
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 490
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 503
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 313
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 456
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 501
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 309
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 493
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 446
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 427
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 417
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 500
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 337
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 467
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 574
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 438
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 459
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 380
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 419
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 429
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 496
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 560
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 573
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 350
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 579
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 478
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 316
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 407
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 468
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 491
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 318
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 349
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 447
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 430
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 362
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 329
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 567
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 441
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 495
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 570
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 454
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 440
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 344
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 433
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 578
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 415
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 561
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 404
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 410
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 359
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 436
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 408
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 331
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 372
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 412
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 386
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on dc1master-lan1:38214 in memory (size: 48.1 KB, free: 397.8 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.0.1.1:43748 in memory (size: 48.1 KB, free: 901.2 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.0.1.2:35775 in memory (size: 48.1 KB, free: 901.2 MB)
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 421
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 406
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 469
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 465
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 369
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 393
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 477
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 494
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 569
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 379
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 351
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 466
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 394
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 374
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 451
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 462
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 383
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 336
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 346
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 463
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 341
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 391
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 326
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 325
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 498
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 330
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 411
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 381
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 340
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 422
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 474
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 392
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 502
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 387
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 357
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 444
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on dc1master-lan1:38214 in memory (size: 6.8 KB, free: 397.8 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.0.1.1:43748 in memory (size: 6.8 KB, free: 901.2 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.0.1.2:35775 in memory (size: 6.8 KB, free: 901.2 MB)
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 333
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 312
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 499
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 566
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 402
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 452
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on dc1master-lan1:38214 in memory (size: 8.4 KB, free: 397.8 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.0.1.1:43748 in memory (size: 8.4 KB, free: 901.2 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.0.1.2:35775 in memory (size: 8.4 KB, free: 901.2 MB)
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 562
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 485
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 365
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 414
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 384
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 504
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 389
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 308
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 458
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 555
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 355
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 483
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 390
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 437
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 347
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 457
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 492
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 307
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 558
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 398
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 435
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 388
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 484
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 371
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 473
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 426
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 405
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 460
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 434
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 556
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 449
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 481
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 317
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 439
18/06/26 03:21:41 INFO spark.ContextCleaner: Cleaned accumulator 358
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 5.2 (TID 76) in 2408 ms on 10.0.1.2 (executor 0) (4/6)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/38/shuffle_0_4_0.data
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0c/shuffle_0_4_0.index
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.2 (TID 74) in 2416 ms on 10.0.1.2 (executor 0) (5/6)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/36/shuffle_0_2_0.data
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_0_2_0.index
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.2 (TID 72) in 2448 ms on 10.0.1.2 (executor 0) (6/6)
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.2, whose tasks have all completed, from pool 
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0c/shuffle_0_0_0.data
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/30/shuffle_0_0_0.index
18/06/26 03:21:41 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:376) finished in 2.454 s
18/06/26 03:21:41 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:21:41 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 9)
18/06/26 03:21:41 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 12, ShuffleMapStage 10, ShuffleMapStage 7, ShuffleMapStage 11, ShuffleMapStage 8)
18/06/26 03:21:41 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:21:41 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[60] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140711
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140711
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143907
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143907
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138095
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138095
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 137127
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 137127
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 121819
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 121819
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144976
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144976
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142734
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142734
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140701
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140701
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139040
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139040
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 123992
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 123992
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142253
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142253
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139836
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139836
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140708
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140708
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 137187
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 137187
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 117413
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 117413
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143012
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143012
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144814
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144814
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140990
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140990
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138697
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138697
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 116604
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 116604
18/06/26 03:21:41 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 126.5 KB, free 333.1 MB)
18/06/26 03:21:41 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 48.1 KB, free 333.0 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on dc1master-lan1:38214 (size: 48.1 KB, free: 397.8 MB)
18/06/26 03:21:41 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:41 INFO scheduler.DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[60] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: Adding task set 7.2 with 12 tasks
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 7.2 (TID 84, 10.0.1.1, executor 1, partition 5, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:41 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 7.2 (TID 85, 10.0.1.2, executor 0, partition 6, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:41 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 7.2 (TID 86, 10.0.1.1, executor 1, partition 7, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:41 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:41 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 7.2 (TID 87, 10.0.1.2, executor 0, partition 8, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 4
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:41 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:41 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:41 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:41 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:41 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:41 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:41 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:41 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.1.1:43748 (size: 48.1 KB, free: 901.2 MB)
18/06/26 03:21:41 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.1.2:35775 (size: 48.1 KB, free: 901.2 MB)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:41 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 9.2 (TID 83) in 1991 ms on 10.0.1.2 (executor 0) (1/6)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/1d/shuffle_4_5_0.data
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/39/shuffle_4_5_0.index
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 9.2 (TID 82) in 2166 ms on 10.0.1.1 (executor 1) (2/6)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/3c/shuffle_4_4_0.data
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/08/shuffle_4_4_0.index
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.2 (TID 78) in 2176 ms on 10.0.1.1 (executor 1) (3/6)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/38/shuffle_4_0_0.data
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0c/shuffle_4_0_0.index
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 9.2 (TID 80) in 2204 ms on 10.0.1.1 (executor 1) (4/6)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/3a/shuffle_4_2_0.data
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0a/shuffle_4_2_0.index
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 9.2 (TID 81) in 2327 ms on 10.0.1.2 (executor 0) (5/6)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/25/shuffle_4_3_0.data
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/37/shuffle_4_3_0.index
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:41 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.2 (TID 79) in 2371 ms on 10.0.1.2 (executor 0) (6/6)
18/06/26 03:21:41 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.2, whose tasks have all completed, from pool 
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/27/shuffle_4_1_0.data
18/06/26 03:21:41 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/35/shuffle_4_1_0.index
18/06/26 03:21:41 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (processCmd at CliDriver.java:376) finished in 2.377 s
18/06/26 03:21:41 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:21:41 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 7)
18/06/26 03:21:41 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 12, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 8)
18/06/26 03:21:41 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:21:41 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[89] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 141775
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 141775
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142237
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142237
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143487
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143487
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142800
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142800
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 116037
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 116037
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142735
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142735
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140513
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140513
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142608
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142608
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 141095
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 141095
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 117229
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 117229
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143225
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143225
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144994
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144994
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 141700
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 141700
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143486
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143486
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 114415
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 114415
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143173
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143173
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 135781
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 135781
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145177
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145177
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140068
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140068
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 117509
18/06/26 03:21:41 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 117509
18/06/26 03:21:41 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 127.2 KB, free 332.9 MB)
18/06/26 03:21:42 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 48.4 KB, free 332.8 MB)
18/06/26 03:21:42 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on dc1master-lan1:38214 (size: 48.4 KB, free: 397.7 MB)
18/06/26 03:21:42 INFO spark.SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:42 INFO scheduler.DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[89] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
18/06/26 03:21:42 INFO scheduler.TaskSchedulerImpl: Adding task set 10.2 with 12 tasks
18/06/26 03:21:42 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:42 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:42 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 10.2 (TID 88, 10.0.1.1, executor 1, partition 5, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:42 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 10.2 (TID 89, 10.0.1.2, executor 0, partition 6, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:42 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 10.2 (TID 90, 10.0.1.1, executor 1, partition 7, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:42 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:42 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 10.2 (TID 91, 10.0.1.2, executor 0, partition 8, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:42 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 4
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:42 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:42 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:42 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:42 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:42 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:42 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.1.2:35775 (size: 48.4 KB, free: 901.1 MB)
18/06/26 03:21:42 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:42 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:42 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:42 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:42 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.1.1:43748 (size: 48.4 KB, free: 901.1 MB)
18/06/26 03:21:42 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:42 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:42 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:42 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:42 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:42 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:42 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:42 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:42 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 142995
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 144511
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 145163
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 143544
18/06/26 03:21:42 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 140711
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 144976
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 142253
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 143012
18/06/26 03:21:42 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 143907
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 142734
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 139836
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 144814
18/06/26 03:21:42 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 138095
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 140701
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 140708
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 140990
18/06/26 03:21:42 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 137127
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 139040
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 137187
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 138697
18/06/26 03:21:42 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 121819
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 123992
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 117413
18/06/26 03:21:42 INFO scheduler.RawMapStatus: 116604
18/06/26 03:21:42 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.2:55800
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.2:55800
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:42 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.1:58478
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.1:58478
18/06/26 03:21:42 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:43 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:43 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 143863
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 142026
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 138876
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 139953
18/06/26 03:21:43 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 141775
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 142735
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 143225
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 143173
18/06/26 03:21:43 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 142237
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 140513
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 144994
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 135781
18/06/26 03:21:43 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 143487
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 142608
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 141700
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 145177
18/06/26 03:21:43 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 142800
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 141095
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 143486
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 140068
18/06/26 03:21:43 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 116037
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 117229
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 114415
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 117509
18/06/26 03:21:43 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:43 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.2:55800
18/06/26 03:21:43 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.1:58478
18/06/26 03:21:43 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 1289372
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 1282786
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 1285962
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 1291904
18/06/26 03:21:43 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 1282137
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 1284311
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 1280329
18/06/26 03:21:43 INFO scheduler.RawMapStatus: 1283523
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:43 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 7.2 (TID 86, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 43748, None), shuffleId=1, mapId=1, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:43 INFO scheduler.TaskSetManager: Task 7.0 in stage 7.2 (TID 86) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:43 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 7 (processCmd at CliDriver.java:376) as failed due to a fetch failure from ShuffleMapStage 6 (processCmd at CliDriver.java:376)
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:43 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (processCmd at CliDriver.java:376) failed in 2.061 s due to org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:43 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 6 (processCmd at CliDriver.java:376) and ShuffleMapStage 7 (processCmd at CliDriver.java:376) due to fetch failure
18/06/26 03:21:43 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 20)
18/06/26 03:21:43 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
18/06/26 03:21:43 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 7.2 (TID 84, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 43748, None), shuffleId=1, mapId=1, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:43 INFO scheduler.TaskSetManager: Task 5.0 in stage 7.2 (TID 84) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:43 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 10.0.1.1, 43748, None)
18/06/26 03:21:43 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor
18/06/26 03:21:43 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 1 (epoch 20)
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:43 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 7.2 (TID 85, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 35775, None), shuffleId=1, mapId=0, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0f/shuffle_1_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0f/shuffle_1_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:43 INFO scheduler.TaskSetManager: Task 6.0 in stage 7.2 (TID 85) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:43 INFO scheduler.DAGScheduler: Executor lost: 0 (epoch 20)
18/06/26 03:21:43 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
18/06/26 03:21:43 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.0.1.2, 35775, None)
18/06/26 03:21:43 INFO storage.BlockManagerMaster: Removed 0 successfully in removeExecutor
18/06/26 03:21:43 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 0 (epoch 20)
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:43 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 7.2 (TID 87, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 35775, None), shuffleId=1, mapId=0, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0f/shuffle_1_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0f/shuffle_1_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:43 INFO scheduler.TaskSetManager: Task 8.0 in stage 7.2 (TID 87) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.2, whose tasks have all completed, from pool 
18/06/26 03:21:43 INFO scheduler.DAGScheduler: Resubmitting failed stages
18/06/26 03:21:43 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[48] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:43 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 13.7 KB, free 332.8 MB)
18/06/26 03:21:43 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 6.8 KB, free 332.8 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on dc1master-lan1:38214 (size: 6.8 KB, free: 397.7 MB)
18/06/26 03:21:43 INFO spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:43 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[48] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: Adding task set 6.4 with 2 tasks
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:43 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[40] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:43 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.4 (TID 92, 10.0.1.1, executor 1, partition 0, ANY, 7909 bytes)
18/06/26 03:21:43 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.4 (TID 93, 10.0.1.2, executor 0, partition 1, ANY, 7909 bytes)
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 03:21:43 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 19.0 KB, free 332.8 MB)
18/06/26 03:21:43 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 8.4 KB, free 332.8 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on dc1master-lan1:38214 (size: 8.4 KB, free: 397.7 MB)
18/06/26 03:21:43 INFO spark.SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:43 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[40] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: Adding task set 5.3 with 6 tasks
18/06/26 03:21:43 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:43 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:43 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.3 (TID 94, 10.0.1.2, executor 0, partition 0, ANY, 7907 bytes)
18/06/26 03:21:43 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:43 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.3 (TID 95, 10.0.1.1, executor 1, partition 1, ANY, 7907 bytes)
18/06/26 03:21:43 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:43 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.3 (TID 96, 10.0.1.2, executor 0, partition 2, ANY, 7907 bytes)
18/06/26 03:21:43 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:43 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 5.3 (TID 97, 10.0.1.1, executor 1, partition 3, ANY, 7907 bytes)
18/06/26 03:21:43 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:43 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 5.3 (TID 98, 10.0.1.2, executor 0, partition 4, ANY, 7907 bytes)
18/06/26 03:21:43 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:43 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 5.3 (TID 99, 10.0.1.1, executor 1, partition 5, ANY, 7907 bytes)
18/06/26 03:21:43 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:43 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 6
18/06/26 03:21:43 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.2:35775 with 912.3 MB RAM, BlockManagerId(0, 10.0.1.2, 35775, None)
18/06/26 03:21:43 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.1:43748 with 912.3 MB RAM, BlockManagerId(1, 10.0.1.1, 43748, None)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.2:35775 (size: 25.2 KB, free: 912.3 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.1:43748 (size: 25.2 KB, free: 912.3 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.1:43748 (size: 3.5 MB, free: 908.7 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.2:35775 (size: 3.5 MB, free: 908.7 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 908.7 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 908.7 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 908.7 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.1:43748 (size: 25.2 KB, free: 908.7 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 908.6 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 908.6 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 908.7 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.2:35775 (size: 25.2 KB, free: 908.7 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 908.6 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 908.6 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 908.6 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 908.6 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 908.6 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 908.6 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 908.6 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.1.2:35775 (size: 48.4 KB, free: 908.5 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.1.2:35775 (size: 48.1 KB, free: 908.5 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 908.5 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 908.5 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.2:35775 (size: 25.0 KB, free: 908.4 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.2:35775 (size: 4.0 MB, free: 904.4 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 904.4 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 904.4 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 904.4 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 908.6 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.0.1.2:35775 (size: 6.8 KB, free: 904.4 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.1.1:43748 (size: 48.4 KB, free: 908.5 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.2:35775 (size: 3.5 KB, free: 904.4 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_14_piece1 in memory on 10.0.1.2:35775 (size: 3.1 MB, free: 901.2 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.2:35775 (size: 69.3 KB, free: 901.2 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 901.1 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.1.1:43748 (size: 48.1 KB, free: 908.5 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.2:35775 (size: 25.1 KB, free: 901.1 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 908.5 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.2:35775 (size: 3.5 KB, free: 901.1 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 908.5 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.1.2:35775 (size: 6.8 KB, free: 901.1 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.1:43748 (size: 25.0 KB, free: 908.4 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.1:43748 (size: 4.0 MB, free: 904.4 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 904.4 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 904.4 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 904.4 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.0.1.1:43748 (size: 6.8 KB, free: 904.4 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.1:43748 (size: 3.5 KB, free: 904.4 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_14_piece1 in memory on 10.0.1.1:43748 (size: 3.1 MB, free: 901.2 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.1:43748 (size: 69.3 KB, free: 901.2 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 901.1 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.1:43748 (size: 25.1 KB, free: 901.1 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.1:43748 (size: 3.5 KB, free: 901.1 MB)
18/06/26 03:21:43 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.1.1:43748 (size: 6.8 KB, free: 901.1 MB)
18/06/26 03:21:44 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.2:55800
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:44 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 10.2 (TID 89, 10.0.1.2, executor 0): FetchFailed(null, shuffleId=1, mapId=-1, reduceId=1, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1414)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1410)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:62)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

)
18/06/26 03:21:44 INFO scheduler.TaskSetManager: Task 6.0 in stage 10.2 (TID 89) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:44 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 10 (processCmd at CliDriver.java:376) as failed due to a fetch failure from ShuffleMapStage 6 (processCmd at CliDriver.java:376)
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:44 INFO scheduler.DAGScheduler: ShuffleMapStage 10 (processCmd at CliDriver.java:376) failed in 2.038 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1414)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1410)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:62)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:44 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 6 (processCmd at CliDriver.java:376) and ShuffleMapStage 10 (processCmd at CliDriver.java:376) due to fetch failure
18/06/26 03:21:44 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 10.2 (TID 91, 10.0.1.2, executor 0): FetchFailed(null, shuffleId=1, mapId=-1, reduceId=3, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1414)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1410)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:62)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

)
18/06/26 03:21:44 INFO scheduler.TaskSetManager: Task 8.0 in stage 10.2 (TID 91) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:44 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.1:58478
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:44 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 10.2 (TID 88, 10.0.1.1, executor 1): FetchFailed(null, shuffleId=1, mapId=-1, reduceId=0, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1414)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1410)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:62)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

)
18/06/26 03:21:44 INFO scheduler.TaskSetManager: Task 5.0 in stage 10.2 (TID 88) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:44 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 10.2 (TID 90, 10.0.1.1, executor 1): FetchFailed(null, shuffleId=1, mapId=-1, reduceId=2, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1414)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:1410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1410)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:62)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

)
18/06/26 03:21:44 INFO scheduler.TaskSetManager: Task 7.0 in stage 10.2 (TID 90) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 10.2, whose tasks have all completed, from pool 
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:44 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.4 (TID 92) in 613 ms on 10.0.1.1 (executor 1) (1/2)
18/06/26 03:21:44 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:44 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/2b/shuffle_1_0_0.data
18/06/26 03:21:44 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0f/shuffle_1_0_0.index
18/06/26 03:21:44 INFO scheduler.DAGScheduler: Resubmitting failed stages
18/06/26 03:21:44 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[76] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:44 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 19.0 KB, free 332.8 MB)
18/06/26 03:21:44 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 8.4 KB, free 332.8 MB)
18/06/26 03:21:44 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on dc1master-lan1:38214 (size: 8.4 KB, free: 397.7 MB)
18/06/26 03:21:44 INFO spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:44 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[76] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: Adding task set 9.3 with 6 tasks
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:44 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.3 (TID 100, 10.0.1.1, executor 1, partition 0, ANY, 7907 bytes)
18/06/26 03:21:44 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:44 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.3 (TID 101, 10.0.1.2, executor 0, partition 1, ANY, 7907 bytes)
18/06/26 03:21:44 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:44 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 9.3 (TID 102, 10.0.1.1, executor 1, partition 2, ANY, 7907 bytes)
18/06/26 03:21:44 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:44 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 9.3 (TID 103, 10.0.1.2, executor 0, partition 3, ANY, 7907 bytes)
18/06/26 03:21:44 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:44 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 9.3 (TID 104, 10.0.1.1, executor 1, partition 4, ANY, 7907 bytes)
18/06/26 03:21:44 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:44 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 9.3 (TID 105, 10.0.1.2, executor 0, partition 5, ANY, 7907 bytes)
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:44 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 9
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 6
18/06/26 03:21:44 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.0.1.1:43748 (size: 8.4 KB, free: 901.1 MB)
18/06/26 03:21:44 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.0.1.2:35775 (size: 8.4 KB, free: 901.1 MB)
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:44 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.4 (TID 93) in 724 ms on 10.0.1.2 (executor 0) (2/2)
18/06/26 03:21:44 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.4, whose tasks have all completed, from pool 
18/06/26 03:21:44 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:44 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0a/shuffle_1_1_0.data
18/06/26 03:21:44 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index
18/06/26 03:21:44 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:376) finished in 0.732 s
18/06/26 03:21:44 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:21:44 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 9, ShuffleMapStage 5)
18/06/26 03:21:44 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 12, ShuffleMapStage 10, ShuffleMapStage 7, ShuffleMapStage 11, ShuffleMapStage 8)
18/06/26 03:21:44 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:45 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 5.3 (TID 99) in 1902 ms on 10.0.1.1 (executor 1) (1/6)
18/06/26 03:21:45 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:45 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/27/shuffle_0_5_0.data
18/06/26 03:21:45 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/35/shuffle_0_5_0.index
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:45 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.3 (TID 95) in 2294 ms on 10.0.1.1 (executor 1) (2/6)
18/06/26 03:21:45 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:45 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/15/shuffle_0_1_0.data
18/06/26 03:21:45 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0f/shuffle_0_1_0.index
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:45 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 5.3 (TID 97) in 2309 ms on 10.0.1.1 (executor 1) (3/6)
18/06/26 03:21:45 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:45 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/29/shuffle_0_3_0.data
18/06/26 03:21:45 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0d/shuffle_0_3_0.index
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:45 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:45 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.3 (TID 96) in 2315 ms on 10.0.1.2 (executor 0) (4/6)
18/06/26 03:21:45 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:45 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/36/shuffle_0_2_0.data
18/06/26 03:21:45 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_0_2_0.index
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 5.3 (TID 98) in 2398 ms on 10.0.1.2 (executor 0) (5/6)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/38/shuffle_0_4_0.data
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0c/shuffle_0_4_0.index
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.3 (TID 94) in 2416 ms on 10.0.1.2 (executor 0) (6/6)
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.3, whose tasks have all completed, from pool 
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/0c/shuffle_0_0_0.data
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/30/shuffle_0_0_0.index
18/06/26 03:21:46 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:376) finished in 2.422 s
18/06/26 03:21:46 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:21:46 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 9)
18/06/26 03:21:46 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 12, ShuffleMapStage 10, ShuffleMapStage 7, ShuffleMapStage 11, ShuffleMapStage 8)
18/06/26 03:21:46 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:21:46 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[60] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140711
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140711
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143907
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143907
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138095
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138095
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 137127
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 137127
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 121819
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 121819
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144976
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144976
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142734
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142734
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140701
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140701
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139040
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139040
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 123992
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 123992
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142253
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142253
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139836
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139836
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140708
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140708
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 137187
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 137187
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 117413
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 117413
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143012
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143012
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144814
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144814
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140990
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140990
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138697
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138697
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 116604
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 116604
18/06/26 03:21:46 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 126.5 KB, free 332.6 MB)
18/06/26 03:21:46 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 48.1 KB, free 332.6 MB)
18/06/26 03:21:46 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on dc1master-lan1:38214 (size: 48.1 KB, free: 397.6 MB)
18/06/26 03:21:46 INFO spark.SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:46 INFO scheduler.DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[60] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: Adding task set 7.3 with 12 tasks
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 7.3 (TID 106, 10.0.1.2, executor 0, partition 5, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 7.3 (TID 107, 10.0.1.1, executor 1, partition 6, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 7.3 (TID 108, 10.0.1.2, executor 0, partition 7, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 7.3 (TID 109, 10.0.1.1, executor 1, partition 8, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 4
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.0.1.1:43748 (size: 48.1 KB, free: 901.0 MB)
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:46 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.0.1.2:35775 (size: 48.1 KB, free: 901.0 MB)
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142995
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144511
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145163
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143544
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:46 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 9.3 (TID 105) in 1963 ms on 10.0.1.2 (executor 0) (1/6)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/1d/shuffle_4_5_0.data
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/39/shuffle_4_5_0.index
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 9.3 (TID 103) in 2191 ms on 10.0.1.2 (executor 0) (2/6)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/25/shuffle_4_3_0.data
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/37/shuffle_4_3_0.index
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 9.3 (TID 104) in 2229 ms on 10.0.1.1 (executor 1) (3/6)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/3c/shuffle_4_4_0.data
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/08/shuffle_4_4_0.index
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 9.3 (TID 102) in 2270 ms on 10.0.1.1 (executor 1) (4/6)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/3a/shuffle_4_2_0.data
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0a/shuffle_4_2_0.index
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.3 (TID 100) in 2278 ms on 10.0.1.1 (executor 1) (5/6)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/38/shuffle_4_0_0.data
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0c/shuffle_4_0_0.index
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.3 (TID 101) in 2329 ms on 10.0.1.2 (executor 0) (6/6)
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.3, whose tasks have all completed, from pool 
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/27/shuffle_4_1_0.data
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/35/shuffle_4_1_0.index
18/06/26 03:21:46 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (processCmd at CliDriver.java:376) finished in 2.334 s
18/06/26 03:21:46 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 03:21:46 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 7)
18/06/26 03:21:46 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 12, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 8)
18/06/26 03:21:46 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 03:21:46 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[89] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 141775
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 141775
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142237
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142237
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143487
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143487
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142800
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142800
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 116037
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 116037
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142735
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142735
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140513
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140513
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142608
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142608
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 141095
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 141095
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 117229
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 117229
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143225
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143225
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 144994
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 144994
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 141700
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 141700
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143486
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143486
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 114415
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 114415
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143173
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143173
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 135781
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 135781
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 145177
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 145177
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 140068
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 140068
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 117509
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 117509
18/06/26 03:21:46 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 127.2 KB, free 332.5 MB)
18/06/26 03:21:46 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 48.4 KB, free 332.4 MB)
18/06/26 03:21:46 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on dc1master-lan1:38214 (size: 48.4 KB, free: 397.6 MB)
18/06/26 03:21:46 INFO spark.SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1074
18/06/26 03:21:46 INFO scheduler.DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[89] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: Adding task set 10.3 with 12 tasks
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 10.3 (TID 110, 10.0.1.2, executor 0, partition 5, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 10.3 (TID 111, 10.0.1.1, executor 1, partition 6, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 10.3 (TID 112, 10.0.1.2, executor 0, partition 7, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:46 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 10.3 (TID 113, 10.0.1.1, executor 1, partition 8, NODE_LOCAL, 8184 bytes)
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 4
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 10
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 9
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 4
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 03:21:46 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626032113-0065
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 4
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 143863
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 142026
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 138876
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 139953
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 139953
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 03:21:46 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.0.1.2:35775 (size: 48.4 KB, free: 901.0 MB)
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 03:21:46 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 03:21:46 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.0.1.1:43748 (size: 48.4 KB, free: 901.0 MB)
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1289372
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1282786
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1285962
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 1291904
18/06/26 03:21:46 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 03:21:46 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:46 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:46 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:47 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.1:58478
18/06/26 03:21:47 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 142995
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 144511
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 145163
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 143544
18/06/26 03:21:47 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 140711
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 144976
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 142253
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 143012
18/06/26 03:21:47 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 143907
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 142734
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 139836
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 144814
18/06/26 03:21:47 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 138095
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 140701
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 140708
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 140990
18/06/26 03:21:47 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 137127
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 139040
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 137187
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 138697
18/06/26 03:21:47 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 121819
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 123992
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 117413
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 116604
18/06/26 03:21:47 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.2:55800
18/06/26 03:21:47 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.1:58478
18/06/26 03:21:47 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.1:58478
18/06/26 03:21:47 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:47 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.2:55800
18/06/26 03:21:47 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.2:55800
18/06/26 03:21:47 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 03:21:47 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:47 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:47 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:47 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:47 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.1.1:58478
18/06/26 03:21:47 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 143863
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 142026
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 138876
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 139953
18/06/26 03:21:47 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 141775
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 142735
18/06/26 03:21:47 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.1.2:55800
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 143225
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 143173
18/06/26 03:21:47 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 142237
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 140513
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 144994
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 135781
18/06/26 03:21:47 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 143487
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 142608
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 141700
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 145177
18/06/26 03:21:47 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 142800
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 141095
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 143486
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 140068
18/06/26 03:21:47 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 116037
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 117229
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 114415
18/06/26 03:21:47 INFO scheduler.RawMapStatus: 117509
18/06/26 03:21:48 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.1:58478
18/06/26 03:21:48 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:48 INFO scheduler.RawMapStatus: 1289372
18/06/26 03:21:48 INFO scheduler.RawMapStatus: 1282786
18/06/26 03:21:48 INFO scheduler.RawMapStatus: 1285962
18/06/26 03:21:48 INFO scheduler.RawMapStatus: 1291904
18/06/26 03:21:48 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 03:21:48 INFO scheduler.RawMapStatus: 1282137
18/06/26 03:21:48 INFO scheduler.RawMapStatus: 1284311
18/06/26 03:21:48 INFO scheduler.RawMapStatus: 1280329
18/06/26 03:21:48 INFO scheduler.RawMapStatus: 1283523
18/06/26 03:21:48 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.2:55800
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:48 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 7.3 (TID 109, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 43748, None), shuffleId=1, mapId=1, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:48 INFO scheduler.TaskSetManager: Task 8.0 in stage 7.3 (TID 109) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:48 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 7 (processCmd at CliDriver.java:376) as failed due to a fetch failure from ShuffleMapStage 6 (processCmd at CliDriver.java:376)
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:48 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (processCmd at CliDriver.java:376) failed in 2.047 s due to org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

18/06/26 03:21:48 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 7.3 (TID 107, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 43748, None), shuffleId=1, mapId=1, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:48 INFO scheduler.TaskSetManager: Task 6.0 in stage 7.3 (TID 107) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: Cancelling stage 10
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: Stage 10 was cancelled
18/06/26 03:21:48 INFO scheduler.DAGScheduler: ShuffleMapStage 10 (processCmd at CliDriver.java:376) failed in 1.508 s due to Job aborted due to stage failure: ShuffleMapStage 7 (processCmd at CliDriver.java:376) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64) 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30) 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source) 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618) 	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83) 	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811) 	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686) 	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213) 	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214) 	at java.nio.file.Files.newByteChannel(Files.java:361) 	at java.nio.file.Files.newByteChannel(Files.java:407) 	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241) 	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58) 	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	... 22 more 
18/06/26 03:21:48 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 29)
18/06/26 03:21:48 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
18/06/26 03:21:48 INFO scheduler.DAGScheduler: Job 5 failed: processCmd at CliDriver.java:376, took 23.232706 s
18/06/26 03:21:48 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 10.0.1.1, 43748, None)
18/06/26 03:21:48 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor
18/06/26 03:21:48 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 1 (epoch 29)
18/06/26 03:21:48 ERROR thriftserver.SparkSQLDriver: Failed in [WITH all_sales AS (
 SELECT d_year
       ,i_brand_id
       ,i_class_id
       ,i_category_id
       ,i_manufact_id
       ,SUM(sales_cnt) AS sales_cnt
       ,SUM(sales_amt) AS sales_amt
 FROM (SELECT d_year
             ,i_brand_id
             ,i_class_id
             ,i_category_id
             ,i_manufact_id
             ,cs_quantity - COALESCE(cr_return_quantity,0) AS sales_cnt
             ,cs_ext_sales_price - COALESCE(cr_return_amount,0.0) AS sales_amt
       FROM catalog_sales JOIN item ON i_item_sk=cs_item_sk
                          JOIN date_dim ON d_date_sk=cs_sold_date_sk
                          LEFT JOIN catalog_returns ON (cs_order_number=cr_order_number 
                                                    AND cs_item_sk=cr_item_sk)
       WHERE i_category='Sports'
       UNION ALL
       SELECT d_year
             ,i_brand_id
             ,i_class_id
             ,i_category_id
             ,i_manufact_id
             ,ss_quantity - COALESCE(sr_return_quantity,0) AS sales_cnt
             ,ss_ext_sales_price - COALESCE(sr_return_amt,0.0) AS sales_amt
       FROM store_sales JOIN item ON i_item_sk=ss_item_sk
                        JOIN date_dim ON d_date_sk=ss_sold_date_sk
                        LEFT JOIN store_returns ON (ss_ticket_number=sr_ticket_number 
                                                AND ss_item_sk=sr_item_sk)
       WHERE i_category='Sports'
       UNION ALL
       SELECT d_year
             ,i_brand_id
             ,i_class_id
             ,i_category_id
             ,i_manufact_id
             ,ws_quantity - COALESCE(wr_return_quantity,0) AS sales_cnt
             ,ws_ext_sales_price - COALESCE(wr_return_amt,0.0) AS sales_amt
       FROM web_sales JOIN item ON i_item_sk=ws_item_sk
                      JOIN date_dim ON d_date_sk=ws_sold_date_sk
                      LEFT JOIN web_returns ON (ws_order_number=wr_order_number 
                                            AND ws_item_sk=wr_item_sk)
       WHERE i_category='Sports') sales_detail
 GROUP BY d_year, i_brand_id, i_class_id, i_category_id, i_manufact_id)
 SELECT  prev_yr.d_year AS prev_year
                          ,curr_yr.d_year AS year
                          ,curr_yr.i_brand_id
                          ,curr_yr.i_class_id
                          ,curr_yr.i_category_id
                          ,curr_yr.i_manufact_id
                          ,prev_yr.sales_cnt AS prev_yr_cnt
                          ,curr_yr.sales_cnt AS curr_yr_cnt
                          ,curr_yr.sales_cnt-prev_yr.sales_cnt AS sales_cnt_diff
                          ,curr_yr.sales_amt-prev_yr.sales_amt AS sales_amt_diff
 FROM all_sales curr_yr, all_sales prev_yr
 WHERE curr_yr.i_brand_id=prev_yr.i_brand_id
   AND curr_yr.i_class_id=prev_yr.i_class_id
   AND curr_yr.i_category_id=prev_yr.i_category_id
   AND curr_yr.i_manufact_id=prev_yr.i_manufact_id
   AND curr_yr.d_year=2002
   AND prev_yr.d_year=2002-1
   AND CAST(curr_yr.sales_cnt AS DECIMAL(17,2))/CAST(prev_yr.sales_cnt AS DECIMAL(17,2))<0.9
 ORDER BY sales_cnt_diff
 limit 100]
org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 7 (processCmd at CliDriver.java:376) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64) 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30) 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source) 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618) 	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83) 	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811) 	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686) 	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213) 	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214) 	at java.nio.file.Files.newByteChannel(Files.java:361) 	at java.nio.file.Files.newByteChannel(Files.java:407) 	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241) 	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58) 	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	... 22 more 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1635)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1623)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1622)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1622)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1394)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1853)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1805)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1794)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2030)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2127)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1029)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1011)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1433)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1420)
	at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:324)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:122)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:363)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:198)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:837)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:912)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:923)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:48 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 7.3 (TID 108, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 35775, None), shuffleId=1, mapId=0, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0f/shuffle_1_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0f/shuffle_1_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:48 INFO scheduler.TaskSetManager: Task 7.0 in stage 7.3 (TID 108) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:48 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 7.3 (TID 106, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 35775, None), shuffleId=1, mapId=0, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0f/shuffle_1_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-dccfaae5-f284-478e-8c3a-b27e74668b84/blockmgr-a447c923-d52a-4c5a-bef0-1811e8ec1be0/0f/shuffle_1_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 22 more

)
18/06/26 03:21:48 INFO scheduler.TaskSetManager: Task 5.0 in stage 7.3 (TID 106) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.3, whose tasks have all completed, from pool 
org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 7 (processCmd at CliDriver.java:376) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64) 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30) 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.sort_addToSorter_0$(Unknown Source) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source) 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618) 	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83) 	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:811) 	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:686) 	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:213) 	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:150) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-cd28770d-35e2-4289-b442-06ff3ad1e1d6/blockmgr-a1572a52-717b-4e37-9fa4-b4fa3000f531/32/shuffle_1_1_0.index 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214) 	at java.nio.file.Files.newByteChannel(Files.java:361) 	at java.nio.file.Files.newByteChannel(Files.java:407) 	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241) 	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58) 	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	... 22 more 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1635)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1623)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1622)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1622)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1394)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1853)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1805)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1794)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2030)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2127)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1029)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1011)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1433)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1420)
	at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:324)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:122)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:363)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:198)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:837)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:912)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:923)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

18/06/26 03:21:48 INFO server.AbstractConnector: Stopped Spark@32e5af53{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18/06/26 03:21:48 INFO ui.SparkUI: Stopped Spark web UI at http://dc1master-lan1:4040
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:21:48 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:21:48 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors
18/06/26 03:21:48 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
18/06/26 03:21:48 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/06/26 03:21:48 INFO memory.MemoryStore: MemoryStore cleared
18/06/26 03:21:48 INFO storage.BlockManager: BlockManager stopped
18/06/26 03:21:48 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/06/26 03:21:48 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/06/26 03:21:48 INFO spark.SparkContext: Successfully stopped SparkContext
18/06/26 03:21:48 INFO util.ShutdownHookManager: Shutdown hook called
18/06/26 03:21:48 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-1130938f-9dcc-404f-81d2-bd9a500306bd
18/06/26 03:21:48 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-8a144546-281c-49c3-bba7-fe001b002757
