18/06/26 02:57:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/06/26 02:57:44 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18/06/26 02:57:44 INFO metastore.ObjectStore: ObjectStore, initialize called
18/06/26 02:57:44 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
18/06/26 02:57:44 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored
18/06/26 02:57:46 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
18/06/26 02:57:47 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 02:57:47 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 02:57:47 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 02:57:47 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 02:57:47 INFO DataNucleus.Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18/06/26 02:57:47 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
18/06/26 02:57:47 INFO metastore.ObjectStore: Initialized ObjectStore
18/06/26 02:57:48 INFO metastore.HiveMetaStore: Added admin role in metastore
18/06/26 02:57:48 INFO metastore.HiveMetaStore: Added public role in metastore
18/06/26 02:57:48 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
18/06/26 02:57:48 INFO metastore.HiveMetaStore: 0: get_all_databases
18/06/26 02:57:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_all_databases	
18/06/26 02:57:48 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
18/06/26 02:57:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
18/06/26 02:57:48 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 02:57:48 INFO metastore.HiveMetaStore: 0: get_functions: db=tpcds_text_2 pat=*
18/06/26 02:57:48 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_functions: db=tpcds_text_2 pat=*	
18/06/26 02:57:48 INFO session.SessionState: Created local directory: /tmp/be9a1515-fab9-4e53-99bd-8322870bde83_resources
18/06/26 02:57:48 INFO session.SessionState: Created HDFS directory: /tmp/hive/wentingt/be9a1515-fab9-4e53-99bd-8322870bde83
18/06/26 02:57:48 INFO session.SessionState: Created local directory: /tmp/wentingt/be9a1515-fab9-4e53-99bd-8322870bde83
18/06/26 02:57:48 INFO session.SessionState: Created HDFS directory: /tmp/hive/wentingt/be9a1515-fab9-4e53-99bd-8322870bde83/_tmp_space.db
18/06/26 02:57:49 INFO spark.SparkContext: Running Spark version 2.4.0-SNAPSHOT
18/06/26 02:57:49 INFO spark.SparkContext: Submitted application: SparkSQL::10.0.1.253
18/06/26 02:57:49 INFO spark.SecurityManager: Changing view acls to: wentingt
18/06/26 02:57:49 INFO spark.SecurityManager: Changing modify acls to: wentingt
18/06/26 02:57:49 INFO spark.SecurityManager: Changing view acls groups to: 
18/06/26 02:57:49 INFO spark.SecurityManager: Changing modify acls groups to: 
18/06/26 02:57:49 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(wentingt); groups with view permissions: Set(); users  with modify permissions: Set(wentingt); groups with modify permissions: Set()
18/06/26 02:57:49 INFO util.Utils: Successfully started service 'sparkDriver' on port 46357.
18/06/26 02:57:49 INFO spark.SparkEnv: Registering MapOutputTracker
18/06/26 02:57:49 INFO spark.SparkEnv: Registering BlockManagerMaster
18/06/26 02:57:49 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/06/26 02:57:49 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/06/26 02:57:49 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-03d00b12-f912-467c-8e25-6dcb204f0e3b
18/06/26 02:57:49 INFO memory.MemoryStore: MemoryStore started with capacity 408.9 MB
18/06/26 02:57:49 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/06/26 02:57:49 INFO util.log: Logging initialized @7414ms
18/06/26 02:57:49 INFO server.Server: jetty-9.3.z-SNAPSHOT
18/06/26 02:57:49 INFO server.Server: Started @7484ms
18/06/26 02:57:49 INFO server.AbstractConnector: Started ServerConnector@32e5af53{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18/06/26 02:57:49 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@722787b5{/jobs,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e807e2{/jobs/json,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c995c5d{/jobs/job,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@730bea0{/jobs/job/json,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41a16eb3{/stages,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@677cb96e{/stages/json,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b1252c8{/stages/stage,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51fe7f15{/stages/stage/json,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5873f3f0{/stages/pool,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@684372d0{/stages/pool/json,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63dda940{/storage,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41f964f9{/storage/json,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@652e345{/storage/rdd,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7574d4ad{/storage/rdd/json,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7bede4ea{/environment,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@713999c2{/environment/json,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6060146b{/executors,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33627576{/executors/json,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27bc1d44{/executors/threadDump,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1af677f8{/executors/threadDump/json,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a55fb81{/static,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52f9e8bb{/,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2035d65b{/api,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d1a859c{/jobs/job/kill,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28554ac8{/stages/stage/kill,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://dc1master-lan1:4040
18/06/26 02:57:50 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 0
18/06/26 02:57:50 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:50 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://dc1master-lan1:7077...
18/06/26 02:57:50 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 0
18/06/26 02:57:50 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:50 INFO client.TransportClientFactory: Successfully created connection to dc1master-lan1/10.0.1.253:7077 after 48 ms (0 ms spent in bootstraps)
18/06/26 02:57:50 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20180626025750-0055
18/06/26 02:57:50 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20180626025750-0055/0 on worker-20180626004527-10.0.1.2-38332 (10.0.1.2:38332) with 10 core(s)
18/06/26 02:57:50 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20180626025750-0055/0 on hostPort 10.0.1.2:38332 with 10 core(s), 2.0 GB RAM
18/06/26 02:57:50 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20180626025750-0055/1 on worker-20180626004527-10.0.1.1-34053 (10.0.1.1:34053) with 10 core(s)
18/06/26 02:57:50 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20180626025750-0055/1 on hostPort 10.0.1.1:34053 with 10 core(s), 2.0 GB RAM
18/06/26 02:57:50 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36135.
18/06/26 02:57:50 INFO netty.NettyBlockTransferService: Server created on dc1master-lan1:36135
18/06/26 02:57:50 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/06/26 02:57:50 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20180626025750-0055/0 is now RUNNING
18/06/26 02:57:50 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20180626025750-0055/1 is now RUNNING
18/06/26 02:57:50 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, dc1master-lan1, 36135, None)
18/06/26 02:57:50 INFO storage.BlockManagerMasterEndpoint: Registering block manager dc1master-lan1:36135 with 408.9 MB RAM, BlockManagerId(driver, dc1master-lan1, 36135, None)
18/06/26 02:57:50 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, dc1master-lan1, 36135, None)
18/06/26 02:57:50 INFO storage.BlockManager: external shuffle service port = 7337
18/06/26 02:57:50 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, dc1master-lan1, 36135, None)
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@774f2992{/metrics/json,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO scheduler.EventLoggingListener: Logging events to hdfs://dc1master:9000/user/wentingt/spark-logs/app-20180626025750-0055
18/06/26 02:57:50 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
18/06/26 02:57:50 INFO internal.SharedState: loading hive config file: file:/users/wentingt/spark-terra/conf/hive-site.xml
18/06/26 02:57:50 INFO internal.SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
18/06/26 02:57:50 INFO internal.SharedState: Warehouse path is '/user/hive/warehouse'.
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a631049{/SQL,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@425b5fe2{/SQL/json,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c815fdc{/SQL/execution,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@769b0752{/SQL/execution/json,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65db548{/static/sql,null,AVAILABLE,@Spark}
18/06/26 02:57:50 INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
18/06/26 02:57:50 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
18/06/26 02:57:50 INFO metastore.HiveMetaStore: 0: get_database: default
18/06/26 02:57:50 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: default	
18/06/26 02:57:51 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 0
18/06/26 02:57:51 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:51 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 0
18/06/26 02:57:51 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:51 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
18/06/26 02:57:51 INFO metastore.HiveMetaStore: 0: get_database: global_temp
18/06/26 02:57:51 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: global_temp	
18/06/26 02:57:51 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
18/06/26 02:57:51 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:57:51 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:57:51 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:57:51 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:57:51 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:57:51 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:57:51 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 02:57:51 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 02:57:52 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 0
18/06/26 02:57:52 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:52 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 0
18/06/26 02:57:52 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:52 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=date_dim
18/06/26 02:57:52 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=date_dim	
18/06/26 02:57:52 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.1.2:38110) with ID 0
18/06/26 02:57:52 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:57:52 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:52 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:57:52 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:52 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=customer_address
18/06/26 02:57:52 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=customer_address	
18/06/26 02:57:52 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=item
18/06/26 02:57:52 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=item	
18/06/26 02:57:52 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.1.1:58408) with ID 1
18/06/26 02:57:52 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:57:52 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:52 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:57:52 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:52 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.2:37932 with 912.3 MB RAM, BlockManagerId(0, 10.0.1.2, 37932, None)
18/06/26 02:57:52 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.1:38322 with 912.3 MB RAM, BlockManagerId(1, 10.0.1.1, 38322, None)
18/06/26 02:57:52 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=item
18/06/26 02:57:52 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=item	
18/06/26 02:57:52 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:57:52 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:57:52 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=catalog_sales
18/06/26 02:57:52 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=catalog_sales	
18/06/26 02:57:52 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=date_dim
18/06/26 02:57:52 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=date_dim	
18/06/26 02:57:52 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=customer_address
18/06/26 02:57:52 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=customer_address	
18/06/26 02:57:52 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=item
18/06/26 02:57:52 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=item	
18/06/26 02:57:52 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=item
18/06/26 02:57:52 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=item	
18/06/26 02:57:52 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:57:52 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:57:52 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=web_sales
18/06/26 02:57:52 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=web_sales	
18/06/26 02:57:52 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=date_dim
18/06/26 02:57:52 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=date_dim	
18/06/26 02:57:52 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=customer_address
18/06/26 02:57:52 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=customer_address	
18/06/26 02:57:52 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=item
18/06/26 02:57:52 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=item	
18/06/26 02:57:53 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=item
18/06/26 02:57:53 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=item	
18/06/26 02:57:53 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:57:53 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:53 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:57:53 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:54 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:57:54 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:54 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:57:54 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:54 WARN util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
18/06/26 02:57:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:57:55 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:57:55 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:55 INFO spark.ContextCleaner: Cleaned accumulator 1
18/06/26 02:57:55 INFO spark.ContextCleaner: Cleaned accumulator 3
18/06/26 02:57:55 INFO spark.ContextCleaner: Cleaned accumulator 2
18/06/26 02:57:55 INFO spark.ContextCleaner: Cleaned accumulator 0
18/06/26 02:57:55 INFO codegen.CodeGenerator: Code generated in 261.339494 ms
18/06/26 02:57:56 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:57:56 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:56 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:57:56 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:56 INFO codegen.CodeGenerator: Code generated in 39.013538 ms
18/06/26 02:57:56 INFO codegen.CodeGenerator: Code generated in 40.234822 ms
18/06/26 02:57:56 INFO codegen.CodeGenerator: Code generated in 40.254207 ms
18/06/26 02:57:56 INFO codegen.CodeGenerator: Code generated in 68.428231 ms
18/06/26 02:57:56 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 290.6 KB, free 408.0 MB)
18/06/26 02:57:56 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 291.1 KB, free 408.0 MB)
18/06/26 02:57:56 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 290.9 KB, free 408.0 MB)
18/06/26 02:57:56 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 25.0 KB, free 408.0 MB)
18/06/26 02:57:56 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.9 KB, free 408.0 MB)
18/06/26 02:57:56 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.0 KB, free 408.0 MB)
18/06/26 02:57:56 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on dc1master-lan1:36135 (size: 24.9 KB, free: 408.9 MB)
18/06/26 02:57:56 INFO spark.SparkContext: Created broadcast 2 from 
18/06/26 02:57:56 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on dc1master-lan1:36135 (size: 25.0 KB, free: 408.9 MB)
18/06/26 02:57:56 INFO spark.SparkContext: Created broadcast 1 from 
18/06/26 02:57:56 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on dc1master-lan1:36135 (size: 25.0 KB, free: 408.8 MB)
18/06/26 02:57:56 INFO spark.SparkContext: Created broadcast 0 from 
18/06/26 02:57:56 INFO codegen.CodeGenerator: Code generated in 90.065759 ms
18/06/26 02:57:56 INFO codegen.CodeGenerator: Code generated in 63.69456 ms
18/06/26 02:57:56 INFO spark.ContextCleaner: Cleaned accumulator 9
18/06/26 02:57:56 INFO spark.ContextCleaner: Cleaned accumulator 17
18/06/26 02:57:56 INFO spark.ContextCleaner: Cleaned accumulator 14
18/06/26 02:57:56 INFO spark.ContextCleaner: Cleaned accumulator 11
18/06/26 02:57:56 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 02:57:56 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 02:57:56 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 02:57:56 INFO spark.SparkContext: Starting job: run at ThreadPoolExecutor.java:1142
18/06/26 02:57:56 INFO spark.SparkContext: Starting job: run at ThreadPoolExecutor.java:1142
18/06/26 02:57:56 INFO spark.SparkContext: Starting job: run at ThreadPoolExecutor.java:1142
18/06/26 02:57:56 INFO scheduler.DAGScheduler: Got job 0 (run at ThreadPoolExecutor.java:1142) with 2 output partitions
18/06/26 02:57:56 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (run at ThreadPoolExecutor.java:1142)
18/06/26 02:57:56 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/06/26 02:57:56 INFO scheduler.DAGScheduler: Missing parents: List()
18/06/26 02:57:56 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[15] at run at ThreadPoolExecutor.java:1142), which has no missing parents
18/06/26 02:57:56 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.1 KB, free 408.0 MB)
18/06/26 02:57:56 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.2 KB, free 408.0 MB)
18/06/26 02:57:56 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on dc1master-lan1:36135 (size: 6.2 KB, free: 408.8 MB)
18/06/26 02:57:56 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1074
18/06/26 02:57:56 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[15] at run at ThreadPoolExecutor.java:1142) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 02:57:56 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
18/06/26 02:57:56 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:57:56 INFO scheduler.DAGScheduler: Got job 2 (run at ThreadPoolExecutor.java:1142) with 2 output partitions
18/06/26 02:57:56 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:56 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (run at ThreadPoolExecutor.java:1142)
18/06/26 02:57:56 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/06/26 02:57:56 INFO scheduler.DAGScheduler: Missing parents: List()
18/06/26 02:57:56 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[17] at run at ThreadPoolExecutor.java:1142), which has no missing parents
18/06/26 02:57:57 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.5 KB, free 407.9 MB)
18/06/26 02:57:57 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.8 KB, free 407.9 MB)
18/06/26 02:57:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on dc1master-lan1:36135 (size: 5.8 KB, free: 408.8 MB)
18/06/26 02:57:57 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1074
18/06/26 02:57:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[17] at run at ThreadPoolExecutor.java:1142) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
18/06/26 02:57:57 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.0.1.1, executor 1, partition 0, ANY, 7911 bytes)
18/06/26 02:57:57 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.0.1.2, executor 0, partition 1, ANY, 7911 bytes)
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 02:57:57 INFO scheduler.DAGScheduler: Got job 1 (run at ThreadPoolExecutor.java:1142) with 2 output partitions
18/06/26 02:57:57 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (run at ThreadPoolExecutor.java:1142)
18/06/26 02:57:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/06/26 02:57:57 INFO scheduler.DAGScheduler: Missing parents: List()
18/06/26 02:57:57 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[16] at run at ThreadPoolExecutor.java:1142), which has no missing parents
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:57 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.2 KB, free 407.9 MB)
18/06/26 02:57:57 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, 10.0.1.1, executor 1, partition 0, ANY, 7923 bytes)
18/06/26 02:57:57 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, 10.0.1.2, executor 0, partition 1, ANY, 7923 bytes)
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 02:57:57 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.0 KB, free 407.9 MB)
18/06/26 02:57:57 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on dc1master-lan1:36135 (size: 6.0 KB, free: 408.8 MB)
18/06/26 02:57:57 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1074
18/06/26 02:57:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[16] at run at ThreadPoolExecutor.java:1142) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:57 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, 10.0.1.2, executor 0, partition 0, ANY, 7915 bytes)
18/06/26 02:57:57 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, 10.0.1.1, executor 1, partition 1, ANY, 7915 bytes)
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 02:57:57 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 0
18/06/26 02:57:57 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 0
18/06/26 02:57:57 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 1
18/06/26 02:57:57 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 1
18/06/26 02:57:57 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 2
18/06/26 02:57:57 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 2
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:57:57 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:57 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.1:38322 (size: 6.2 KB, free: 912.3 MB)
18/06/26 02:57:57 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.1.2:37932 (size: 6.0 KB, free: 912.3 MB)
18/06/26 02:57:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.1.1:38322 (size: 5.8 KB, free: 912.3 MB)
18/06/26 02:57:57 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.1.1:38322 (size: 6.0 KB, free: 912.3 MB)
18/06/26 02:57:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.1.2:37932 (size: 5.8 KB, free: 912.3 MB)
18/06/26 02:57:57 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.1.2:37932 (size: 6.2 KB, free: 912.3 MB)
18/06/26 02:57:57 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 912.3 MB)
18/06/26 02:57:57 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.2:37932 (size: 24.9 KB, free: 912.3 MB)
18/06/26 02:57:57 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.1:38322 (size: 24.9 KB, free: 912.2 MB)
18/06/26 02:57:57 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 912.2 MB)
18/06/26 02:57:57 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 912.2 MB)
18/06/26 02:57:57 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 912.2 MB)
18/06/26 02:57:58 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:57:58 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:58 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:57:58 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2266 ms on 10.0.1.1 (executor 1) (1/2)
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 2249 ms on 10.0.1.1 (executor 1) (1/2)
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 2266 ms on 10.0.1.1 (executor 1) (1/2)
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2342 ms on 10.0.1.2 (executor 0) (2/2)
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/06/26 02:57:59 INFO scheduler.DAGScheduler: ResultStage 0 (run at ThreadPoolExecutor.java:1142) finished in 2.426 s
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 2326 ms on 10.0.1.2 (executor 0) (2/2)
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
18/06/26 02:57:59 INFO scheduler.DAGScheduler: ResultStage 2 (run at ThreadPoolExecutor.java:1142) finished in 2.342 s
18/06/26 02:57:59 INFO scheduler.DAGScheduler: Job 0 finished: run at ThreadPoolExecutor.java:1142, took 2.497943 s
18/06/26 02:57:59 INFO scheduler.DAGScheduler: Job 1 finished: run at ThreadPoolExecutor.java:1142, took 2.498124 s
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 2346 ms on 10.0.1.2 (executor 0) (2/2)
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/06/26 02:57:59 INFO scheduler.DAGScheduler: ResultStage 1 (run at ThreadPoolExecutor.java:1142) finished in 2.383 s
18/06/26 02:57:59 INFO scheduler.DAGScheduler: Job 2 finished: run at ThreadPoolExecutor.java:1142, took 2.504439 s
18/06/26 02:57:59 INFO codegen.CodeGenerator: Code generated in 14.249085 ms
18/06/26 02:57:59 INFO codegen.CodeGenerator: Code generated in 14.512086 ms
18/06/26 02:57:59 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 1024.3 KB, free 406.9 MB)
18/06/26 02:57:59 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 521.0 B, free 406.9 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on dc1master-lan1:36135 (size: 521.0 B, free: 408.8 MB)
18/06/26 02:57:59 INFO spark.SparkContext: Created broadcast 6 from run at ThreadPoolExecutor.java:1142
18/06/26 02:57:59 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 1152.0 KB, free 405.8 MB)
18/06/26 02:57:59 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 44.7 KB, free 405.8 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on dc1master-lan1:36135 (size: 44.7 KB, free: 408.8 MB)
18/06/26 02:57:59 INFO spark.SparkContext: Created broadcast 7 from run at ThreadPoolExecutor.java:1142
18/06/26 02:57:59 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 1586.5 KB, free 404.2 MB)
18/06/26 02:57:59 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 362.4 KB, free 403.8 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on dc1master-lan1:36135 (size: 362.4 KB, free: 408.4 MB)
18/06/26 02:57:59 INFO spark.SparkContext: Created broadcast 8 from run at ThreadPoolExecutor.java:1142
18/06/26 02:57:59 INFO codegen.CodeGenerator: Code generated in 32.166313 ms
18/06/26 02:57:59 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 290.9 KB, free 403.6 MB)
18/06/26 02:57:59 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 25.0 KB, free 403.5 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on dc1master-lan1:36135 (size: 25.0 KB, free: 408.4 MB)
18/06/26 02:57:59 INFO spark.SparkContext: Created broadcast 9 from 
18/06/26 02:57:59 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 02:57:59 INFO spark.SparkContext: Starting job: run at ThreadPoolExecutor.java:1142
18/06/26 02:57:59 INFO scheduler.DAGScheduler: Got job 3 (run at ThreadPoolExecutor.java:1142) with 2 output partitions
18/06/26 02:57:59 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (run at ThreadPoolExecutor.java:1142)
18/06/26 02:57:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/06/26 02:57:59 INFO scheduler.DAGScheduler: Missing parents: List()
18/06/26 02:57:59 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[23] at run at ThreadPoolExecutor.java:1142), which has no missing parents
18/06/26 02:57:59 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 13.7 KB, free 403.5 MB)
18/06/26 02:57:59 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.7 KB, free 403.5 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on dc1master-lan1:36135 (size: 6.7 KB, free: 408.4 MB)
18/06/26 02:57:59 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1074
18/06/26 02:57:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[23] at run at ThreadPoolExecutor.java:1142) (first 15 tasks are for partitions Vector(0, 1))
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6, 10.0.1.2, executor 0, partition 0, ANY, 7911 bytes)
18/06/26 02:57:59 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 3
18/06/26 02:57:59 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 7, 10.0.1.1, executor 1, partition 1, ANY, 7911 bytes)
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:57:59 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 3
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 2
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.2:37932 (size: 6.7 KB, free: 912.2 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 912.2 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.1.1:38322 (size: 6.7 KB, free: 912.2 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 912.2 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.1.2:37932 (size: 44.7 KB, free: 912.1 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.1.1:38322 (size: 44.7 KB, free: 912.1 MB)
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 250 ms on 10.0.1.2 (executor 0) (1/2)
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:57:59 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 7) in 274 ms on 10.0.1.1 (executor 1) (2/2)
18/06/26 02:57:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
18/06/26 02:57:59 INFO scheduler.DAGScheduler: ResultStage 3 (run at ThreadPoolExecutor.java:1142) finished in 0.283 s
18/06/26 02:57:59 INFO scheduler.DAGScheduler: Job 3 finished: run at ThreadPoolExecutor.java:1142, took 0.285989 s
18/06/26 02:57:59 INFO codegen.CodeGenerator: Code generated in 9.860833 ms
18/06/26 02:57:59 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 1227.1 KB, free 402.3 MB)
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 154
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 195
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 189
18/06/26 02:57:59 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 77.7 KB, free 402.2 MB)
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 168
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 169
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 140
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 130
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 167
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 128
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 176
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 160
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 126
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on dc1master-lan1:36135 (size: 77.7 KB, free: 408.3 MB)
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 141
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 165
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 171
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 170
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 172
18/06/26 02:57:59 INFO spark.SparkContext: Created broadcast 11 from run at ThreadPoolExecutor.java:1142
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on dc1master-lan1:36135 in memory (size: 5.8 KB, free: 408.3 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.1.1:38322 in memory (size: 5.8 KB, free: 912.1 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.1.2:37932 in memory (size: 5.8 KB, free: 912.1 MB)
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 159
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 190
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on dc1master-lan1:36135 in memory (size: 6.0 KB, free: 408.3 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.1.2:37932 in memory (size: 6.0 KB, free: 912.1 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.1.1:38322 in memory (size: 6.0 KB, free: 912.1 MB)
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 151
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 131
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 162
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 173
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 149
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 124
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 185
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 144
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 175
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 135
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 146
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 122
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 179
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on dc1master-lan1:36135 in memory (size: 6.7 KB, free: 408.3 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.1.2:37932 in memory (size: 6.7 KB, free: 912.2 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.1.1:38322 in memory (size: 6.7 KB, free: 912.2 MB)
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 178
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 148
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 184
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 143
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 181
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 192
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 183
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 147
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 187
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 177
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 129
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on dc1master-lan1:36135 in memory (size: 6.2 KB, free: 408.3 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.1.2:37932 in memory (size: 6.2 KB, free: 912.2 MB)
18/06/26 02:57:59 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.1.1:38322 in memory (size: 6.2 KB, free: 912.2 MB)
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 134
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 150
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 161
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 127
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 137
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 133
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 142
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 123
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 145
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 186
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 166
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 158
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 194
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 152
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 157
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 121
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 188
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 182
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 163
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 191
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 136
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 125
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 180
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 139
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 132
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 153
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 156
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 138
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 164
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 155
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 193
18/06/26 02:57:59 INFO spark.ContextCleaner: Cleaned accumulator 174
18/06/26 02:57:59 INFO codegen.CodeGenerator: Code generated in 62.192318 ms
18/06/26 02:58:00 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 291.1 KB, free 402.0 MB)
18/06/26 02:58:00 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 25.0 KB, free 402.0 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on dc1master-lan1:36135 (size: 25.0 KB, free: 408.3 MB)
18/06/26 02:58:00 INFO spark.SparkContext: Created broadcast 12 from 
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:00 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 02:58:00 INFO codegen.CodeGenerator: Code generated in 24.615982 ms
18/06/26 02:58:00 INFO codegen.CodeGenerator: Code generated in 44.538782 ms
18/06/26 02:58:00 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 291.7 KB, free 401.7 MB)
18/06/26 02:58:00 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 25.1 KB, free 401.7 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on dc1master-lan1:36135 (size: 25.1 KB, free: 408.3 MB)
18/06/26 02:58:00 INFO spark.SparkContext: Created broadcast 13 from 
18/06/26 02:58:00 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 02:58:00 INFO codegen.CodeGenerator: Code generated in 24.176104 ms
18/06/26 02:58:00 INFO codegen.CodeGenerator: Code generated in 49.587695 ms
18/06/26 02:58:00 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 291.6 KB, free 401.4 MB)
18/06/26 02:58:00 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 25.1 KB, free 401.4 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on dc1master-lan1:36135 (size: 25.1 KB, free: 408.3 MB)
18/06/26 02:58:00 INFO spark.SparkContext: Created broadcast 14 from 
18/06/26 02:58:00 INFO mapred.FileInputFormat: Total input paths to process : 1
18/06/26 02:58:00 INFO spark.SparkContext: Starting job: processCmd at CliDriver.java:376
18/06/26 02:58:00 INFO scheduler.DAGScheduler: Registering RDD 45 (processCmd at CliDriver.java:376)
18/06/26 02:58:00 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerShuffle id:2
18/06/26 02:58:00 INFO scheduler.DAGScheduler: Registering RDD 37 (processCmd at CliDriver.java:376)
18/06/26 02:58:00 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerShuffle id:1
18/06/26 02:58:00 INFO scheduler.DAGScheduler: Registering RDD 29 (processCmd at CliDriver.java:376)
18/06/26 02:58:00 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerShuffle id:0
18/06/26 02:58:00 INFO scheduler.DAGScheduler: Registering RDD 50 (processCmd at CliDriver.java:376)
18/06/26 02:58:00 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerShuffle id:3
18/06/26 02:58:00 INFO scheduler.DAGScheduler: Got job 4 (processCmd at CliDriver.java:376) with 4 output partitions
18/06/26 02:58:00 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (processCmd at CliDriver.java:376)
18/06/26 02:58:00 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
18/06/26 02:58:00 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 7)
18/06/26 02:58:00 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[45] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:00 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 64.3 KB, free 401.3 MB)
18/06/26 02:58:00 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 26.1 KB, free 401.3 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on dc1master-lan1:36135 (size: 26.1 KB, free: 408.2 MB)
18/06/26 02:58:00 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:00 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[45] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2))
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 3 tasks
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:00 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:00 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 8, 10.0.1.2, executor 0, partition 0, ANY, 7905 bytes)
18/06/26 02:58:00 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 9, 10.0.1.1, executor 1, partition 1, ANY, 7905 bytes)
18/06/26 02:58:00 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 10, 10.0.1.2, executor 0, partition 2, ANY, 7905 bytes)
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 3
18/06/26 02:58:00 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 64.3 KB, free 401.2 MB)
18/06/26 02:58:00 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 26.2 KB, free 401.2 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on dc1master-lan1:36135 (size: 26.2 KB, free: 408.2 MB)
18/06/26 02:58:00 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:00 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 5 tasks
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:00 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:00 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 11, 10.0.1.2, executor 0, partition 0, ANY, 7909 bytes)
18/06/26 02:58:00 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 12, 10.0.1.1, executor 1, partition 1, ANY, 7909 bytes)
18/06/26 02:58:00 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 13, 10.0.1.2, executor 0, partition 2, ANY, 7909 bytes)
18/06/26 02:58:00 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 5.0 (TID 14, 10.0.1.1, executor 1, partition 3, ANY, 7909 bytes)
18/06/26 02:58:00 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 5.0 (TID 15, 10.0.1.2, executor 0, partition 4, ANY, 7909 bytes)
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 5
18/06/26 02:58:00 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 61.7 KB, free 401.2 MB)
18/06/26 02:58:00 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 25.1 KB, free 401.1 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on dc1master-lan1:36135 (size: 25.1 KB, free: 408.2 MB)
18/06/26 02:58:00 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:00 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 6 tasks
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:00 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 02:58:00 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 02:58:00 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 02:58:00 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.1:38322 (size: 26.1 KB, free: 912.1 MB)
18/06/26 02:58:00 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:00 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:00 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:00 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:00 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 16, 10.0.1.2, executor 0, partition 0, ANY, 7907 bytes)
18/06/26 02:58:00 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:00 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 17, 10.0.1.1, executor 1, partition 1, ANY, 7907 bytes)
18/06/26 02:58:00 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:00 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 18, 10.0.1.2, executor 0, partition 2, ANY, 7907 bytes)
18/06/26 02:58:00 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:00 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 19, 10.0.1.1, executor 1, partition 3, ANY, 7907 bytes)
18/06/26 02:58:00 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:00 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 20, 10.0.1.2, executor 0, partition 4, ANY, 7907 bytes)
18/06/26 02:58:00 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:00 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 21, 10.0.1.1, executor 1, partition 5, ANY, 7907 bytes)
18/06/26 02:58:00 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:00 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 6
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.2:37932 (size: 26.1 KB, free: 912.1 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.2:37932 (size: 25.1 KB, free: 912.1 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 912.1 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 912.1 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.1:38322 (size: 25.1 KB, free: 912.1 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 912.1 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.1:38322 (size: 25.1 KB, free: 912.1 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 912.0 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.2:37932 (size: 25.1 KB, free: 912.0 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.2:37932 (size: 25.1 KB, free: 912.0 MB)
18/06/26 02:58:00 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.1:38322 (size: 25.1 KB, free: 912.0 MB)
18/06/26 02:58:01 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.1.1:38322 (size: 521.0 B, free: 912.0 MB)
18/06/26 02:58:01 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.1:38322 (size: 362.4 KB, free: 911.7 MB)
18/06/26 02:58:01 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.1:38322 (size: 77.7 KB, free: 911.6 MB)
18/06/26 02:58:01 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.1.2:37932 (size: 521.0 B, free: 912.0 MB)
18/06/26 02:58:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:01 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.2:37932 (size: 362.4 KB, free: 911.7 MB)
18/06/26 02:58:01 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.2:37932 (size: 77.7 KB, free: 911.6 MB)
18/06/26 02:58:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:01 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:01 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:01 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 10) in 1143 ms on 10.0.1.2 (executor 0) (1/3)
18/06/26 02:58:01 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:01 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/38/shuffle_2_2_0.data
18/06/26 02:58:01 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0c/shuffle_2_2_0.index
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 5.0 (TID 15) in 1504 ms on 10.0.1.2 (executor 0) (1/5)
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/19/shuffle_1_4_0.data
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/35/shuffle_1_4_0.index
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 5.0 (TID 14) in 2134 ms on 10.0.1.1 (executor 1) (2/5)
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/08/shuffle_1_3_0.data
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0c/shuffle_1_3_0.index
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 8) in 2162 ms on 10.0.1.2 (executor 0) (2/3)
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0a/shuffle_2_0_0.data
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 11) in 2213 ms on 10.0.1.2 (executor 0) (3/5)
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/2b/shuffle_1_0_0.data
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0f/shuffle_1_0_0.index
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 9) in 2240 ms on 10.0.1.1 (executor 1) (3/3)
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
18/06/26 02:58:02 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 12) in 2228 ms on 10.0.1.1 (executor 1) (4/5)
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/29/shuffle_2_1_0.data
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0d/shuffle_2_1_0.index
18/06/26 02:58:02 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (processCmd at CliDriver.java:376) finished in 2.260 s
18/06/26 02:58:02 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 02:58:02 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 5, ShuffleMapStage 6)
18/06/26 02:58:02 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
18/06/26 02:58:02 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0a/shuffle_1_1_0.data
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_1_1_0.index
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 13) in 2242 ms on 10.0.1.2 (executor 0) (5/5)
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/17/shuffle_1_2_0.data
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0d/shuffle_1_2_0.index
18/06/26 02:58:02 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:376) finished in 2.256 s
18/06/26 02:58:02 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 02:58:02 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 6)
18/06/26 02:58:02 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
18/06/26 02:58:02 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:02 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:02 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 6.0 (TID 21) in 2438 ms on 10.0.1.1 (executor 1) (1/6)
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/27/shuffle_0_5_0.data
18/06/26 02:58:02 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/35/shuffle_0_5_0.index
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 17) in 2775 ms on 10.0.1.1 (executor 1) (2/6)
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/15/shuffle_0_1_0.data
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0f/shuffle_0_1_0.index
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 6.0 (TID 19) in 2782 ms on 10.0.1.1 (executor 1) (3/6)
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/29/shuffle_0_3_0.data
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0d/shuffle_0_3_0.index
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 6.0 (TID 20) in 2834 ms on 10.0.1.2 (executor 0) (4/6)
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/38/shuffle_0_4_0.data
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0c/shuffle_0_4_0.index
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 6.0 (TID 18) in 2896 ms on 10.0.1.2 (executor 0) (5/6)
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/36/shuffle_0_2_0.data
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_0_2_0.index
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 16) in 2917 ms on 10.0.1.2 (executor 0) (6/6)
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0c/shuffle_0_0_0.data
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/30/shuffle_0_0_0.index
18/06/26 02:58:03 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:376) finished in 2.928 s
18/06/26 02:58:03 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 02:58:03 INFO scheduler.DAGScheduler: running: Set()
18/06/26 02:58:03 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
18/06/26 02:58:03 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 02:58:03 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[50] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3788
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3788
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4363
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4363
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4155
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4155
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3667
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3667
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3118
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3118
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3377
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3377
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4308
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4308
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4185
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4185
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3613
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3613
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3101
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3101
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4156
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4156
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4236
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4236
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4239
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4239
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3967
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3967
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3592
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3592
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3724
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3724
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3919
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3919
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3962
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3962
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3586
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3586
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3273
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3273
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 6781
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 6781
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 6857
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 6857
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 7283
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 7283
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 6424
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 6424
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2959
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2959
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 723
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 723
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3009
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3009
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 763
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 763
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3220
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3220
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 934
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 934
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2879
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2879
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 837
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 837
18/06/26 02:58:03 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 97.0 KB, free 401.0 MB)
18/06/26 02:58:03 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 38.5 KB, free 401.0 MB)
18/06/26 02:58:03 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on dc1master-lan1:36135 (size: 38.5 KB, free: 408.1 MB)
18/06/26 02:58:03 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:03 INFO scheduler.DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[50] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 12 tasks
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 22, 10.0.1.1, executor 1, partition 0, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 23, 10.0.1.2, executor 0, partition 1, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 7.0 (TID 24, 10.0.1.1, executor 1, partition 2, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 7.0 (TID 25, 10.0.1.2, executor 0, partition 3, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 7.0 (TID 26, 10.0.1.1, executor 1, partition 4, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 7.0 (TID 27, 10.0.1.2, executor 0, partition 8, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 7.0 (TID 28, 10.0.1.1, executor 1, partition 5, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 7.0 (TID 29, 10.0.1.2, executor 0, partition 9, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 7.0 (TID 30, 10.0.1.1, executor 1, partition 6, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 7.0 (TID 31, 10.0.1.2, executor 0, partition 10, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 7.0 (TID 32, 10.0.1.1, executor 1, partition 7, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 7.0 (TID 33, 10.0.1.2, executor 0, partition 11, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle not finished !!!
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:03 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.1:38322 (size: 38.5 KB, free: 911.5 MB)
18/06/26 02:58:03 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.2:37932 (size: 38.5 KB, free: 911.5 MB)
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:03 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:03 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.1:58408
18/06/26 02:58:03 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:38110
18/06/26 02:58:03 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 2 to 10.0.1.2:38110
18/06/26 02:58:03 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58408
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58408
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 2 to 10.0.1.2:38110
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:38110
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.1:58408
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 02:58:03 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:04 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.1.2:38110
18/06/26 02:58:04 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.1:58408
18/06/26 02:58:04 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.2:38110
18/06/26 02:58:04 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.1:58408
18/06/26 02:58:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3107
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3729
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 2947
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3032
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 2758
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3635
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3813
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3554
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 2959
18/06/26 02:58:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3009
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3220
18/06/26 02:58:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3788
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 6781
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 6857
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 7283
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 6424
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 2879
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3377
18/06/26 02:58:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 723
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 763
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 934
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 837
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 4156
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3724
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 4363
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 4308
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 4236
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3919
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 4155
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 4185
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 4239
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3962
18/06/26 02:58:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3667
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3613
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3967
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3586
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3118
18/06/26 02:58:04 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3101
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3592
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 3273
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:04 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 7.0 (TID 24, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=0, mapId=0, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Task 2.0 in stage 7.0 (TID 24) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:04 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 7.0 (TID 22, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=0, mapId=0, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Task 0.0 in stage 7.0 (TID 22) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 7 (processCmd at CliDriver.java:376) as failed due to a fetch failure from ShuffleMapStage 6 (processCmd at CliDriver.java:376)
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (processCmd at CliDriver.java:376) failed in 1.255 s due to org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

18/06/26 02:58:04 WARN scheduler.TaskSetManager: Lost task 11.0 in stage 7.0 (TID 33, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=2, mapId=1, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0d/shuffle_2_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0d/shuffle_2_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Task 11.0 in stage 7.0 (TID 33) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 6 (processCmd at CliDriver.java:376) and ShuffleMapStage 7 (processCmd at CliDriver.java:376) due to fetch failure
18/06/26 02:58:04 WARN scheduler.TaskSetManager: Lost task 9.0 in stage 7.0 (TID 29, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=2, mapId=1, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0d/shuffle_2_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0d/shuffle_2_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Task 9.0 in stage 7.0 (TID 29) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:04 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 7.0 (TID 27, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=2, mapId=1, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0d/shuffle_2_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0d/shuffle_2_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Task 8.0 in stage 7.0 (TID 27) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:04 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 7.0 (TID 25, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=0, mapId=1, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Task 3.0 in stage 7.0 (TID 25) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 3)
18/06/26 02:58:04 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
18/06/26 02:58:04 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 10.0.1.1, 38322, None)
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:04 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:04 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 7.0 (TID 23, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=0, mapId=1, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Task 1.0 in stage 7.0 (TID 23) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 1 (epoch 3)
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 7.0 (TID 26) in 1251 ms on 10.0.1.1 (executor 1) (8/12)
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:04 WARN scheduler.TaskSetManager: Lost task 10.0 in stage 7.0 (TID 31, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=2, mapId=1, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0d/shuffle_2_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0d/shuffle_2_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Task 10.0 in stage 7.0 (TID 31) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 7.0 (TID 32) in 1252 ms on 10.0.1.1 (executor 1) (10/12)
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 7.0 (TID 28) in 1254 ms on 10.0.1.1 (executor 1) (11/12)
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 7.0 (TID 30) in 1254 ms on 10.0.1.1 (executor 1) (12/12)
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Executor lost: 0 (epoch 3)
18/06/26 02:58:04 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
18/06/26 02:58:04 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.0.1.2, 37932, None)
18/06/26 02:58:04 INFO storage.BlockManagerMaster: Removed 0 successfully in removeExecutor
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 0 (epoch 3)
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Ignoring possibly bogus ShuffleMapTask(7, 4) completion from executor 1
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Ignoring possibly bogus ShuffleMapTask(7, 7) completion from executor 1
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Ignoring possibly bogus ShuffleMapTask(7, 5) completion from executor 1
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Ignoring possibly bogus ShuffleMapTask(7, 6) completion from executor 1
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Resubmitting failed stages
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:04 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 61.7 KB, free 400.9 MB)
18/06/26 02:58:04 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 25.2 KB, free 400.9 MB)
18/06/26 02:58:04 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on dc1master-lan1:36135 (size: 25.2 KB, free: 408.1 MB)
18/06/26 02:58:04 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: Adding task set 6.1 with 6 tasks
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.1 (TID 34, 10.0.1.1, executor 1, partition 0, ANY, 7907 bytes)
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.1 (TID 35, 10.0.1.2, executor 0, partition 1, ANY, 7907 bytes)
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.1 (TID 36, 10.0.1.1, executor 1, partition 2, ANY, 7907 bytes)
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[45] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.1 (TID 37, 10.0.1.2, executor 0, partition 3, ANY, 7907 bytes)
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.1 (TID 38, 10.0.1.1, executor 1, partition 4, ANY, 7907 bytes)
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.1 (TID 39, 10.0.1.2, executor 0, partition 5, ANY, 7907 bytes)
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 6
18/06/26 02:58:04 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 64.3 KB, free 400.9 MB)
18/06/26 02:58:04 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 26.2 KB, free 400.8 MB)
18/06/26 02:58:04 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on dc1master-lan1:36135 (size: 26.2 KB, free: 408.1 MB)
18/06/26 02:58:04 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[45] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2))
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: Adding task set 4.1 with 3 tasks
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.1 (TID 40, 10.0.1.2, executor 0, partition 0, ANY, 7905 bytes)
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.1 (TID 41, 10.0.1.1, executor 1, partition 1, ANY, 7905 bytes)
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 4.1 (TID 42, 10.0.1.2, executor 0, partition 2, ANY, 7905 bytes)
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 3
18/06/26 02:58:04 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 64.3 KB, free 400.8 MB)
18/06/26 02:58:04 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 26.2 KB, free 400.7 MB)
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 281
18/06/26 02:58:04 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on dc1master-lan1:36135 (size: 26.2 KB, free: 408.1 MB)
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 263
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 200
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 198
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 196
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 245
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 294
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 241
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 274
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 221
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 239
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 285
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 286
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 229
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 267
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 255
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 208
18/06/26 02:58:04 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 244
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 271
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 209
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 220
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 217
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 232
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 243
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 231
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 235
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 204
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 249
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 205
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 258
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 288
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 237
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 292
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 199
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 273
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 268
18/06/26 02:58:04 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 252
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: Adding task set 5.1 with 5 tasks
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 257
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 282
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 207
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 210
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 269
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 247
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 236
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 256
18/06/26 02:58:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 225
18/06/26 02:58:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 227
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 262
18/06/26 02:58:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 201
18/06/26 02:58:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 234
18/06/26 02:58:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 270
18/06/26 02:58:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.1 (TID 43, 10.0.1.1, executor 1, partition 0, ANY, 7909 bytes)
18/06/26 02:58:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.1 (TID 44, 10.0.1.2, executor 0, partition 1, ANY, 7909 bytes)
18/06/26 02:58:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.1 (TID 45, 10.0.1.1, executor 1, partition 2, ANY, 7909 bytes)
18/06/26 02:58:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 5.1 (TID 46, 10.0.1.2, executor 0, partition 3, ANY, 7909 bytes)
18/06/26 02:58:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:04 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 5.1 (TID 47, 10.0.1.1, executor 1, partition 4, ANY, 7909 bytes)
18/06/26 02:58:04 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on dc1master-lan1:36135 in memory (size: 26.1 KB, free: 408.1 MB)
18/06/26 02:58:04 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:04 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 5
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 248
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 280
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 276
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 260
18/06/26 02:58:04 INFO spark.ContextCleaner: Cleaned accumulator 238
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 259
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 265
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 222
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 214
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 197
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 284
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on dc1master-lan1:36135 in memory (size: 38.5 KB, free: 408.1 MB)
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 213
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 287
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 226
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 275
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 264
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 230
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 216
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 290
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 279
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 215
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 224
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 206
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 278
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 211
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 212
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 242
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 251
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 203
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 289
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 233
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 266
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 240
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 246
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 223
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 295
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 218
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 291
18/06/26 02:58:05 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.2:37932 with 912.3 MB RAM, BlockManagerId(0, 10.0.1.2, 37932, None)
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 277
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 293
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 272
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 283
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 254
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 228
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 253
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 261
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 202
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 219
18/06/26 02:58:05 INFO spark.ContextCleaner: Cleaned accumulator 250
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.2:37932 (size: 38.5 KB, free: 912.3 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 912.2 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.2:37932 (size: 25.1 KB, free: 912.2 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 912.2 MB)
18/06/26 02:58:05 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.1:38322 with 912.3 MB RAM, BlockManagerId(1, 10.0.1.1, 38322, None)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 912.2 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.1:38322 (size: 38.5 KB, free: 912.3 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 912.1 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.1:38322 (size: 25.1 KB, free: 912.2 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.2:37932 (size: 24.9 KB, free: 912.1 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 912.1 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.2:37932 (size: 25.1 KB, free: 912.1 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 912.2 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 912.2 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.1.2:37932 (size: 521.0 B, free: 912.1 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 912.2 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.1.2:37932 (size: 44.7 KB, free: 912.0 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 912.1 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.1:38322 (size: 24.9 KB, free: 912.1 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 912.1 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.1:38322 (size: 25.1 KB, free: 912.1 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.1.1:38322 (size: 521.0 B, free: 912.1 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.1.1:38322 (size: 44.7 KB, free: 912.0 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.1:38322 (size: 25.2 KB, free: 912.0 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.2:37932 (size: 25.2 KB, free: 912.0 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 912.0 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 912.0 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.1:38322 (size: 362.4 KB, free: 911.6 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.1:38322 (size: 77.7 KB, free: 911.5 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 911.5 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.1:38322 (size: 26.1 KB, free: 911.5 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.2:37932 (size: 362.4 KB, free: 911.6 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.1:38322 (size: 25.1 KB, free: 911.5 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.2:37932 (size: 77.7 KB, free: 911.5 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 911.5 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.2:37932 (size: 26.1 KB, free: 911.5 MB)
18/06/26 02:58:05 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.2:37932 (size: 25.1 KB, free: 911.5 MB)
18/06/26 02:58:05 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:05 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:05 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:05 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:05 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:05 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:05 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:05 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:05 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 4.1 (TID 42) in 471 ms on 10.0.1.2 (executor 0) (1/3)
18/06/26 02:58:05 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:05 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/38/shuffle_2_2_0.data
18/06/26 02:58:05 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0c/shuffle_2_2_0.index
18/06/26 02:58:05 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:05 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:05 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:05 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:05 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 5.1 (TID 47) in 880 ms on 10.0.1.1 (executor 1) (1/5)
18/06/26 02:58:05 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:05 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/19/shuffle_1_4_0.data
18/06/26 02:58:05 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/35/shuffle_1_4_0.index
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 5.1 (TID 46) in 1539 ms on 10.0.1.2 (executor 0) (2/5)
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/08/shuffle_1_3_0.data
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0c/shuffle_1_3_0.index
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.1 (TID 40) in 1615 ms on 10.0.1.2 (executor 0) (2/3)
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0a/shuffle_2_0_0.data
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.1 (TID 44) in 1589 ms on 10.0.1.2 (executor 0) (3/5)
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0a/shuffle_1_1_0.data
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_1_1_0.index
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.1 (TID 45) in 1698 ms on 10.0.1.1 (executor 1) (4/5)
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/17/shuffle_1_2_0.data
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0d/shuffle_1_2_0.index
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.1 (TID 41) in 1731 ms on 10.0.1.1 (executor 1) (3/3)
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.1, whose tasks have all completed, from pool 
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/29/shuffle_2_1_0.data
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0d/shuffle_2_1_0.index
18/06/26 02:58:06 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (processCmd at CliDriver.java:376) finished in 1.742 s
18/06/26 02:58:06 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 02:58:06 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 5, ShuffleMapStage 6)
18/06/26 02:58:06 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
18/06/26 02:58:06 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.1 (TID 43) in 1730 ms on 10.0.1.1 (executor 1) (5/5)
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.1, whose tasks have all completed, from pool 
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/2b/shuffle_1_0_0.data
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0f/shuffle_1_0_0.index
18/06/26 02:58:06 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:376) finished in 1.759 s
18/06/26 02:58:06 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 02:58:06 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 6)
18/06/26 02:58:06 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
18/06/26 02:58:06 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:06 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:06 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 6.1 (TID 39) in 1888 ms on 10.0.1.2 (executor 0) (1/6)
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/27/shuffle_0_5_0.data
18/06/26 02:58:06 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/35/shuffle_0_5_0.index
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 6.1 (TID 37) in 2256 ms on 10.0.1.2 (executor 0) (2/6)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/29/shuffle_0_3_0.data
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0d/shuffle_0_3_0.index
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.1 (TID 35) in 2279 ms on 10.0.1.2 (executor 0) (3/6)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/15/shuffle_0_1_0.data
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0f/shuffle_0_1_0.index
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 6.1 (TID 36) in 2324 ms on 10.0.1.1 (executor 1) (4/6)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/36/shuffle_0_2_0.data
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_0_2_0.index
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 6.1 (TID 38) in 2345 ms on 10.0.1.1 (executor 1) (5/6)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/38/shuffle_0_4_0.data
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0c/shuffle_0_4_0.index
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.1 (TID 34) in 2354 ms on 10.0.1.1 (executor 1) (6/6)
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.1, whose tasks have all completed, from pool 
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0c/shuffle_0_0_0.data
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/30/shuffle_0_0_0.index
18/06/26 02:58:07 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:376) finished in 2.364 s
18/06/26 02:58:07 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 02:58:07 INFO scheduler.DAGScheduler: running: Set()
18/06/26 02:58:07 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
18/06/26 02:58:07 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 02:58:07 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[50] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3788
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3788
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4363
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4363
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4155
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4155
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3667
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3667
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3118
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3118
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3377
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3377
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4308
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4308
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4185
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4185
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3613
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3613
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3101
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3101
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4156
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4156
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4236
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4236
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4239
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4239
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3967
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3967
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3592
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3592
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3724
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3724
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3919
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3919
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3962
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3962
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3586
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3586
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3273
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3273
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 6781
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 6781
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 6857
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 6857
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 7283
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 7283
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 6424
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 6424
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2959
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2959
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 723
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 723
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3009
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3009
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 763
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 763
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3220
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3220
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 934
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 934
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2879
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2879
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 837
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 837
18/06/26 02:58:07 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 97.0 KB, free 400.9 MB)
18/06/26 02:58:07 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 38.6 KB, free 400.8 MB)
18/06/26 02:58:07 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on dc1master-lan1:36135 (size: 38.6 KB, free: 408.1 MB)
18/06/26 02:58:07 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:07 INFO scheduler.DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[50] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: Adding task set 7.1 with 12 tasks
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.1 (TID 48, 10.0.1.2, executor 0, partition 0, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.1 (TID 49, 10.0.1.1, executor 1, partition 1, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 7.1 (TID 50, 10.0.1.2, executor 0, partition 2, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 7.1 (TID 51, 10.0.1.1, executor 1, partition 3, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 7.1 (TID 52, 10.0.1.2, executor 0, partition 4, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 7.1 (TID 53, 10.0.1.1, executor 1, partition 8, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 7.1 (TID 54, 10.0.1.2, executor 0, partition 5, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 7.1 (TID 55, 10.0.1.1, executor 1, partition 9, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 7.1 (TID 56, 10.0.1.2, executor 0, partition 6, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 7.1 (TID 57, 10.0.1.1, executor 1, partition 10, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 7.1 (TID 58, 10.0.1.2, executor 0, partition 7, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:07 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 7.1 (TID 59, 10.0.1.1, executor 1, partition 11, NODE_LOCAL, 7856 bytes)
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:07 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:07 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.2:37932 (size: 38.6 KB, free: 911.4 MB)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.1:38322 (size: 38.6 KB, free: 911.4 MB)
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.2:38110
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.2:38110
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 2 to 10.0.1.1:58408
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58408
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 2 to 10.0.1.1:58408
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58408
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:38110
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:38110
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 02:58:07 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:07 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:07 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:08 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.1.1:58408
18/06/26 02:58:08 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3107
18/06/26 02:58:08 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.1:58408
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 2947
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3032
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 2758
18/06/26 02:58:08 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 2959
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3009
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3220
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 2879
18/06/26 02:58:08 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 723
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 763
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 934
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 837
18/06/26 02:58:08 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3729
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3635
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3813
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3554
18/06/26 02:58:08 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3788
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3377
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 4156
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3724
18/06/26 02:58:08 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 4363
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 4308
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 4236
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3919
18/06/26 02:58:08 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 4155
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 4185
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 4239
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3962
18/06/26 02:58:08 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3667
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3613
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3967
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3586
18/06/26 02:58:08 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3118
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3101
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3592
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 3273
18/06/26 02:58:08 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.2:38110
18/06/26 02:58:08 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 6781
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 6857
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 7283
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 6424
18/06/26 02:58:08 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:08 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.2:38110
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:08 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 7.1 (TID 51, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=0, mapId=1, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Task 3.0 in stage 7.1 (TID 51) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 7 (processCmd at CliDriver.java:376) as failed due to a fetch failure from ShuffleMapStage 6 (processCmd at CliDriver.java:376)
18/06/26 02:58:08 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (processCmd at CliDriver.java:376) failed in 1.085 s due to org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

18/06/26 02:58:08 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 6 (processCmd at CliDriver.java:376) and ShuffleMapStage 7 (processCmd at CliDriver.java:376) due to fetch failure
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 16)
18/06/26 02:58:08 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
18/06/26 02:58:08 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 10.0.1.1, 38322, None)
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:08 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 1 (epoch 16)
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:08 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 7.1 (TID 49, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=0, mapId=1, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Task 1.0 in stage 7.1 (TID 49) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:08 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 7.1 (TID 50, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=0, mapId=0, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Task 2.0 in stage 7.1 (TID 50) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Executor lost: 0 (epoch 16)
18/06/26 02:58:08 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
18/06/26 02:58:08 WARN storage.BlockManagerMasterEndpoint: No more replicas available for broadcast_18_piece0 !
18/06/26 02:58:08 WARN storage.BlockManagerMasterEndpoint: No more replicas available for broadcast_15_piece0 !
18/06/26 02:58:08 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.0.1.2, 37932, None)
18/06/26 02:58:08 INFO storage.BlockManagerMaster: Removed 0 successfully in removeExecutor
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 0 (epoch 16)
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:08 WARN scheduler.TaskSetManager: Lost task 9.0 in stage 7.1 (TID 55, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=2, mapId=0, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Task 9.0 in stage 7.1 (TID 55) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:08 WARN scheduler.TaskSetManager: Lost task 10.0 in stage 7.1 (TID 57, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=2, mapId=0, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Task 10.0 in stage 7.1 (TID 57) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:08 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 7.1 (TID 53, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=2, mapId=0, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Task 8.0 in stage 7.1 (TID 53) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:08 WARN scheduler.TaskSetManager: Lost task 11.0 in stage 7.1 (TID 59, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=2, mapId=0, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Task 11.0 in stage 7.1 (TID 59) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:08 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 7.1 (TID 48, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=0, mapId=0, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Task 0.0 in stage 7.1 (TID 48) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 7.1 (TID 52) in 1110 ms on 10.0.1.2 (executor 0) (9/12)
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Ignoring possibly bogus ShuffleMapTask(7, 4) completion from executor 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 7.1 (TID 54) in 1116 ms on 10.0.1.2 (executor 0) (10/12)
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Ignoring possibly bogus ShuffleMapTask(7, 5) completion from executor 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 7.1 (TID 58) in 1117 ms on 10.0.1.2 (executor 0) (11/12)
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Ignoring possibly bogus ShuffleMapTask(7, 7) completion from executor 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 7.1 (TID 56) in 1133 ms on 10.0.1.2 (executor 0) (12/12)
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.1, whose tasks have all completed, from pool 
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Ignoring possibly bogus ShuffleMapTask(7, 6) completion from executor 0
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Resubmitting failed stages
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:08 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 61.7 KB, free 400.8 MB)
18/06/26 02:58:08 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 25.2 KB, free 400.7 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on dc1master-lan1:36135 (size: 25.2 KB, free: 408.1 MB)
18/06/26 02:58:08 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: Adding task set 6.2 with 6 tasks
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[45] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.2 (TID 60, 10.0.1.2, executor 0, partition 0, ANY, 7907 bytes)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.2 (TID 61, 10.0.1.1, executor 1, partition 1, ANY, 7907 bytes)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.2 (TID 62, 10.0.1.2, executor 0, partition 2, ANY, 7907 bytes)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.2 (TID 63, 10.0.1.1, executor 1, partition 3, ANY, 7907 bytes)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.2 (TID 64, 10.0.1.2, executor 0, partition 4, ANY, 7907 bytes)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.2 (TID 65, 10.0.1.1, executor 1, partition 5, ANY, 7907 bytes)
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 6
18/06/26 02:58:08 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 64.3 KB, free 400.7 MB)
18/06/26 02:58:08 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 26.2 KB, free 400.7 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on dc1master-lan1:36135 (size: 26.2 KB, free: 408.0 MB)
18/06/26 02:58:08 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[45] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2))
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: Adding task set 4.2 with 3 tasks
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.2 (TID 66, 10.0.1.2, executor 0, partition 0, ANY, 7905 bytes)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.2 (TID 67, 10.0.1.1, executor 1, partition 1, ANY, 7905 bytes)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 4.2 (TID 68, 10.0.1.2, executor 0, partition 2, ANY, 7905 bytes)
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 3
18/06/26 02:58:08 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 64.3 KB, free 400.6 MB)
18/06/26 02:58:08 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 26.2 KB, free 400.6 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on dc1master-lan1:36135 (size: 26.2 KB, free: 408.0 MB)
18/06/26 02:58:08 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.1:38322 with 912.3 MB RAM, BlockManagerId(1, 10.0.1.1, 38322, None)
18/06/26 02:58:08 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:08 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: Adding task set 5.2 with 5 tasks
18/06/26 02:58:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.2 (TID 69, 10.0.1.1, executor 1, partition 0, ANY, 7909 bytes)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.1:38322 (size: 38.5 KB, free: 912.3 MB)
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.2 (TID 70, 10.0.1.2, executor 0, partition 1, ANY, 7909 bytes)
18/06/26 02:58:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.2 (TID 71, 10.0.1.1, executor 1, partition 2, ANY, 7909 bytes)
18/06/26 02:58:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 02:58:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 02:58:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 02:58:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 5.2 (TID 72, 10.0.1.2, executor 0, partition 3, ANY, 7909 bytes)
18/06/26 02:58:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:08 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 5.2 (TID 73, 10.0.1.1, executor 1, partition 4, ANY, 7909 bytes)
18/06/26 02:58:08 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:08 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 5
18/06/26 02:58:08 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.2:37932 with 912.3 MB RAM, BlockManagerId(0, 10.0.1.2, 37932, None)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.1:38322 (size: 25.1 KB, free: 912.2 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 912.3 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.2:37932 (size: 38.5 KB, free: 912.2 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 912.2 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 912.2 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.2:37932 (size: 25.1 KB, free: 912.2 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 912.2 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 912.2 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 912.1 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 912.2 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 912.1 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 912.1 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 912.1 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 912.1 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 912.1 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.1:38322 (size: 38.6 KB, free: 912.1 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.1:38322 (size: 24.9 KB, free: 912.0 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 912.0 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.1:38322 (size: 25.1 KB, free: 912.0 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.1.1:38322 (size: 521.0 B, free: 912.0 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.1.1:38322 (size: 44.7 KB, free: 911.9 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.1:38322 (size: 25.2 KB, free: 911.9 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 912.1 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 911.9 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Updated broadcast_24_piece0 in memory on 10.0.1.2:37932 (current size: 26.2 KB, original size: 26.2 KB, free: 912.1 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.1:38322 (size: 362.4 KB, free: 911.5 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.2:37932 (size: 38.6 KB, free: 912.0 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.2:37932 (size: 24.9 KB, free: 912.0 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.2:37932 (size: 25.1 KB, free: 912.0 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.1.2:37932 (size: 521.0 B, free: 912.0 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.1.2:37932 (size: 44.7 KB, free: 911.9 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.1:38322 (size: 77.7 KB, free: 911.5 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 911.4 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.1:38322 (size: 26.1 KB, free: 911.4 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.1:38322 (size: 25.1 KB, free: 911.4 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.1.1:38322 (size: 25.2 KB, free: 911.4 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.2:37932 (size: 25.2 KB, free: 911.9 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 911.9 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.2:37932 (size: 362.4 KB, free: 911.5 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.2:37932 (size: 77.7 KB, free: 911.5 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 911.4 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.2:37932 (size: 26.1 KB, free: 911.4 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.2:37932 (size: 25.1 KB, free: 911.4 MB)
18/06/26 02:58:08 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.1.2:37932 (size: 25.2 KB, free: 911.4 MB)
18/06/26 02:58:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:09 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 4.2 (TID 68) in 493 ms on 10.0.1.2 (executor 0) (1/3)
18/06/26 02:58:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/38/shuffle_2_2_0.data
18/06/26 02:58:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0c/shuffle_2_2_0.index
18/06/26 02:58:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:09 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:09 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:09 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 5.2 (TID 73) in 736 ms on 10.0.1.1 (executor 1) (1/5)
18/06/26 02:58:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/19/shuffle_1_4_0.data
18/06/26 02:58:09 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/35/shuffle_1_4_0.index
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.2 (TID 71) in 1506 ms on 10.0.1.1 (executor 1) (2/5)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/17/shuffle_1_2_0.data
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0d/shuffle_1_2_0.index
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.2 (TID 69) in 1514 ms on 10.0.1.1 (executor 1) (3/5)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/2b/shuffle_1_0_0.data
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0f/shuffle_1_0_0.index
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 5.2 (TID 72) in 1520 ms on 10.0.1.2 (executor 0) (4/5)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/08/shuffle_1_3_0.data
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0c/shuffle_1_3_0.index
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.2 (TID 66) in 1546 ms on 10.0.1.2 (executor 0) (2/3)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0a/shuffle_2_0_0.data
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.2 (TID 67) in 1548 ms on 10.0.1.1 (executor 1) (3/3)
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.2, whose tasks have all completed, from pool 
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/29/shuffle_2_1_0.data
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0d/shuffle_2_1_0.index
18/06/26 02:58:10 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (processCmd at CliDriver.java:376) finished in 1.557 s
18/06/26 02:58:10 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 02:58:10 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 5, ShuffleMapStage 6)
18/06/26 02:58:10 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
18/06/26 02:58:10 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.2 (TID 70) in 1546 ms on 10.0.1.2 (executor 0) (5/5)
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.2, whose tasks have all completed, from pool 
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0a/shuffle_1_1_0.data
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_1_1_0.index
18/06/26 02:58:10 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:376) finished in 1.555 s
18/06/26 02:58:10 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 02:58:10 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 6)
18/06/26 02:58:10 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
18/06/26 02:58:10 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 6.2 (TID 65) in 1886 ms on 10.0.1.1 (executor 1) (1/6)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/27/shuffle_0_5_0.data
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/35/shuffle_0_5_0.index
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 6.2 (TID 63) in 2132 ms on 10.0.1.1 (executor 1) (2/6)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/29/shuffle_0_3_0.data
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0d/shuffle_0_3_0.index
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.2 (TID 61) in 2142 ms on 10.0.1.1 (executor 1) (3/6)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/15/shuffle_0_1_0.data
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0f/shuffle_0_1_0.index
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 6.2 (TID 64) in 2149 ms on 10.0.1.2 (executor 0) (4/6)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/38/shuffle_0_4_0.data
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0c/shuffle_0_4_0.index
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 6.2 (TID 62) in 2155 ms on 10.0.1.2 (executor 0) (5/6)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/36/shuffle_0_2_0.data
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_0_2_0.index
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.2 (TID 60) in 2209 ms on 10.0.1.2 (executor 0) (6/6)
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.2, whose tasks have all completed, from pool 
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0c/shuffle_0_0_0.data
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/30/shuffle_0_0_0.index
18/06/26 02:58:10 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:376) finished in 2.218 s
18/06/26 02:58:10 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 02:58:10 INFO scheduler.DAGScheduler: running: Set()
18/06/26 02:58:10 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
18/06/26 02:58:10 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 02:58:10 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[50] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3788
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3788
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4363
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4363
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4155
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4155
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3667
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3667
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3118
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3118
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3377
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3377
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4308
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4308
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4185
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4185
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3613
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3613
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3101
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3101
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4156
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4156
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4236
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4236
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4239
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4239
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3967
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3967
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3592
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3592
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3724
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3724
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3919
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3919
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3962
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3962
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3586
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3586
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3273
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3273
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 6781
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 6781
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 6857
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 6857
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 7283
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 7283
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 6424
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 6424
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2959
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2959
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 723
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 723
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3009
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3009
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 763
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 763
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3220
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3220
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 934
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 934
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2879
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2879
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 837
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 837
18/06/26 02:58:10 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 97.0 KB, free 400.5 MB)
18/06/26 02:58:10 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 38.6 KB, free 400.4 MB)
18/06/26 02:58:10 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on dc1master-lan1:36135 (size: 38.6 KB, free: 408.0 MB)
18/06/26 02:58:10 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:10 INFO scheduler.DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[50] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: Adding task set 7.2 with 12 tasks
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.2 (TID 74, 10.0.1.1, executor 1, partition 0, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.2 (TID 75, 10.0.1.2, executor 0, partition 1, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 7.2 (TID 76, 10.0.1.1, executor 1, partition 2, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 7.2 (TID 77, 10.0.1.2, executor 0, partition 3, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 7.2 (TID 78, 10.0.1.1, executor 1, partition 8, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 7.2 (TID 79, 10.0.1.2, executor 0, partition 4, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 7.2 (TID 80, 10.0.1.1, executor 1, partition 9, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 7.2 (TID 81, 10.0.1.2, executor 0, partition 5, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 7.2 (TID 82, 10.0.1.1, executor 1, partition 10, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 7.2 (TID 83, 10.0.1.2, executor 0, partition 6, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 7.2 (TID 84, 10.0.1.1, executor 1, partition 11, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 7.2 (TID 85, 10.0.1.2, executor 0, partition 7, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 12
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:10 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.1.1:38322 (size: 38.6 KB, free: 911.3 MB)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.1.2:37932 (size: 38.6 KB, free: 911.3 MB)
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58408
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58408
18/06/26 02:58:10 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 2 to 10.0.1.1:58408
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 2 to 10.0.1.1:58408
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:38110
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.2:38110
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:38110
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.2:38110
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 02:58:10 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:10 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:10 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:11 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.1:58408
18/06/26 02:58:11 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.1.1:58408
18/06/26 02:58:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3729
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3635
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3813
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3554
18/06/26 02:58:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3107
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 2947
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3032
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 2758
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3788
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3377
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 4156
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3724
18/06/26 02:58:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 2959
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3009
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3220
18/06/26 02:58:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 4363
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 4308
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 4236
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3919
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 2879
18/06/26 02:58:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 4155
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 4185
18/06/26 02:58:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 723
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 763
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 934
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 837
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 4239
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3962
18/06/26 02:58:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3667
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3613
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3967
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3586
18/06/26 02:58:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3118
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3101
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3592
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 3273
18/06/26 02:58:11 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.2:38110
18/06/26 02:58:11 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.2:38110
18/06/26 02:58:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 6781
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 6857
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 7283
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 6424
18/06/26 02:58:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:11 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 7.2 (TID 74, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=0, mapId=0, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:11 INFO scheduler.TaskSetManager: Task 0.0 in stage 7.2 (TID 74) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:11 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 7 (processCmd at CliDriver.java:376) as failed due to a fetch failure from ShuffleMapStage 6 (processCmd at CliDriver.java:376)
18/06/26 02:58:11 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (processCmd at CliDriver.java:376) failed in 1.053 s due to org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

18/06/26 02:58:11 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 6 (processCmd at CliDriver.java:376) and ShuffleMapStage 7 (processCmd at CliDriver.java:376) due to fetch failure
18/06/26 02:58:11 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 29)
18/06/26 02:58:11 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
18/06/26 02:58:11 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 10.0.1.1, 38322, None)
18/06/26 02:58:11 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor
18/06/26 02:58:11 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 1 (epoch 29)
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:11 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 7.2 (TID 76, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=0, mapId=0, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:11 INFO scheduler.TaskSetManager: Task 2.0 in stage 7.2 (TID 76) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 7.2 (TID 83) in 1049 ms on 10.0.1.2 (executor 0) (3/12)
18/06/26 02:58:11 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:11 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/23/shuffle_3_6_0.data
18/06/26 02:58:11 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/39/shuffle_3_6_0.index
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:11 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 7.2 (TID 77, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=0, mapId=1, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:11 INFO scheduler.TaskSetManager: Task 3.0 in stage 7.2 (TID 77) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:11 INFO scheduler.DAGScheduler: Executor lost: 0 (epoch 29)
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:11 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:11 WARN storage.BlockManagerMasterEndpoint: No more replicas available for broadcast_18_piece0 !
18/06/26 02:58:11 WARN storage.BlockManagerMasterEndpoint: No more replicas available for broadcast_15_piece0 !
18/06/26 02:58:11 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.0.1.2, 37932, None)
18/06/26 02:58:11 WARN scheduler.TaskSetManager: Lost task 10.0 in stage 7.2 (TID 82, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=2, mapId=0, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:11 INFO storage.BlockManagerMaster: Removed 0 successfully in removeExecutor
18/06/26 02:58:11 INFO scheduler.TaskSetManager: Task 10.0 in stage 7.2 (TID 82) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:11 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 0 (epoch 29)
18/06/26 02:58:11 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 7.2 (TID 78, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=2, mapId=0, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:11 INFO scheduler.TaskSetManager: Task 8.0 in stage 7.2 (TID 78) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:11 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 7.2 (TID 75, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=0, mapId=1, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:11 INFO scheduler.TaskSetManager: Task 1.0 in stage 7.2 (TID 75) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:11 WARN scheduler.TaskSetManager: Lost task 9.0 in stage 7.2 (TID 80, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=2, mapId=0, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:11 INFO scheduler.TaskSetManager: Task 9.0 in stage 7.2 (TID 80) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 7.2 (TID 81) in 1058 ms on 10.0.1.2 (executor 0) (9/12)
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:11 WARN scheduler.TaskSetManager: Lost task 11.0 in stage 7.2 (TID 84, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=2, mapId=0, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/32/shuffle_2_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:11 INFO scheduler.DAGScheduler: Ignoring possibly bogus ShuffleMapTask(7, 5) completion from executor 0
18/06/26 02:58:11 INFO scheduler.TaskSetManager: Task 11.0 in stage 7.2 (TID 84) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:11 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 7.2 (TID 85) in 1056 ms on 10.0.1.2 (executor 0) (11/12)
18/06/26 02:58:11 INFO scheduler.DAGScheduler: Ignoring possibly bogus ShuffleMapTask(7, 7) completion from executor 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:11 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 7.2 (TID 79) in 1061 ms on 10.0.1.2 (executor 0) (12/12)
18/06/26 02:58:11 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.2, whose tasks have all completed, from pool 
18/06/26 02:58:11 INFO scheduler.DAGScheduler: Ignoring possibly bogus ShuffleMapTask(7, 4) completion from executor 0
18/06/26 02:58:12 INFO scheduler.DAGScheduler: Resubmitting failed stages
18/06/26 02:58:12 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:12 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 61.7 KB, free 400.4 MB)
18/06/26 02:58:12 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 25.2 KB, free 400.3 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on dc1master-lan1:36135 (size: 25.2 KB, free: 408.0 MB)
18/06/26 02:58:12 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:12 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[29] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: Adding task set 6.3 with 6 tasks
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:12 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[45] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.3 (TID 86, 10.0.1.1, executor 1, partition 0, ANY, 7907 bytes)
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.3 (TID 87, 10.0.1.2, executor 0, partition 1, ANY, 7907 bytes)
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.3 (TID 88, 10.0.1.1, executor 1, partition 2, ANY, 7907 bytes)
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.3 (TID 89, 10.0.1.2, executor 0, partition 3, ANY, 7907 bytes)
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.3 (TID 90, 10.0.1.1, executor 1, partition 4, ANY, 7907 bytes)
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.3 (TID 91, 10.0.1.2, executor 0, partition 5, ANY, 7907 bytes)
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 6
18/06/26 02:58:12 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 64.3 KB, free 400.3 MB)
18/06/26 02:58:12 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 26.2 KB, free 400.3 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on dc1master-lan1:36135 (size: 26.2 KB, free: 407.9 MB)
18/06/26 02:58:12 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:12 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[45] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2))
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: Adding task set 4.3 with 3 tasks
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:12 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.3 (TID 92, 10.0.1.1, executor 1, partition 0, ANY, 7905 bytes)
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.3 (TID 93, 10.0.1.2, executor 0, partition 1, ANY, 7905 bytes)
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 4.3 (TID 94, 10.0.1.1, executor 1, partition 2, ANY, 7905 bytes)
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 3
18/06/26 02:58:12 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 64.3 KB, free 400.2 MB)
18/06/26 02:58:12 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.2:37932 with 912.3 MB RAM, BlockManagerId(0, 10.0.1.2, 37932, None)
18/06/26 02:58:12 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.1:38322 with 912.3 MB RAM, BlockManagerId(1, 10.0.1.1, 38322, None)
18/06/26 02:58:12 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 26.2 KB, free 400.2 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on dc1master-lan1:36135 (size: 26.2 KB, free: 407.9 MB)
18/06/26 02:58:12 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:12 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[37] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: Adding task set 5.3 with 5 tasks
18/06/26 02:58:12 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:12 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:12 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:12 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:12 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:12 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 6
18/06/26 02:58:12 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 02:58:12 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 02:58:12 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 4
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.1:38322 (size: 38.5 KB, free: 912.3 MB)
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.3 (TID 95, 10.0.1.2, executor 0, partition 0, ANY, 7909 bytes)
18/06/26 02:58:12 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.3 (TID 96, 10.0.1.1, executor 1, partition 1, ANY, 7909 bytes)
18/06/26 02:58:12 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.3 (TID 97, 10.0.1.2, executor 0, partition 2, ANY, 7909 bytes)
18/06/26 02:58:12 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 5.3 (TID 98, 10.0.1.1, executor 1, partition 3, ANY, 7909 bytes)
18/06/26 02:58:12 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 5.3 (TID 99, 10.0.1.2, executor 0, partition 4, ANY, 7909 bytes)
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:12 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 5
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.1:38322 (size: 25.1 KB, free: 912.2 MB)
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 5
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 912.2 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 912.2 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 912.2 MB)
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 912.1 MB)
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 912.1 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 912.1 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 912.1 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.1:38322 (size: 38.6 KB, free: 912.0 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.1:38322 (size: 24.9 KB, free: 912.0 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.1.2:37932 (size: 38.5 KB, free: 912.3 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.1:38322 (size: 25.1 KB, free: 912.0 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.1.1:38322 (size: 521.0 B, free: 912.0 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.1.1:38322 (size: 44.7 KB, free: 911.9 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.1.2:37932 (size: 25.1 KB, free: 912.2 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.1.1:38322 (size: 25.2 KB, free: 911.9 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.1:38322 (size: 25.2 KB, free: 911.9 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 911.9 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.1:38322 (size: 362.4 KB, free: 911.5 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 911.5 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.1:38322 (size: 77.7 KB, free: 911.4 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 912.2 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.1:38322 (size: 25.0 KB, free: 911.4 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 912.2 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.1.1:38322 (size: 26.2 KB, free: 911.4 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 912.2 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.1.1:38322 (size: 38.6 KB, free: 911.3 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.1:38322 (size: 26.1 KB, free: 911.3 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 912.1 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.1:38322 (size: 25.1 KB, free: 911.3 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 912.1 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 912.1 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.1.1:38322 (size: 25.2 KB, free: 911.2 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 912.1 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.1.2:37932 (size: 38.6 KB, free: 912.0 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.1.2:37932 (size: 24.9 KB, free: 912.0 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 912.0 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.1.2:37932 (size: 25.1 KB, free: 912.0 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.1.2:37932 (size: 521.0 B, free: 911.9 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.1.2:37932 (size: 44.7 KB, free: 911.9 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.1.2:37932 (size: 25.2 KB, free: 911.9 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.1.2:37932 (size: 25.2 KB, free: 911.9 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 911.8 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.1.2:37932 (size: 362.4 KB, free: 911.5 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.1.2:37932 (size: 77.7 KB, free: 911.4 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.1.2:37932 (size: 25.0 KB, free: 911.4 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.1.2:37932 (size: 26.2 KB, free: 911.4 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.1.2:37932 (size: 38.6 KB, free: 911.3 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.1.2:37932 (size: 26.1 KB, free: 911.3 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.1.2:37932 (size: 25.1 KB, free: 911.3 MB)
18/06/26 02:58:12 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.1.2:37932 (size: 25.2 KB, free: 911.2 MB)
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 4.3 (TID 94) in 348 ms on 10.0.1.1 (executor 1) (1/3)
18/06/26 02:58:12 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:12 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/38/shuffle_2_2_0.data
18/06/26 02:58:12 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0c/shuffle_2_2_0.index
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:12 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:12 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 5.3 (TID 99) in 728 ms on 10.0.1.2 (executor 0) (1/5)
18/06/26 02:58:12 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:12 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/19/shuffle_1_4_0.data
18/06/26 02:58:12 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/35/shuffle_1_4_0.index
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:13 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.3 (TID 96) in 1458 ms on 10.0.1.1 (executor 1) (2/5)
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0a/shuffle_1_1_0.data
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_1_1_0.index
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:13 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.3 (TID 97) in 1489 ms on 10.0.1.2 (executor 0) (3/5)
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/17/shuffle_1_2_0.data
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0d/shuffle_1_2_0.index
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:13 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 5.3 (TID 98) in 1497 ms on 10.0.1.1 (executor 1) (4/5)
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/08/shuffle_1_3_0.data
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0c/shuffle_1_3_0.index
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:13 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.3 (TID 93) in 1507 ms on 10.0.1.2 (executor 0) (2/3)
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/29/shuffle_2_1_0.data
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0d/shuffle_2_1_0.index
18/06/26 02:58:13 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.3 (TID 92) in 1508 ms on 10.0.1.1 (executor 1) (3/3)
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.3, whose tasks have all completed, from pool 
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0a/shuffle_2_0_0.data
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_2_0_0.index
18/06/26 02:58:13 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (processCmd at CliDriver.java:376) finished in 1.515 s
18/06/26 02:58:13 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 02:58:13 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 5, ShuffleMapStage 6)
18/06/26 02:58:13 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
18/06/26 02:58:13 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:13 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.3 (TID 95) in 1505 ms on 10.0.1.2 (executor 0) (5/5)
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.3, whose tasks have all completed, from pool 
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/2b/shuffle_1_0_0.data
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0f/shuffle_1_0_0.index
18/06/26 02:58:13 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:376) finished in 1.513 s
18/06/26 02:58:13 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 02:58:13 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 6)
18/06/26 02:58:13 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
18/06/26 02:58:13 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:13 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 6.3 (TID 91) in 1841 ms on 10.0.1.2 (executor 0) (1/6)
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/27/shuffle_0_5_0.data
18/06/26 02:58:13 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/35/shuffle_0_5_0.index
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.3 (TID 86) in 2038 ms on 10.0.1.1 (executor 1) (2/6)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0c/shuffle_0_0_0.data
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/30/shuffle_0_0_0.index
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 6.3 (TID 88) in 2048 ms on 10.0.1.1 (executor 1) (3/6)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/36/shuffle_0_2_0.data
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_0_2_0.index
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.3 (TID 87) in 2070 ms on 10.0.1.2 (executor 0) (4/6)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/15/shuffle_0_1_0.data
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0f/shuffle_0_1_0.index
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 6.3 (TID 90) in 2116 ms on 10.0.1.1 (executor 1) (5/6)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(1, 10.0.1.1, 7337, None)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/38/shuffle_0_4_0.data
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/0c/shuffle_0_4_0.index
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 6.3 (TID 89) in 2132 ms on 10.0.1.2 (executor 0) (6/6)
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.3, whose tasks have all completed, from pool 
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.location.toString: BlockManagerId(0, 10.0.1.2, 7337, None)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getDataFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/29/shuffle_0_3_0.data
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MapOutputTracker.registerMapOutput status.getIndexFile: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0d/shuffle_0_3_0.index
18/06/26 02:58:14 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:376) finished in 2.141 s
18/06/26 02:58:14 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/06/26 02:58:14 INFO scheduler.DAGScheduler: running: Set()
18/06/26 02:58:14 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 7, ResultStage 8)
18/06/26 02:58:14 INFO scheduler.DAGScheduler: failed: Set()
18/06/26 02:58:14 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[50] at processCmd at CliDriver.java:376), which has no missing parents
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3788
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3788
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4363
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4363
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4155
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4155
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3667
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3667
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3118
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3118
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3377
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3377
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4308
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4308
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4185
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4185
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3613
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3613
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3101
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3101
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4156
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4156
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4236
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4236
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 4239
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 4239
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3967
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3967
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3592
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3592
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3724
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3724
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3919
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3919
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3962
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3962
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3586
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3586
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3273
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3273
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 6781
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 6781
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 6857
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 6857
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 7283
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 7283
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 6424
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 6424
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2959
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2959
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 723
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 723
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3009
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3009
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 763
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 763
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3220
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3220
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 934
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 934
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2879
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2879
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 837
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 837
18/06/26 02:58:14 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 97.0 KB, free 400.1 MB)
18/06/26 02:58:14 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 38.6 KB, free 400.0 MB)
18/06/26 02:58:14 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on dc1master-lan1:36135 (size: 38.6 KB, free: 407.9 MB)
18/06/26 02:58:14 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1074
18/06/26 02:58:14 INFO scheduler.DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[50] at processCmd at CliDriver.java:376) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: Adding task set 7.3 with 12 tasks
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.3 (TID 100, 10.0.1.1, executor 1, partition 0, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.3 (TID 101, 10.0.1.2, executor 0, partition 1, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 7.3 (TID 102, 10.0.1.1, executor 1, partition 2, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 7.3 (TID 103, 10.0.1.2, executor 0, partition 3, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 7.3 (TID 104, 10.0.1.1, executor 1, partition 4, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 7.3 (TID 105, 10.0.1.2, executor 0, partition 8, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 7.3 (TID 106, 10.0.1.1, executor 1, partition 5, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 7.3 (TID 107, 10.0.1.2, executor 0, partition 9, NODE_LOCAL, 7856 bytes)
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 7.3 (TID 108, 10.0.1.1, executor 1, partition 6, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 7.3 (TID 109, 10.0.1.2, executor 0, partition 10, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 7.3 (TID 110, 10.0.1.1, executor 1, partition 7, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 7.3 (TID 111, 10.0.1.2, executor 0, partition 11, NODE_LOCAL, 7856 bytes)
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 12
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 0
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 3
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 4
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 8
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 5
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 1
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 9
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 6
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 10
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 7
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::stageId: 7
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 5
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 6
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentStageId: 4
18/06/26 02:58:14 INFO scheduler.DAGScheduler: handleBeginEvent::parentShuffleMapStage
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.numReduces: 12
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceId: 11
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceHost: 10.0.1.2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.reduceExecutorId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: onTaskStart.appId: app-20180626025750-0055
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 2
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.1.1:38322 (size: 38.6 KB, free: 911.2 MB)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.1.2:37932 (size: 38.6 KB, free: 911.2 MB)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 3
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 4
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 8
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 2 to 10.0.1.2:38110
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 2 to 10.0.1.2:38110
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.1:58408
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.1:58408
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 5
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 1 to 10.0.1.1:58408
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 1 to 10.0.1.1:58408
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send terra finished for shuffle 0 to 10.0.1.2:38110
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: Handling request to send terra finished for shuffle 0 to 10.0.1.2:38110
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: MessageLoop reply ### true
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 9
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 6
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 10
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 7
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 1
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 0
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3729
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3635
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3813
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3554
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop take shuffleId: 2
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop reduceId: 11
18/06/26 02:58:14 ERROR spark.MapOutputTrackerMaster: TerraLoop.shuffleStatus reduceId isDefined.
18/06/26 02:58:14 INFO spark.MapOutputTrackerMaster: TerraLoop shuffle finished !!!
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 0
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3107
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 1
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2947
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 2
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 3032
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 3
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock compressedSizes(reduceId): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock MapStatus.decompressSize(compressedSizes(reduceId)): 2758
18/06/26 02:58:14 INFO scheduler.RawMapStatus: getSizeForBlock reduceId: 4
java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.scheduler.RawMapStatus$$anonfun$getSizeForBlock$2.apply(MapStatus.scala:106)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.scheduler.RawMapStatus.logInfo(MapStatus.scala:79)
	at org.apache.spark.scheduler.RawMapStatus.getSizeForBlock(MapStatus.scala:106)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:684)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8$$anonfun$apply$8.apply(MapOutputTracker.scala:669)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:669)
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$submitShuffle$8.apply(MapOutputTracker.scala:668)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.MapOutputTrackerMaster.submitShuffle(MapOutputTracker.scala:668)
	at org.apache.spark.MapOutputTrackerMaster$TerraLoop.run(MapOutputTracker.scala:602)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:15 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.1.2:38110
18/06/26 02:58:15 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3107
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 2947
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3032
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 2758
18/06/26 02:58:15 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 2959
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3009
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3220
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 2879
18/06/26 02:58:15 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 723
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 763
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 934
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 837
18/06/26 02:58:15 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.1:58408
18/06/26 02:58:15 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3729
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3635
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3813
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3554
18/06/26 02:58:15 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3788
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3377
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 4156
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3724
18/06/26 02:58:15 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 4363
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 4308
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 4236
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3919
18/06/26 02:58:15 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 4155
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 4185
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 4239
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3962
18/06/26 02:58:15 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3667
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3613
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3967
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3586
18/06/26 02:58:15 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3118
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3101
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3592
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 3273
18/06/26 02:58:15 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.1.1:58408
18/06/26 02:58:15 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 6781
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 6857
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 7283
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 6424
18/06/26 02:58:15 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO scheduler.RawMapStatus: WriteExternal compressedSizes
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO scheduler.RawMapStatus: 0
18/06/26 02:58:15 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.1.2:38110
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:15 WARN scheduler.TaskSetManager: Lost task 9.0 in stage 7.3 (TID 107, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=2, mapId=0, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_2_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_2_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:15 INFO scheduler.TaskSetManager: Task 9.0 in stage 7.3 (TID 107) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:15 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 7 (processCmd at CliDriver.java:376) as failed due to a fetch failure from ShuffleMapStage 4 (processCmd at CliDriver.java:376)
18/06/26 02:58:15 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (processCmd at CliDriver.java:376) failed in 1.051 s due to org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_2_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_2_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:15 WARN scheduler.TaskSetManager: Lost task 10.0 in stage 7.3 (TID 109, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=2, mapId=0, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_2_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_2_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:15 INFO scheduler.TaskSetManager: Task 10.0 in stage 7.3 (TID 109) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:15 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 7.3 (TID 105, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=2, mapId=0, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_2_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_2_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:15 INFO scheduler.TaskSetManager: Task 8.0 in stage 7.3 (TID 105) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:15 WARN scheduler.TaskSetManager: Lost task 11.0 in stage 7.3 (TID 111, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=2, mapId=0, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_2_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_2_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:15 INFO scheduler.TaskSetManager: Task 11.0 in stage 7.3 (TID 111) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:15 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 7.3 (TID 101, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=0, mapId=0, reduceId=1, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:15 INFO scheduler.TaskSetManager: Task 1.0 in stage 7.3 (TID 101) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:15 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 7.3 (TID 102, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=0, mapId=1, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:15 INFO scheduler.TaskSetManager: Task 2.0 in stage 7.3 (TID 102) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:15 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 7.3 (TID 103, 10.0.1.2, executor 0): FetchFailed(BlockManagerId(0, 10.0.1.2, 37932, None), shuffleId=0, mapId=0, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/30/shuffle_0_0_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/30/shuffle_0_0_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:15 INFO scheduler.TaskSetManager: Task 3.0 in stage 7.3 (TID 103) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:15 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 7.3 (TID 100, 10.0.1.1, executor 1): FetchFailed(BlockManagerId(1, 10.0.1.1, 38322, None), shuffleId=0, mapId=1, reduceId=0, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0f/shuffle_0_1_0.index
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /tmp/spark-6e3b625e-cea2-4464-8d6f-907283947a20/executor-e2db5882-40dc-4a9d-bf1a-92fd9fa423b4/blockmgr-7a8154bd-c600-4bc8-9a6a-66a9eb44560a/0f/shuffle_0_1_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241)
	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	... 7 more

)
18/06/26 02:58:15 INFO scheduler.TaskSetManager: Task 0.0 in stage 7.3 (TID 100) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
18/06/26 02:58:15 INFO scheduler.DAGScheduler: Job 4 failed: processCmd at CliDriver.java:376, took 14.799584 s
18/06/26 02:58:15 INFO scheduler.DAGScheduler: Executor lost: 0 (epoch 42)
18/06/26 02:58:15 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
18/06/26 02:58:15 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.0.1.2, 37932, None)
18/06/26 02:58:15 INFO storage.BlockManagerMaster: Removed 0 successfully in removeExecutor
18/06/26 02:58:15 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 0 (epoch 42)
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 7.3 (TID 104) in 1054 ms on 10.0.1.1 (executor 1) (9/12)
18/06/26 02:58:15 ERROR thriftserver.SparkSQLDriver: Failed in [with ss as (
 select
          i_item_id,sum(ss_ext_sales_price) total_sales
 from
        store_sales,
        date_dim,
         customer_address,
         item
 where
         item.i_item_id in (select
  i.i_item_id
from
 item i
where i_category in ('Children'))
 and     ss_item_sk              = i_item_sk
 and     ss_sold_date_sk         = d_date_sk
 and     d_year                  = 1999
 and     d_moy                   = 9
 and     ss_addr_sk              = ca_address_sk
 and     ca_gmt_offset           = -6 
 group by i_item_id),
 cs as (
 select
          i_item_id,sum(cs_ext_sales_price) total_sales
 from
        catalog_sales,
        date_dim,
         customer_address,
         item
 where
         item.i_item_id               in (select
  i.i_item_id
from
 item i
where i_category in ('Children'))
 and     cs_item_sk              = i_item_sk
 and     cs_sold_date_sk         = d_date_sk
 and     d_year                  = 1999
 and     d_moy                   = 9
 and     cs_bill_addr_sk         = ca_address_sk
 and     ca_gmt_offset           = -6 
 group by i_item_id),
 ws as (
 select
          i_item_id,sum(ws_ext_sales_price) total_sales
 from
        web_sales,
        date_dim,
         customer_address,
         item
 where
         item.i_item_id               in (select
  i.i_item_id
from
 item i
where i_category in ('Children'))
 and     ws_item_sk              = i_item_sk
 and     ws_sold_date_sk         = d_date_sk
 and     d_year                  = 1999
 and     d_moy                   = 9
 and     ws_bill_addr_sk         = ca_address_sk
 and     ca_gmt_offset           = -6
 group by i_item_id)
  select   
  i_item_id
,sum(total_sales) total_sales
 from  (select * from ss 
        union all
        select * from cs 
        union all
        select * from ws) tmp1
 group by i_item_id
 order by i_item_id
      ,total_sales
 limit 100]
org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 7 (processCmd at CliDriver.java:376) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_2_0_0.index 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64) 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30) 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source) 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source) 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_2_0_0.index 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214) 	at java.nio.file.Files.newByteChannel(Files.java:361) 	at java.nio.file.Files.newByteChannel(Files.java:407) 	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241) 	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58) 	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	... 7 more 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1635)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1623)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1622)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1622)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1394)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1853)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1805)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1794)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2030)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2127)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1029)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1011)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1433)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1420)
	at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:324)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:122)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:363)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:198)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:837)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:912)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:923)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
18/06/26 02:58:15 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 7.3 (TID 108) in 1054 ms on 10.0.1.1 (executor 1) (10/12)
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:58:15 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 7.3 (TID 106) in 1061 ms on 10.0.1.1 (executor 1) (11/12)
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.3, whose tasks have all completed, from pool 
18/06/26 02:58:15 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 7.3 (TID 110) in 1061 ms on 10.0.1.1 (executor 1) (12/12)
18/06/26 02:58:15 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.3, whose tasks have all completed, from pool 
org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 7 (processCmd at CliDriver.java:376) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_2_0_0.index 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:652) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:583) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64) 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30) 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.agg_doAggregateWithKeys_0$(Unknown Source) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source) 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.agg_doAggregateWithKeys_0$(Unknown Source) 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage19.processNext(Unknown Source) 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:618) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:748) Caused by: java.nio.file.NoSuchFileException: /tmp/spark-70f7e7c2-3be3-4eeb-8a73-612cf7e61654/executor-641d5bba-3f31-40f0-84b7-dd4f087fc442/blockmgr-0da11a41-252a-4419-85fe-e963d22621dc/32/shuffle_2_0_0.index 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214) 	at java.nio.file.Files.newByteChannel(Files.java:361) 	at java.nio.file.Files.newByteChannel(Files.java:407) 	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockDataNew(IndexShuffleBlockResolver.scala:241) 	at org.apache.spark.storage.BlockManager.getBlockDataNew(BlockManager.scala:396) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocksNew(ShuffleBlockFetcherIterator.scala:395) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:443) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:162) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:58) 	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	... 7 more 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1635)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1623)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1622)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1622)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1394)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1853)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1805)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1794)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2030)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2127)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1029)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1011)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1433)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1420)
	at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:324)
	at org.apache.spark.sql.execution.QueryExecution.hiveResultString(QueryExecution.scala:122)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver$$anonfun$run$1.apply(SparkSQLDriver.scala:64)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:363)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:198)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:837)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:912)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:923)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

18/06/26 02:58:15 INFO server.AbstractConnector: Stopped Spark@32e5af53{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18/06/26 02:58:15 INFO ui.SparkUI: Stopped Spark web UI at http://dc1master-lan1:4040
18/06/26 02:58:15 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors
18/06/26 02:58:15 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
18/06/26 02:58:15 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/06/26 02:58:15 INFO memory.MemoryStore: MemoryStore cleared
18/06/26 02:58:15 INFO storage.BlockManager: BlockManager stopped
18/06/26 02:58:15 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/06/26 02:58:15 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/06/26 02:58:15 INFO spark.SparkContext: Successfully stopped SparkContext
18/06/26 02:58:15 INFO util.ShutdownHookManager: Shutdown hook called
18/06/26 02:58:15 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-00961cc9-38f3-4eec-9c06-0673966c1798
18/06/26 02:58:15 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-cdd40faa-b26e-4351-ae4a-7d5a3dfb19bb
