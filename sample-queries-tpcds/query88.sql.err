18/06/26 03:32:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/06/26 03:32:06 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18/06/26 03:32:06 INFO metastore.ObjectStore: ObjectStore, initialize called
18/06/26 03:32:06 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
18/06/26 03:32:06 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored
18/06/26 03:32:08 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
18/06/26 03:32:09 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 03:32:09 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 03:32:09 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 03:32:09 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 03:32:09 INFO DataNucleus.Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18/06/26 03:32:09 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
18/06/26 03:32:09 INFO metastore.ObjectStore: Initialized ObjectStore
18/06/26 03:32:10 INFO metastore.HiveMetaStore: Added admin role in metastore
18/06/26 03:32:10 INFO metastore.HiveMetaStore: Added public role in metastore
18/06/26 03:32:10 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
18/06/26 03:32:10 INFO metastore.HiveMetaStore: 0: get_all_databases
18/06/26 03:32:10 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_all_databases	
18/06/26 03:32:10 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
18/06/26 03:32:10 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
18/06/26 03:32:10 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 03:32:10 INFO metastore.HiveMetaStore: 0: get_functions: db=tpcds_text_2 pat=*
18/06/26 03:32:10 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_functions: db=tpcds_text_2 pat=*	
18/06/26 03:32:10 INFO session.SessionState: Created local directory: /tmp/ea080e98-4334-472b-9d85-313ec690c1f1_resources
18/06/26 03:32:10 INFO session.SessionState: Created HDFS directory: /tmp/hive/wentingt/ea080e98-4334-472b-9d85-313ec690c1f1
18/06/26 03:32:10 INFO session.SessionState: Created local directory: /tmp/wentingt/ea080e98-4334-472b-9d85-313ec690c1f1
18/06/26 03:32:10 INFO session.SessionState: Created HDFS directory: /tmp/hive/wentingt/ea080e98-4334-472b-9d85-313ec690c1f1/_tmp_space.db
18/06/26 03:32:11 INFO spark.SparkContext: Running Spark version 2.4.0-SNAPSHOT
18/06/26 03:32:11 INFO spark.SparkContext: Submitted application: SparkSQL::10.0.1.253
18/06/26 03:32:11 INFO spark.SecurityManager: Changing view acls to: wentingt
18/06/26 03:32:11 INFO spark.SecurityManager: Changing modify acls to: wentingt
18/06/26 03:32:11 INFO spark.SecurityManager: Changing view acls groups to: 
18/06/26 03:32:11 INFO spark.SecurityManager: Changing modify acls groups to: 
18/06/26 03:32:11 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(wentingt); groups with view permissions: Set(); users  with modify permissions: Set(wentingt); groups with modify permissions: Set()
18/06/26 03:32:11 INFO util.Utils: Successfully started service 'sparkDriver' on port 44183.
18/06/26 03:32:11 INFO spark.SparkEnv: Registering MapOutputTracker
18/06/26 03:32:11 INFO spark.SparkEnv: Registering BlockManagerMaster
18/06/26 03:32:11 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/06/26 03:32:11 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/06/26 03:32:11 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-36c783f9-ea98-4120-b040-c6ef6e164ae9
18/06/26 03:32:11 INFO memory.MemoryStore: MemoryStore started with capacity 399.6 MB
18/06/26 03:32:11 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/06/26 03:32:11 INFO util.log: Logging initialized @7299ms
18/06/26 03:32:11 INFO server.Server: jetty-9.3.z-SNAPSHOT
18/06/26 03:32:11 INFO server.Server: Started @7404ms
18/06/26 03:32:11 INFO server.AbstractConnector: Started ServerConnector@52fdbb7d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18/06/26 03:32:11 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a9b3956{/jobs,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51fe7f15{/jobs/json,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5873f3f0{/jobs/job,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63dda940{/jobs/job/json,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41f964f9{/stages,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@652e345{/stages/json,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7574d4ad{/stages/stage,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6060146b{/stages/stage/json,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33627576{/stages/pool,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27bc1d44{/stages/pool/json,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1af677f8{/storage,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a55fb81{/storage/json,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a3cf878{/storage/rdd,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d2d8846{/storage/rdd/json,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34cd65ac{/environment,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61911947{/environment/json,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c53c235{/executors,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2dcd0e41{/executors/json,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7272ee51{/executors/threadDump,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b409a79{/executors/threadDump/json,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5940b14e{/static,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3070f3e6{/,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3271ec2a{/api,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c7cd434{/jobs/job/kill,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@13004dd8{/stages/stage/kill,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://dc1master-lan1:4040
18/06/26 03:32:12 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 0
18/06/26 03:32:12 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:32:12 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://dc1master-lan1:7077...
18/06/26 03:32:12 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 0
18/06/26 03:32:12 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:32:12 INFO client.TransportClientFactory: Successfully created connection to dc1master-lan1/10.0.1.253:7077 after 47 ms (0 ms spent in bootstraps)
18/06/26 03:32:12 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20180626033212-0075
18/06/26 03:32:12 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20180626033212-0075/0 on worker-20180626004527-10.0.1.2-38332 (10.0.1.2:38332) with 10 core(s)
18/06/26 03:32:12 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20180626033212-0075/0 on hostPort 10.0.1.2:38332 with 10 core(s), 2.0 GB RAM
18/06/26 03:32:12 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20180626033212-0075/1 on worker-20180626004527-10.0.1.1-34053 (10.0.1.1:34053) with 10 core(s)
18/06/26 03:32:12 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20180626033212-0075/1 on hostPort 10.0.1.1:34053 with 10 core(s), 2.0 GB RAM
18/06/26 03:32:12 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35580.
18/06/26 03:32:12 INFO netty.NettyBlockTransferService: Server created on dc1master-lan1:35580
18/06/26 03:32:12 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/06/26 03:32:12 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20180626033212-0075/0 is now RUNNING
18/06/26 03:32:12 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20180626033212-0075/1 is now RUNNING
18/06/26 03:32:12 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, dc1master-lan1, 35580, None)
18/06/26 03:32:12 INFO storage.BlockManagerMasterEndpoint: Registering block manager dc1master-lan1:35580 with 399.6 MB RAM, BlockManagerId(driver, dc1master-lan1, 35580, None)
18/06/26 03:32:12 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, dc1master-lan1, 35580, None)
18/06/26 03:32:12 INFO storage.BlockManager: external shuffle service port = 7337
18/06/26 03:32:12 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, dc1master-lan1, 35580, None)
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@674184d{/metrics/json,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO scheduler.EventLoggingListener: Logging events to hdfs://dc1master:9000/user/wentingt/spark-logs/app-20180626033212-0075
18/06/26 03:32:12 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
18/06/26 03:32:12 INFO internal.SharedState: loading hive config file: file:/users/wentingt/spark-terra/conf/hive-site.xml
18/06/26 03:32:12 INFO internal.SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
18/06/26 03:32:12 INFO internal.SharedState: Warehouse path is '/user/hive/warehouse'.
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ff09659{/SQL,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@517fbf62{/SQL/json,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@429e7914{/SQL/execution,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@298263fa{/SQL/execution/json,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@280484c7{/static/sql,null,AVAILABLE,@Spark}
18/06/26 03:32:12 INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
18/06/26 03:32:12 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
18/06/26 03:32:12 INFO metastore.HiveMetaStore: 0: get_database: default
18/06/26 03:32:12 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: default	
18/06/26 03:32:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 0
18/06/26 03:32:13 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:32:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 0
18/06/26 03:32:13 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:32:13 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
18/06/26 03:32:13 INFO metastore.HiveMetaStore: 0: get_database: global_temp
18/06/26 03:32:13 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: global_temp	
18/06/26 03:32:13 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
18/06/26 03:32:13 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:32:13 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:32:13 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:32:13 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:32:13 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:32:13 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:32:13 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:32:13 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:32:13 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:32:13 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:32:13 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:32:13 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:32:13 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:32:13 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:32:13 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:32:13 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:32:13 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 03:32:13 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 03:32:13 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 03:32:13 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 03:32:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 0
18/06/26 03:32:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:32:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 0
18/06/26 03:32:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=household_demographics
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=household_demographics	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=time_dim
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=time_dim	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store	
18/06/26 03:32:14 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.1.2:45638) with ID 0
18/06/26 03:32:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 03:32:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:32:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 03:32:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=household_demographics
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=household_demographics	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=time_dim
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=time_dim	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store	
18/06/26 03:32:14 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.2:44559 with 912.3 MB RAM, BlockManagerId(0, 10.0.1.2, 44559, None)
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=household_demographics
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=household_demographics	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=time_dim
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=time_dim	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store	
18/06/26 03:32:14 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.1.1:40810) with ID 1
18/06/26 03:32:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:32:14 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:32:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:32:14 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=household_demographics
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=household_demographics	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=time_dim
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=time_dim	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=household_demographics
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=household_demographics	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=time_dim
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=time_dim	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store	
18/06/26 03:32:14 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.1:35790 with 912.3 MB RAM, BlockManagerId(1, 10.0.1.1, 35790, None)
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=household_demographics
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=household_demographics	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=time_dim
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=time_dim	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=household_demographics
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=household_demographics	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=time_dim
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=time_dim	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=household_demographics
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=household_demographics	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=time_dim
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=time_dim	
18/06/26 03:32:14 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store
18/06/26 03:32:14 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store	
18/06/26 03:32:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:32:15 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:32:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:32:15 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:32:16 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:32:16 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:32:16 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:32:16 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 03:32:16 WARN util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
18/06/26 03:32:17 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 03:32:17 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 03:32:17 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 03:32:17 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
Error in query: Detected implicit cartesian product for INNER join between logical plans
Join Inner
:- Join Inner
:  :- Join Inner
:  :  :- Join Inner
:  :  :  :- Join Inner
:  :  :  :  :- Join Inner
:  :  :  :  :  :- Aggregate [count(1) AS h8_30_to_9#0L]
:  :  :  :  :  :  +- Project
:  :  :  :  :  :     +- Join Inner, (ss_store_sk#23 = s_store_sk#54)
:  :  :  :  :  :        :- Project [ss_store_sk#23]
:  :  :  :  :  :        :  +- Join Inner, (ss_sold_time_sk#17 = t_time_sk#44)
:  :  :  :  :  :        :     :- Project [ss_sold_time_sk#17, ss_store_sk#23]
:  :  :  :  :  :        :     :  +- Join Inner, (ss_hdemo_sk#21 = hd_demo_sk#39)
:  :  :  :  :  :        :     :     :- Project [ss_sold_time_sk#17, ss_hdemo_sk#21, ss_store_sk#23]
:  :  :  :  :  :        :     :     :  +- Filter ((isnotnull(ss_hdemo_sk#21) && isnotnull(ss_sold_time_sk#17)) && isnotnull(ss_store_sk#23))
:  :  :  :  :  :        :     :     :     +- HiveTableRelation `tpcds_text_2`.`store_sales`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [ss_sold_date_sk#16, ss_sold_time_sk#17, ss_item_sk#18, ss_customer_sk#19, ss_cdemo_sk#20, ss_hdemo_sk#21, ss_addr_sk#22, ss_store_sk#23, ss_promo_sk#24, ss_ticket_number#25L, ss_quantity#26, ss_wholesale_cost#27, ss_list_price#28, ss_sales_price#29, ss_ext_discount_amt#30, ss_ext_sales_price#31, ss_ext_wholesale_cost#32, ss_ext_list_price#33, ss_ext_tax#34, ss_coupon_amt#35, ss_net_paid#36, ss_net_paid_inc_tax#37, ss_net_profit#38]
:  :  :  :  :  :        :     :     +- Project [hd_demo_sk#39]
:  :  :  :  :  :        :     :        +- Filter (((((hd_dep_count#42 = 3) && (hd_vehicle_count#43 <= 5)) || ((hd_dep_count#42 = 0) && (hd_vehicle_count#43 <= 2))) || ((hd_dep_count#42 = 1) && (hd_vehicle_count#43 <= 3))) && isnotnull(hd_demo_sk#39))
:  :  :  :  :  :        :     :           +- HiveTableRelation `tpcds_text_2`.`household_demographics`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [hd_demo_sk#39, hd_income_band_sk#40, hd_buy_potential#41, hd_dep_count#42, hd_vehicle_count#43]
:  :  :  :  :  :        :     +- Project [t_time_sk#44]
:  :  :  :  :  :        :        +- Filter (((isnotnull(t_hour#47) && isnotnull(t_minute#48)) && ((t_hour#47 = 8) && (t_minute#48 >= 30))) && isnotnull(t_time_sk#44))
:  :  :  :  :  :        :           +- HiveTableRelation `tpcds_text_2`.`time_dim`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [t_time_sk#44, t_time_id#45, t_time#46, t_hour#47, t_minute#48, t_second#49, t_am_pm#50, t_shift#51, t_sub_shift#52, t_meal_time#53]
:  :  :  :  :  :        +- Project [s_store_sk#54]
:  :  :  :  :  :           +- Filter ((isnotnull(s_store_name#59) && (s_store_name#59 = ese)) && isnotnull(s_store_sk#54))
:  :  :  :  :  :              +- HiveTableRelation `tpcds_text_2`.`store`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [s_store_sk#54, s_store_id#55, s_rec_start_date#56, s_rec_end_date#57, s_closed_date_sk#58, s_store_name#59, s_number_employees#60, s_floor_space#61, s_hours#62, s_manager#63, s_market_id#64, s_geography_class#65, s_market_desc#66, s_market_manager#67, s_division_id#68, s_division_name#69, s_company_id#70, s_company_name#71, s_street_number#72, s_street_name#73, s_street_type#74, s_suite_number#75, s_city#76, s_county#77, ... 5 more fields]
:  :  :  :  :  +- Aggregate [count(1) AS h9_to_9_30#1L]
:  :  :  :  :     +- Project
:  :  :  :  :        +- Join Inner, (ss_store_sk#90 = s_store_sk#121)
:  :  :  :  :           :- Project [ss_store_sk#90]
:  :  :  :  :           :  +- Join Inner, (ss_sold_time_sk#84 = t_time_sk#111)
:  :  :  :  :           :     :- Project [ss_sold_time_sk#84, ss_store_sk#90]
:  :  :  :  :           :     :  +- Join Inner, (ss_hdemo_sk#88 = hd_demo_sk#106)
:  :  :  :  :           :     :     :- Project [ss_sold_time_sk#84, ss_hdemo_sk#88, ss_store_sk#90]
:  :  :  :  :           :     :     :  +- Filter ((isnotnull(ss_hdemo_sk#88) && isnotnull(ss_sold_time_sk#84)) && isnotnull(ss_store_sk#90))
:  :  :  :  :           :     :     :     +- HiveTableRelation `tpcds_text_2`.`store_sales`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [ss_sold_date_sk#83, ss_sold_time_sk#84, ss_item_sk#85, ss_customer_sk#86, ss_cdemo_sk#87, ss_hdemo_sk#88, ss_addr_sk#89, ss_store_sk#90, ss_promo_sk#91, ss_ticket_number#92L, ss_quantity#93, ss_wholesale_cost#94, ss_list_price#95, ss_sales_price#96, ss_ext_discount_amt#97, ss_ext_sales_price#98, ss_ext_wholesale_cost#99, ss_ext_list_price#100, ss_ext_tax#101, ss_coupon_amt#102, ss_net_paid#103, ss_net_paid_inc_tax#104, ss_net_profit#105]
:  :  :  :  :           :     :     +- Project [hd_demo_sk#106]
:  :  :  :  :           :     :        +- Filter (((((hd_dep_count#109 = 3) && (hd_vehicle_count#110 <= 5)) || ((hd_dep_count#109 = 0) && (hd_vehicle_count#110 <= 2))) || ((hd_dep_count#109 = 1) && (hd_vehicle_count#110 <= 3))) && isnotnull(hd_demo_sk#106))
:  :  :  :  :           :     :           +- HiveTableRelation `tpcds_text_2`.`household_demographics`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [hd_demo_sk#106, hd_income_band_sk#107, hd_buy_potential#108, hd_dep_count#109, hd_vehicle_count#110]
:  :  :  :  :           :     +- Project [t_time_sk#111]
:  :  :  :  :           :        +- Filter (((isnotnull(t_hour#114) && isnotnull(t_minute#115)) && ((t_hour#114 = 9) && (t_minute#115 < 30))) && isnotnull(t_time_sk#111))
:  :  :  :  :           :           +- HiveTableRelation `tpcds_text_2`.`time_dim`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [t_time_sk#111, t_time_id#112, t_time#113, t_hour#114, t_minute#115, t_second#116, t_am_pm#117, t_shift#118, t_sub_shift#119, t_meal_time#120]
:  :  :  :  :           +- Project [s_store_sk#121]
:  :  :  :  :              +- Filter ((isnotnull(s_store_name#126) && (s_store_name#126 = ese)) && isnotnull(s_store_sk#121))
:  :  :  :  :                 +- HiveTableRelation `tpcds_text_2`.`store`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [s_store_sk#121, s_store_id#122, s_rec_start_date#123, s_rec_end_date#124, s_closed_date_sk#125, s_store_name#126, s_number_employees#127, s_floor_space#128, s_hours#129, s_manager#130, s_market_id#131, s_geography_class#132, s_market_desc#133, s_market_manager#134, s_division_id#135, s_division_name#136, s_company_id#137, s_company_name#138, s_street_number#139, s_street_name#140, s_street_type#141, s_suite_number#142, s_city#143, s_county#144, ... 5 more fields]
:  :  :  :  +- Aggregate [count(1) AS h9_30_to_10#2L]
:  :  :  :     +- Project
:  :  :  :        +- Join Inner, (ss_store_sk#157 = s_store_sk#188)
:  :  :  :           :- Project [ss_store_sk#157]
:  :  :  :           :  +- Join Inner, (ss_sold_time_sk#151 = t_time_sk#178)
:  :  :  :           :     :- Project [ss_sold_time_sk#151, ss_store_sk#157]
:  :  :  :           :     :  +- Join Inner, (ss_hdemo_sk#155 = hd_demo_sk#173)
:  :  :  :           :     :     :- Project [ss_sold_time_sk#151, ss_hdemo_sk#155, ss_store_sk#157]
:  :  :  :           :     :     :  +- Filter ((isnotnull(ss_hdemo_sk#155) && isnotnull(ss_sold_time_sk#151)) && isnotnull(ss_store_sk#157))
:  :  :  :           :     :     :     +- HiveTableRelation `tpcds_text_2`.`store_sales`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [ss_sold_date_sk#150, ss_sold_time_sk#151, ss_item_sk#152, ss_customer_sk#153, ss_cdemo_sk#154, ss_hdemo_sk#155, ss_addr_sk#156, ss_store_sk#157, ss_promo_sk#158, ss_ticket_number#159L, ss_quantity#160, ss_wholesale_cost#161, ss_list_price#162, ss_sales_price#163, ss_ext_discount_amt#164, ss_ext_sales_price#165, ss_ext_wholesale_cost#166, ss_ext_list_price#167, ss_ext_tax#168, ss_coupon_amt#169, ss_net_paid#170, ss_net_paid_inc_tax#171, ss_net_profit#172]
:  :  :  :           :     :     +- Project [hd_demo_sk#173]
:  :  :  :           :     :        +- Filter (((((hd_dep_count#176 = 3) && (hd_vehicle_count#177 <= 5)) || ((hd_dep_count#176 = 0) && (hd_vehicle_count#177 <= 2))) || ((hd_dep_count#176 = 1) && (hd_vehicle_count#177 <= 3))) && isnotnull(hd_demo_sk#173))
:  :  :  :           :     :           +- HiveTableRelation `tpcds_text_2`.`household_demographics`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [hd_demo_sk#173, hd_income_band_sk#174, hd_buy_potential#175, hd_dep_count#176, hd_vehicle_count#177]
:  :  :  :           :     +- Project [t_time_sk#178]
:  :  :  :           :        +- Filter (((isnotnull(t_hour#181) && isnotnull(t_minute#182)) && ((t_hour#181 = 9) && (t_minute#182 >= 30))) && isnotnull(t_time_sk#178))
:  :  :  :           :           +- HiveTableRelation `tpcds_text_2`.`time_dim`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [t_time_sk#178, t_time_id#179, t_time#180, t_hour#181, t_minute#182, t_second#183, t_am_pm#184, t_shift#185, t_sub_shift#186, t_meal_time#187]
:  :  :  :           +- Project [s_store_sk#188]
:  :  :  :              +- Filter ((isnotnull(s_store_name#193) && (s_store_name#193 = ese)) && isnotnull(s_store_sk#188))
:  :  :  :                 +- HiveTableRelation `tpcds_text_2`.`store`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [s_store_sk#188, s_store_id#189, s_rec_start_date#190, s_rec_end_date#191, s_closed_date_sk#192, s_store_name#193, s_number_employees#194, s_floor_space#195, s_hours#196, s_manager#197, s_market_id#198, s_geography_class#199, s_market_desc#200, s_market_manager#201, s_division_id#202, s_division_name#203, s_company_id#204, s_company_name#205, s_street_number#206, s_street_name#207, s_street_type#208, s_suite_number#209, s_city#210, s_county#211, ... 5 more fields]
:  :  :  +- Aggregate [count(1) AS h10_to_10_30#3L]
:  :  :     +- Project
:  :  :        +- Join Inner, (ss_store_sk#224 = s_store_sk#255)
:  :  :           :- Project [ss_store_sk#224]
:  :  :           :  +- Join Inner, (ss_sold_time_sk#218 = t_time_sk#245)
:  :  :           :     :- Project [ss_sold_time_sk#218, ss_store_sk#224]
:  :  :           :     :  +- Join Inner, (ss_hdemo_sk#222 = hd_demo_sk#240)
:  :  :           :     :     :- Project [ss_sold_time_sk#218, ss_hdemo_sk#222, ss_store_sk#224]
:  :  :           :     :     :  +- Filter ((isnotnull(ss_hdemo_sk#222) && isnotnull(ss_sold_time_sk#218)) && isnotnull(ss_store_sk#224))
:  :  :           :     :     :     +- HiveTableRelation `tpcds_text_2`.`store_sales`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [ss_sold_date_sk#217, ss_sold_time_sk#218, ss_item_sk#219, ss_customer_sk#220, ss_cdemo_sk#221, ss_hdemo_sk#222, ss_addr_sk#223, ss_store_sk#224, ss_promo_sk#225, ss_ticket_number#226L, ss_quantity#227, ss_wholesale_cost#228, ss_list_price#229, ss_sales_price#230, ss_ext_discount_amt#231, ss_ext_sales_price#232, ss_ext_wholesale_cost#233, ss_ext_list_price#234, ss_ext_tax#235, ss_coupon_amt#236, ss_net_paid#237, ss_net_paid_inc_tax#238, ss_net_profit#239]
:  :  :           :     :     +- Project [hd_demo_sk#240]
:  :  :           :     :        +- Filter (((((hd_dep_count#243 = 3) && (hd_vehicle_count#244 <= 5)) || ((hd_dep_count#243 = 0) && (hd_vehicle_count#244 <= 2))) || ((hd_dep_count#243 = 1) && (hd_vehicle_count#244 <= 3))) && isnotnull(hd_demo_sk#240))
:  :  :           :     :           +- HiveTableRelation `tpcds_text_2`.`household_demographics`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [hd_demo_sk#240, hd_income_band_sk#241, hd_buy_potential#242, hd_dep_count#243, hd_vehicle_count#244]
:  :  :           :     +- Project [t_time_sk#245]
:  :  :           :        +- Filter (((isnotnull(t_hour#248) && isnotnull(t_minute#249)) && ((t_hour#248 = 10) && (t_minute#249 < 30))) && isnotnull(t_time_sk#245))
:  :  :           :           +- HiveTableRelation `tpcds_text_2`.`time_dim`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [t_time_sk#245, t_time_id#246, t_time#247, t_hour#248, t_minute#249, t_second#250, t_am_pm#251, t_shift#252, t_sub_shift#253, t_meal_time#254]
:  :  :           +- Project [s_store_sk#255]
:  :  :              +- Filter ((isnotnull(s_store_name#260) && (s_store_name#260 = ese)) && isnotnull(s_store_sk#255))
:  :  :                 +- HiveTableRelation `tpcds_text_2`.`store`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [s_store_sk#255, s_store_id#256, s_rec_start_date#257, s_rec_end_date#258, s_closed_date_sk#259, s_store_name#260, s_number_employees#261, s_floor_space#262, s_hours#263, s_manager#264, s_market_id#265, s_geography_class#266, s_market_desc#267, s_market_manager#268, s_division_id#269, s_division_name#270, s_company_id#271, s_company_name#272, s_street_number#273, s_street_name#274, s_street_type#275, s_suite_number#276, s_city#277, s_county#278, ... 5 more fields]
:  :  +- Aggregate [count(1) AS h10_30_to_11#4L]
:  :     +- Project
:  :        +- Join Inner, (ss_store_sk#291 = s_store_sk#322)
:  :           :- Project [ss_store_sk#291]
:  :           :  +- Join Inner, (ss_sold_time_sk#285 = t_time_sk#312)
:  :           :     :- Project [ss_sold_time_sk#285, ss_store_sk#291]
:  :           :     :  +- Join Inner, (ss_hdemo_sk#289 = hd_demo_sk#307)
:  :           :     :     :- Project [ss_sold_time_sk#285, ss_hdemo_sk#289, ss_store_sk#291]
:  :           :     :     :  +- Filter ((isnotnull(ss_hdemo_sk#289) && isnotnull(ss_sold_time_sk#285)) && isnotnull(ss_store_sk#291))
:  :           :     :     :     +- HiveTableRelation `tpcds_text_2`.`store_sales`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [ss_sold_date_sk#284, ss_sold_time_sk#285, ss_item_sk#286, ss_customer_sk#287, ss_cdemo_sk#288, ss_hdemo_sk#289, ss_addr_sk#290, ss_store_sk#291, ss_promo_sk#292, ss_ticket_number#293L, ss_quantity#294, ss_wholesale_cost#295, ss_list_price#296, ss_sales_price#297, ss_ext_discount_amt#298, ss_ext_sales_price#299, ss_ext_wholesale_cost#300, ss_ext_list_price#301, ss_ext_tax#302, ss_coupon_amt#303, ss_net_paid#304, ss_net_paid_inc_tax#305, ss_net_profit#306]
:  :           :     :     +- Project [hd_demo_sk#307]
:  :           :     :        +- Filter (((((hd_dep_count#310 = 3) && (hd_vehicle_count#311 <= 5)) || ((hd_dep_count#310 = 0) && (hd_vehicle_count#311 <= 2))) || ((hd_dep_count#310 = 1) && (hd_vehicle_count#311 <= 3))) && isnotnull(hd_demo_sk#307))
:  :           :     :           +- HiveTableRelation `tpcds_text_2`.`household_demographics`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [hd_demo_sk#307, hd_income_band_sk#308, hd_buy_potential#309, hd_dep_count#310, hd_vehicle_count#311]
:  :           :     +- Project [t_time_sk#312]
:  :           :        +- Filter (((isnotnull(t_hour#315) && isnotnull(t_minute#316)) && ((t_hour#315 = 10) && (t_minute#316 >= 30))) && isnotnull(t_time_sk#312))
:  :           :           +- HiveTableRelation `tpcds_text_2`.`time_dim`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [t_time_sk#312, t_time_id#313, t_time#314, t_hour#315, t_minute#316, t_second#317, t_am_pm#318, t_shift#319, t_sub_shift#320, t_meal_time#321]
:  :           +- Project [s_store_sk#322]
:  :              +- Filter ((isnotnull(s_store_name#327) && (s_store_name#327 = ese)) && isnotnull(s_store_sk#322))
:  :                 +- HiveTableRelation `tpcds_text_2`.`store`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [s_store_sk#322, s_store_id#323, s_rec_start_date#324, s_rec_end_date#325, s_closed_date_sk#326, s_store_name#327, s_number_employees#328, s_floor_space#329, s_hours#330, s_manager#331, s_market_id#332, s_geography_class#333, s_market_desc#334, s_market_manager#335, s_division_id#336, s_division_name#337, s_company_id#338, s_company_name#339, s_street_number#340, s_street_name#341, s_street_type#342, s_suite_number#343, s_city#344, s_county#345, ... 5 more fields]
:  +- Aggregate [count(1) AS h11_to_11_30#5L]
:     +- Project
:        +- Join Inner, (ss_store_sk#358 = s_store_sk#389)
:           :- Project [ss_store_sk#358]
:           :  +- Join Inner, (ss_sold_time_sk#352 = t_time_sk#379)
:           :     :- Project [ss_sold_time_sk#352, ss_store_sk#358]
:           :     :  +- Join Inner, (ss_hdemo_sk#356 = hd_demo_sk#374)
:           :     :     :- Project [ss_sold_time_sk#352, ss_hdemo_sk#356, ss_store_sk#358]
:           :     :     :  +- Filter ((isnotnull(ss_hdemo_sk#356) && isnotnull(ss_sold_time_sk#352)) && isnotnull(ss_store_sk#358))
:           :     :     :     +- HiveTableRelation `tpcds_text_2`.`store_sales`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [ss_sold_date_sk#351, ss_sold_time_sk#352, ss_item_sk#353, ss_customer_sk#354, ss_cdemo_sk#355, ss_hdemo_sk#356, ss_addr_sk#357, ss_store_sk#358, ss_promo_sk#359, ss_ticket_number#360L, ss_quantity#361, ss_wholesale_cost#362, ss_list_price#363, ss_sales_price#364, ss_ext_discount_amt#365, ss_ext_sales_price#366, ss_ext_wholesale_cost#367, ss_ext_list_price#368, ss_ext_tax#369, ss_coupon_amt#370, ss_net_paid#371, ss_net_paid_inc_tax#372, ss_net_profit#373]
:           :     :     +- Project [hd_demo_sk#374]
:           :     :        +- Filter (((((hd_dep_count#377 = 3) && (hd_vehicle_count#378 <= 5)) || ((hd_dep_count#377 = 0) && (hd_vehicle_count#378 <= 2))) || ((hd_dep_count#377 = 1) && (hd_vehicle_count#378 <= 3))) && isnotnull(hd_demo_sk#374))
:           :     :           +- HiveTableRelation `tpcds_text_2`.`household_demographics`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [hd_demo_sk#374, hd_income_band_sk#375, hd_buy_potential#376, hd_dep_count#377, hd_vehicle_count#378]
:           :     +- Project [t_time_sk#379]
:           :        +- Filter (((isnotnull(t_hour#382) && isnotnull(t_minute#383)) && ((t_hour#382 = 11) && (t_minute#383 < 30))) && isnotnull(t_time_sk#379))
:           :           +- HiveTableRelation `tpcds_text_2`.`time_dim`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [t_time_sk#379, t_time_id#380, t_time#381, t_hour#382, t_minute#383, t_second#384, t_am_pm#385, t_shift#386, t_sub_shift#387, t_meal_time#388]
:           +- Project [s_store_sk#389]
:              +- Filter ((isnotnull(s_store_name#394) && (s_store_name#394 = ese)) && isnotnull(s_store_sk#389))
:                 +- HiveTableRelation `tpcds_text_2`.`store`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [s_store_sk#389, s_store_id#390, s_rec_start_date#391, s_rec_end_date#392, s_closed_date_sk#393, s_store_name#394, s_number_employees#395, s_floor_space#396, s_hours#397, s_manager#398, s_market_id#399, s_geography_class#400, s_market_desc#401, s_market_manager#402, s_division_id#403, s_division_name#404, s_company_id#405, s_company_name#406, s_street_number#407, s_street_name#408, s_street_type#409, s_suite_number#410, s_city#411, s_county#412, ... 5 more fields]
+- Aggregate [count(1) AS h11_30_to_12#6L]
   +- Project
      +- Join Inner, (ss_store_sk#425 = s_store_sk#456)
         :- Project [ss_store_sk#425]
         :  +- Join Inner, (ss_sold_time_sk#419 = t_time_sk#446)
         :     :- Project [ss_sold_time_sk#419, ss_store_sk#425]
         :     :  +- Join Inner, (ss_hdemo_sk#423 = hd_demo_sk#441)
         :     :     :- Project [ss_sold_time_sk#419, ss_hdemo_sk#423, ss_store_sk#425]
         :     :     :  +- Filter ((isnotnull(ss_hdemo_sk#423) && isnotnull(ss_sold_time_sk#419)) && isnotnull(ss_store_sk#425))
         :     :     :     +- HiveTableRelation `tpcds_text_2`.`store_sales`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [ss_sold_date_sk#418, ss_sold_time_sk#419, ss_item_sk#420, ss_customer_sk#421, ss_cdemo_sk#422, ss_hdemo_sk#423, ss_addr_sk#424, ss_store_sk#425, ss_promo_sk#426, ss_ticket_number#427L, ss_quantity#428, ss_wholesale_cost#429, ss_list_price#430, ss_sales_price#431, ss_ext_discount_amt#432, ss_ext_sales_price#433, ss_ext_wholesale_cost#434, ss_ext_list_price#435, ss_ext_tax#436, ss_coupon_amt#437, ss_net_paid#438, ss_net_paid_inc_tax#439, ss_net_profit#440]
         :     :     +- Project [hd_demo_sk#441]
         :     :        +- Filter (((((hd_dep_count#444 = 3) && (hd_vehicle_count#445 <= 5)) || ((hd_dep_count#444 = 0) && (hd_vehicle_count#445 <= 2))) || ((hd_dep_count#444 = 1) && (hd_vehicle_count#445 <= 3))) && isnotnull(hd_demo_sk#441))
         :     :           +- HiveTableRelation `tpcds_text_2`.`household_demographics`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [hd_demo_sk#441, hd_income_band_sk#442, hd_buy_potential#443, hd_dep_count#444, hd_vehicle_count#445]
         :     +- Project [t_time_sk#446]
         :        +- Filter (((isnotnull(t_hour#449) && isnotnull(t_minute#450)) && ((t_hour#449 = 11) && (t_minute#450 >= 30))) && isnotnull(t_time_sk#446))
         :           +- HiveTableRelation `tpcds_text_2`.`time_dim`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [t_time_sk#446, t_time_id#447, t_time#448, t_hour#449, t_minute#450, t_second#451, t_am_pm#452, t_shift#453, t_sub_shift#454, t_meal_time#455]
         +- Project [s_store_sk#456]
            +- Filter ((isnotnull(s_store_name#461) && (s_store_name#461 = ese)) && isnotnull(s_store_sk#456))
               +- HiveTableRelation `tpcds_text_2`.`store`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [s_store_sk#456, s_store_id#457, s_rec_start_date#458, s_rec_end_date#459, s_closed_date_sk#460, s_store_name#461, s_number_employees#462, s_floor_space#463, s_hours#464, s_manager#465, s_market_id#466, s_geography_class#467, s_market_desc#468, s_market_manager#469, s_division_id#470, s_division_name#471, s_company_id#472, s_company_name#473, s_street_number#474, s_street_name#475, s_street_type#476, s_suite_number#477, s_city#478, s_county#479, ... 5 more fields]
and
Aggregate [count(1) AS h12_to_12_30#7L]
+- Project
   +- Join Inner, (ss_store_sk#492 = s_store_sk#523)
      :- Project [ss_store_sk#492]
      :  +- Join Inner, (ss_sold_time_sk#486 = t_time_sk#513)
      :     :- Project [ss_sold_time_sk#486, ss_store_sk#492]
      :     :  +- Join Inner, (ss_hdemo_sk#490 = hd_demo_sk#508)
      :     :     :- Project [ss_sold_time_sk#486, ss_hdemo_sk#490, ss_store_sk#492]
      :     :     :  +- Filter ((isnotnull(ss_hdemo_sk#490) && isnotnull(ss_sold_time_sk#486)) && isnotnull(ss_store_sk#492))
      :     :     :     +- HiveTableRelation `tpcds_text_2`.`store_sales`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [ss_sold_date_sk#485, ss_sold_time_sk#486, ss_item_sk#487, ss_customer_sk#488, ss_cdemo_sk#489, ss_hdemo_sk#490, ss_addr_sk#491, ss_store_sk#492, ss_promo_sk#493, ss_ticket_number#494L, ss_quantity#495, ss_wholesale_cost#496, ss_list_price#497, ss_sales_price#498, ss_ext_discount_amt#499, ss_ext_sales_price#500, ss_ext_wholesale_cost#501, ss_ext_list_price#502, ss_ext_tax#503, ss_coupon_amt#504, ss_net_paid#505, ss_net_paid_inc_tax#506, ss_net_profit#507]
      :     :     +- Project [hd_demo_sk#508]
      :     :        +- Filter (((((hd_dep_count#511 = 3) && (hd_vehicle_count#512 <= 5)) || ((hd_dep_count#511 = 0) && (hd_vehicle_count#512 <= 2))) || ((hd_dep_count#511 = 1) && (hd_vehicle_count#512 <= 3))) && isnotnull(hd_demo_sk#508))
      :     :           +- HiveTableRelation `tpcds_text_2`.`household_demographics`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [hd_demo_sk#508, hd_income_band_sk#509, hd_buy_potential#510, hd_dep_count#511, hd_vehicle_count#512]
      :     +- Project [t_time_sk#513]
      :        +- Filter (((isnotnull(t_hour#516) && isnotnull(t_minute#517)) && ((t_hour#516 = 12) && (t_minute#517 < 30))) && isnotnull(t_time_sk#513))
      :           +- HiveTableRelation `tpcds_text_2`.`time_dim`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [t_time_sk#513, t_time_id#514, t_time#515, t_hour#516, t_minute#517, t_second#518, t_am_pm#519, t_shift#520, t_sub_shift#521, t_meal_time#522]
      +- Project [s_store_sk#523]
         +- Filter ((isnotnull(s_store_name#528) && (s_store_name#528 = ese)) && isnotnull(s_store_sk#523))
            +- HiveTableRelation `tpcds_text_2`.`store`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [s_store_sk#523, s_store_id#524, s_rec_start_date#525, s_rec_end_date#526, s_closed_date_sk#527, s_store_name#528, s_number_employees#529, s_floor_space#530, s_hours#531, s_manager#532, s_market_id#533, s_geography_class#534, s_market_desc#535, s_market_manager#536, s_division_id#537, s_division_name#538, s_company_id#539, s_company_name#540, s_street_number#541, s_street_name#542, s_street_type#543, s_suite_number#544, s_city#545, s_county#546, ... 5 more fields]
Join condition is missing or trivial.
Either: use the CROSS JOIN syntax to allow cartesian products between these
relations, or: enable implicit cartesian products by setting the configuration
variable spark.sql.crossJoin.enabled=true;
18/06/26 03:32:17 INFO server.AbstractConnector: Stopped Spark@52fdbb7d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18/06/26 03:32:17 INFO ui.SparkUI: Stopped Spark web UI at http://dc1master-lan1:4040
18/06/26 03:32:17 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors
18/06/26 03:32:17 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
18/06/26 03:32:17 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/06/26 03:32:17 INFO memory.MemoryStore: MemoryStore cleared
18/06/26 03:32:17 INFO storage.BlockManager: BlockManager stopped
18/06/26 03:32:17 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/06/26 03:32:17 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/06/26 03:32:17 INFO spark.SparkContext: Successfully stopped SparkContext
18/06/26 03:32:17 INFO util.ShutdownHookManager: Shutdown hook called
18/06/26 03:32:17 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-433815c9-49d3-44b0-a7cc-d2f33d583b7b
18/06/26 03:32:17 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-5bd0138e-dd4b-485f-9b91-1ee1c265cb44
