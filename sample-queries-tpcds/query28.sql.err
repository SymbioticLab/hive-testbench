18/06/26 02:27:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/06/26 02:27:22 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18/06/26 02:27:22 INFO metastore.ObjectStore: ObjectStore, initialize called
18/06/26 02:27:22 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
18/06/26 02:27:22 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored
18/06/26 02:27:24 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
18/06/26 02:27:25 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 02:27:25 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 02:27:26 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 02:27:26 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 02:27:26 INFO DataNucleus.Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
18/06/26 02:27:26 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
18/06/26 02:27:26 INFO metastore.ObjectStore: Initialized ObjectStore
18/06/26 02:27:26 INFO metastore.HiveMetaStore: Added admin role in metastore
18/06/26 02:27:26 INFO metastore.HiveMetaStore: Added public role in metastore
18/06/26 02:27:26 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
18/06/26 02:27:26 INFO metastore.HiveMetaStore: 0: get_all_databases
18/06/26 02:27:26 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_all_databases	
18/06/26 02:27:26 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
18/06/26 02:27:26 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
18/06/26 02:27:26 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
18/06/26 02:27:26 INFO metastore.HiveMetaStore: 0: get_functions: db=tpcds_text_2 pat=*
18/06/26 02:27:26 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_functions: db=tpcds_text_2 pat=*	
18/06/26 02:27:27 INFO session.SessionState: Created local directory: /tmp/147826f6-ff89-486a-b380-d5f75298b2d7_resources
18/06/26 02:27:27 INFO session.SessionState: Created HDFS directory: /tmp/hive/wentingt/147826f6-ff89-486a-b380-d5f75298b2d7
18/06/26 02:27:27 INFO session.SessionState: Created local directory: /tmp/wentingt/147826f6-ff89-486a-b380-d5f75298b2d7
18/06/26 02:27:27 INFO session.SessionState: Created HDFS directory: /tmp/hive/wentingt/147826f6-ff89-486a-b380-d5f75298b2d7/_tmp_space.db
18/06/26 02:27:27 INFO spark.SparkContext: Running Spark version 2.4.0-SNAPSHOT
18/06/26 02:27:27 INFO spark.SparkContext: Submitted application: SparkSQL::10.0.1.253
18/06/26 02:27:27 INFO spark.SecurityManager: Changing view acls to: wentingt
18/06/26 02:27:27 INFO spark.SecurityManager: Changing modify acls to: wentingt
18/06/26 02:27:27 INFO spark.SecurityManager: Changing view acls groups to: 
18/06/26 02:27:27 INFO spark.SecurityManager: Changing modify acls groups to: 
18/06/26 02:27:27 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(wentingt); groups with view permissions: Set(); users  with modify permissions: Set(wentingt); groups with modify permissions: Set()
18/06/26 02:27:27 INFO util.Utils: Successfully started service 'sparkDriver' on port 45134.
18/06/26 02:27:27 INFO spark.SparkEnv: Registering MapOutputTracker
18/06/26 02:27:27 INFO spark.SparkEnv: Registering BlockManagerMaster
18/06/26 02:27:27 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/06/26 02:27:27 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/06/26 02:27:27 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-633a91f8-c5fc-46f1-8f5b-62ca77439675
18/06/26 02:27:27 INFO memory.MemoryStore: MemoryStore started with capacity 394.2 MB
18/06/26 02:27:27 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/06/26 02:27:28 INFO util.log: Logging initialized @7695ms
18/06/26 02:27:28 INFO server.Server: jetty-9.3.z-SNAPSHOT
18/06/26 02:27:28 INFO server.Server: Started @7793ms
18/06/26 02:27:28 INFO server.AbstractConnector: Started ServerConnector@1641ea3e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18/06/26 02:27:28 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79f5a6ed{/jobs,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b44318{/jobs/json,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e807e2{/jobs/job,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@203b953c{/jobs/job/json,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@730bea0{/stages,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41a16eb3{/stages/json,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@677cb96e{/stages/stage,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53cb0bcb{/stages/stage/json,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51fe7f15{/stages/pool,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5873f3f0{/stages/pool/json,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@684372d0{/storage,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63dda940{/storage/json,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41f964f9{/storage/rdd,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@652e345{/storage/rdd/json,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7574d4ad{/environment,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7bede4ea{/environment/json,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@713999c2{/executors,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6060146b{/executors/json,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33627576{/executors/threadDump,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27bc1d44{/executors/threadDump/json,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1af677f8{/static,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e0a9b1d{/,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52f9e8bb{/api,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@539316bb{/jobs/job/kill,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d1a859c{/stages/stage/kill,null,AVAILABLE,@Spark}
18/06/26 02:27:28 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://dc1master-lan1:4040
18/06/26 02:27:28 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 0
18/06/26 02:27:28 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:27:28 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://dc1master-lan1:7077...
18/06/26 02:27:28 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 0
18/06/26 02:27:28 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:27:28 INFO client.TransportClientFactory: Successfully created connection to dc1master-lan1/10.0.1.253:7077 after 45 ms (0 ms spent in bootstraps)
18/06/26 02:27:28 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20180626022728-0034
18/06/26 02:27:28 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20180626022728-0034/0 on worker-20180626004527-10.0.1.2-38332 (10.0.1.2:38332) with 10 core(s)
18/06/26 02:27:28 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20180626022728-0034/0 on hostPort 10.0.1.2:38332 with 10 core(s), 2.0 GB RAM
18/06/26 02:27:28 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20180626022728-0034/1 on worker-20180626004527-10.0.1.1-34053 (10.0.1.1:34053) with 10 core(s)
18/06/26 02:27:28 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20180626022728-0034/1 on hostPort 10.0.1.1:34053 with 10 core(s), 2.0 GB RAM
18/06/26 02:27:28 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33708.
18/06/26 02:27:28 INFO netty.NettyBlockTransferService: Server created on dc1master-lan1:33708
18/06/26 02:27:28 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/06/26 02:27:28 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20180626022728-0034/0 is now RUNNING
18/06/26 02:27:28 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20180626022728-0034/1 is now RUNNING
18/06/26 02:27:28 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, dc1master-lan1, 33708, None)
18/06/26 02:27:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager dc1master-lan1:33708 with 394.2 MB RAM, BlockManagerId(driver, dc1master-lan1, 33708, None)
18/06/26 02:27:28 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, dc1master-lan1, 33708, None)
18/06/26 02:27:28 INFO storage.BlockManager: external shuffle service port = 7337
18/06/26 02:27:28 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, dc1master-lan1, 33708, None)
18/06/26 02:27:28 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@671d97bc{/metrics/json,null,AVAILABLE,@Spark}
18/06/26 02:27:29 INFO scheduler.EventLoggingListener: Logging events to hdfs://dc1master:9000/user/wentingt/spark-logs/app-20180626022728-0034
18/06/26 02:27:29 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
18/06/26 02:27:29 INFO internal.SharedState: loading hive config file: file:/users/wentingt/spark-terra/conf/hive-site.xml
18/06/26 02:27:29 INFO internal.SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
18/06/26 02:27:29 INFO internal.SharedState: Warehouse path is '/user/hive/warehouse'.
18/06/26 02:27:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38e00b47{/SQL,null,AVAILABLE,@Spark}
18/06/26 02:27:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a631049{/SQL/json,null,AVAILABLE,@Spark}
18/06/26 02:27:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@343db2f6{/SQL/execution,null,AVAILABLE,@Spark}
18/06/26 02:27:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c815fdc{/SQL/execution/json,null,AVAILABLE,@Spark}
18/06/26 02:27:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@517fbf62{/static/sql,null,AVAILABLE,@Spark}
18/06/26 02:27:29 INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
18/06/26 02:27:29 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
18/06/26 02:27:29 INFO metastore.HiveMetaStore: 0: get_database: default
18/06/26 02:27:29 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: default	
18/06/26 02:27:29 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 0
18/06/26 02:27:29 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:27:29 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 0
18/06/26 02:27:29 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:27:29 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
18/06/26 02:27:29 INFO metastore.HiveMetaStore: 0: get_database: global_temp
18/06/26 02:27:29 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: global_temp	
18/06/26 02:27:29 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
18/06/26 02:27:29 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:29 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 0
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 0
18/06/26 02:27:30 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_database: tpcds_text_2
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_database: tpcds_text_2	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 02:27:30 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.1.2:48138) with ID 0
18/06/26 02:27:30 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 1
18/06/26 02:27:30 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:27:30 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 1
18/06/26 02:27:30 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:27:30 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.1.1:37966) with ID 1
18/06/26 02:27:30 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:27:30 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:27:30 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:27:30 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:27:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.2:37267 with 912.3 MB RAM, BlockManagerId(0, 10.0.1.2, 37267, None)
18/06/26 02:27:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.1.1:40338 with 912.3 MB RAM, BlockManagerId(1, 10.0.1.1, 40338, None)
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 02:27:30 INFO metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=store_sales
18/06/26 02:27:30 INFO HiveMetaStore.audit: ugi=wentingt	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=store_sales	
18/06/26 02:27:31 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:27:31 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:27:31 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:27:31 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
18/06/26 02:27:32 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.size ### 2
18/06/26 02:27:32 INFO scheduler.TaskSchedulerImpl: resourceOffers initialized tasks.flatten.size ### 0
18/06/26 02:27:32 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.size ### 2
18/06/26 02:27:32 INFO scheduler.TaskSchedulerImpl: resourceOffers returned tasks.flatten.size ### 0
Error in query: Detected implicit cartesian product for INNER join between logical plans
Join Inner
:- Join Inner
:  :- Join Inner
:  :  :- Join Inner
:  :  :  :- Aggregate [avg(cast(ss_list_price#30 as double)) AS B1_LP#0, count(ss_list_price#30) AS B1_CNT#1L, count(distinct ss_list_price#30) AS B1_CNTD#2L]
:  :  :  :  +- Project [ss_list_price#30]
:  :  :  :     +- Filter (isnotnull(ss_quantity#28) && (((ss_quantity#28 >= 0) && (ss_quantity#28 <= 5)) && ((((ss_list_price#30 >= 11.0) && (ss_list_price#30 <= 21.0)) || ((ss_coupon_amt#37 >= 460.0) && (ss_coupon_amt#37 <= 1460.0))) || ((ss_wholesale_cost#29 >= 14.0) && (ss_wholesale_cost#29 <= 34.0)))))
:  :  :  :        +- HiveTableRelation `tpcds_text_2`.`store_sales`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [ss_sold_date_sk#18, ss_sold_time_sk#19, ss_item_sk#20, ss_customer_sk#21, ss_cdemo_sk#22, ss_hdemo_sk#23, ss_addr_sk#24, ss_store_sk#25, ss_promo_sk#26, ss_ticket_number#27L, ss_quantity#28, ss_wholesale_cost#29, ss_list_price#30, ss_sales_price#31, ss_ext_discount_amt#32, ss_ext_sales_price#33, ss_ext_wholesale_cost#34, ss_ext_list_price#35, ss_ext_tax#36, ss_coupon_amt#37, ss_net_paid#38, ss_net_paid_inc_tax#39, ss_net_profit#40]
:  :  :  +- Aggregate [avg(cast(ss_list_price#53 as double)) AS B2_LP#3, count(ss_list_price#53) AS B2_CNT#4L, count(distinct ss_list_price#53) AS B2_CNTD#5L]
:  :  :     +- Project [ss_list_price#53]
:  :  :        +- Filter (isnotnull(ss_quantity#51) && (((ss_quantity#51 >= 6) && (ss_quantity#51 <= 10)) && ((((ss_list_price#53 >= 91.0) && (ss_list_price#53 <= 101.0)) || ((ss_coupon_amt#60 >= 1430.0) && (ss_coupon_amt#60 <= 2430.0))) || ((ss_wholesale_cost#52 >= 32.0) && (ss_wholesale_cost#52 <= 52.0)))))
:  :  :           +- HiveTableRelation `tpcds_text_2`.`store_sales`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [ss_sold_date_sk#41, ss_sold_time_sk#42, ss_item_sk#43, ss_customer_sk#44, ss_cdemo_sk#45, ss_hdemo_sk#46, ss_addr_sk#47, ss_store_sk#48, ss_promo_sk#49, ss_ticket_number#50L, ss_quantity#51, ss_wholesale_cost#52, ss_list_price#53, ss_sales_price#54, ss_ext_discount_amt#55, ss_ext_sales_price#56, ss_ext_wholesale_cost#57, ss_ext_list_price#58, ss_ext_tax#59, ss_coupon_amt#60, ss_net_paid#61, ss_net_paid_inc_tax#62, ss_net_profit#63]
:  :  +- Aggregate [avg(cast(ss_list_price#76 as double)) AS B3_LP#6, count(ss_list_price#76) AS B3_CNT#7L, count(distinct ss_list_price#76) AS B3_CNTD#8L]
:  :     +- Project [ss_list_price#76]
:  :        +- Filter (isnotnull(ss_quantity#74) && (((ss_quantity#74 >= 11) && (ss_quantity#74 <= 15)) && ((((ss_list_price#76 >= 66.0) && (ss_list_price#76 <= 76.0)) || ((ss_coupon_amt#83 >= 920.0) && (ss_coupon_amt#83 <= 1920.0))) || ((ss_wholesale_cost#75 >= 4.0) && (ss_wholesale_cost#75 <= 24.0)))))
:  :           +- HiveTableRelation `tpcds_text_2`.`store_sales`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [ss_sold_date_sk#64, ss_sold_time_sk#65, ss_item_sk#66, ss_customer_sk#67, ss_cdemo_sk#68, ss_hdemo_sk#69, ss_addr_sk#70, ss_store_sk#71, ss_promo_sk#72, ss_ticket_number#73L, ss_quantity#74, ss_wholesale_cost#75, ss_list_price#76, ss_sales_price#77, ss_ext_discount_amt#78, ss_ext_sales_price#79, ss_ext_wholesale_cost#80, ss_ext_list_price#81, ss_ext_tax#82, ss_coupon_amt#83, ss_net_paid#84, ss_net_paid_inc_tax#85, ss_net_profit#86]
:  +- Aggregate [avg(cast(ss_list_price#99 as double)) AS B4_LP#9, count(ss_list_price#99) AS B4_CNT#10L, count(distinct ss_list_price#99) AS B4_CNTD#11L]
:     +- Project [ss_list_price#99]
:        +- Filter (isnotnull(ss_quantity#97) && (((ss_quantity#97 >= 16) && (ss_quantity#97 <= 20)) && ((((ss_list_price#99 >= 142.0) && (ss_list_price#99 <= 152.0)) || ((ss_coupon_amt#106 >= 3054.0) && (ss_coupon_amt#106 <= 4054.0))) || ((ss_wholesale_cost#98 >= 80.0) && (ss_wholesale_cost#98 <= 100.0)))))
:           +- HiveTableRelation `tpcds_text_2`.`store_sales`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [ss_sold_date_sk#87, ss_sold_time_sk#88, ss_item_sk#89, ss_customer_sk#90, ss_cdemo_sk#91, ss_hdemo_sk#92, ss_addr_sk#93, ss_store_sk#94, ss_promo_sk#95, ss_ticket_number#96L, ss_quantity#97, ss_wholesale_cost#98, ss_list_price#99, ss_sales_price#100, ss_ext_discount_amt#101, ss_ext_sales_price#102, ss_ext_wholesale_cost#103, ss_ext_list_price#104, ss_ext_tax#105, ss_coupon_amt#106, ss_net_paid#107, ss_net_paid_inc_tax#108, ss_net_profit#109]
+- Aggregate [avg(cast(ss_list_price#122 as double)) AS B5_LP#12, count(ss_list_price#122) AS B5_CNT#13L, count(distinct ss_list_price#122) AS B5_CNTD#14L]
   +- Project [ss_list_price#122]
      +- Filter (isnotnull(ss_quantity#120) && (((ss_quantity#120 >= 21) && (ss_quantity#120 <= 25)) && ((((ss_list_price#122 >= 135.0) && (ss_list_price#122 <= 145.0)) || ((ss_coupon_amt#129 >= 14180.0) && (ss_coupon_amt#129 <= 15180.0))) || ((ss_wholesale_cost#121 >= 38.0) && (ss_wholesale_cost#121 <= 58.0)))))
         +- HiveTableRelation `tpcds_text_2`.`store_sales`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [ss_sold_date_sk#110, ss_sold_time_sk#111, ss_item_sk#112, ss_customer_sk#113, ss_cdemo_sk#114, ss_hdemo_sk#115, ss_addr_sk#116, ss_store_sk#117, ss_promo_sk#118, ss_ticket_number#119L, ss_quantity#120, ss_wholesale_cost#121, ss_list_price#122, ss_sales_price#123, ss_ext_discount_amt#124, ss_ext_sales_price#125, ss_ext_wholesale_cost#126, ss_ext_list_price#127, ss_ext_tax#128, ss_coupon_amt#129, ss_net_paid#130, ss_net_paid_inc_tax#131, ss_net_profit#132]
and
Aggregate [avg(cast(ss_list_price#145 as double)) AS B6_LP#15, count(ss_list_price#145) AS B6_CNT#16L, count(distinct ss_list_price#145) AS B6_CNTD#17L]
+- Project [ss_list_price#145]
   +- Filter (isnotnull(ss_quantity#143) && (((ss_quantity#143 >= 26) && (ss_quantity#143 <= 30)) && ((((ss_list_price#145 >= 28.0) && (ss_list_price#145 <= 38.0)) || ((ss_coupon_amt#152 >= 2513.0) && (ss_coupon_amt#152 <= 3513.0))) || ((ss_wholesale_cost#144 >= 42.0) && (ss_wholesale_cost#144 <= 62.0)))))
      +- HiveTableRelation `tpcds_text_2`.`store_sales`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [ss_sold_date_sk#133, ss_sold_time_sk#134, ss_item_sk#135, ss_customer_sk#136, ss_cdemo_sk#137, ss_hdemo_sk#138, ss_addr_sk#139, ss_store_sk#140, ss_promo_sk#141, ss_ticket_number#142L, ss_quantity#143, ss_wholesale_cost#144, ss_list_price#145, ss_sales_price#146, ss_ext_discount_amt#147, ss_ext_sales_price#148, ss_ext_wholesale_cost#149, ss_ext_list_price#150, ss_ext_tax#151, ss_coupon_amt#152, ss_net_paid#153, ss_net_paid_inc_tax#154, ss_net_profit#155]
Join condition is missing or trivial.
Either: use the CROSS JOIN syntax to allow cartesian products between these
relations, or: enable implicit cartesian products by setting the configuration
variable spark.sql.crossJoin.enabled=true;
18/06/26 02:27:33 INFO server.AbstractConnector: Stopped Spark@1641ea3e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18/06/26 02:27:33 INFO ui.SparkUI: Stopped Spark web UI at http://dc1master-lan1:4040
18/06/26 02:27:33 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors
18/06/26 02:27:33 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
18/06/26 02:27:33 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/06/26 02:27:33 INFO memory.MemoryStore: MemoryStore cleared
18/06/26 02:27:33 INFO storage.BlockManager: BlockManager stopped
18/06/26 02:27:33 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/06/26 02:27:33 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/06/26 02:27:33 INFO spark.SparkContext: Successfully stopped SparkContext
18/06/26 02:27:33 INFO util.ShutdownHookManager: Shutdown hook called
18/06/26 02:27:33 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-06f5808b-082d-4573-b7d7-6baf0344e2b3
18/06/26 02:27:33 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-543f5be2-7ab8-447a-b8ba-1e3df52a16d4
